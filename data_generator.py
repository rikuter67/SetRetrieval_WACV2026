import os
import gzip
import pickle
import numpy as np
import tensorflow as tf
from collections import defaultdict
from tensorflow.keras.utils import Sequence

# ============================================================================
# ãƒ‡ãƒ¼ã‚¿ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿æœ¬ä½“
# ============================================================================

class DataGenerator(Sequence):
    """
    ã‚»ãƒƒãƒˆæ¤œç´¢ã‚¿ã‚¹ã‚¯ç”¨ã®é«˜æ©Ÿèƒ½ãƒ‡ãƒ¼ã‚¿ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ã€‚

    ä¸»ãªæ©Ÿèƒ½:
    - å®Œå…¨ãªã‚¢ã‚¤ãƒ†ãƒ é›†åˆã‚’å‹•çš„ã«ã‚¯ã‚¨ãƒªã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã«åˆ†å‰²ã€‚
    - é›†åˆã®æœ€å°ã‚¢ã‚¤ãƒ†ãƒ æ•°ã‚’ä¿è¨¼ã—ã€ãƒ‡ãƒ¼ã‚¿ã®ä¸€è²«æ€§ã‚’æ‹…ä¿ã€‚
    - TPaNegç”¨ã®å€™è£œãƒã‚¬ãƒ†ã‚£ãƒ–ã‚’äº‹å‰è¨ˆç®—ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰é«˜é€Ÿã«èª­ã¿è¾¼ã¿ã€‚
    - ãƒ›ãƒ¯ã‚¤ãƒˆãƒ‹ãƒ³ã‚°å¤‰æ›ã‚’ã‚µãƒãƒ¼ãƒˆã€‚
    """
    def __init__(self,
                 split_path: str,
                 batch_size: int = 32,
                 shuffle: bool = True,
                 seed: int = 42,
                 # --- TPaNeg / Hard Negative Mining ---
                 use_negatives: bool = False,
                 negative_cache_path: str = None,
                 candidate_neg_num: int = 50,
                 # --- Set Integrity & Splitting ---
                 random_split: bool = True,
                 min_query_items: int = 2,
                 min_target_items: int = 2,
                 # --- Optional Transformations ---
                 whitening_params: dict = None,
                 include_set_ids: bool = False):
        """
        Args:
            split_path (str): ãƒ‡ãƒ¼ã‚¿åˆ†å‰²æƒ…å ±ã‚’å«ã‚€ .pkl ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã€‚
            batch_size (int): ãƒãƒƒãƒã‚µã‚¤ã‚ºã€‚
            shuffle (bool): ã‚¨ãƒãƒƒã‚¯ã”ã¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã™ã‚‹ã‹ã©ã†ã‹ã€‚
            seed (int): ä¹±æ•°ã‚·ãƒ¼ãƒ‰ã€‚
            use_negatives (bool): TPaNegç”¨ã®å€™è£œãƒã‚¬ãƒ†ã‚£ãƒ–ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹ã€‚
            negative_cache_path (str): äº‹å‰è¨ˆç®—ã•ã‚ŒãŸå€™è£œãƒã‚¬ãƒ†ã‚£ãƒ–ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã€‚
            candidate_neg_num (int): 1ã¤ã®æ­£è§£ã‚¢ã‚¤ãƒ†ãƒ ã«å¯¾ã—ã¦ä½¿ç”¨ã™ã‚‹å€™è£œãƒã‚¬ãƒ†ã‚£ãƒ–ã®æœ€å¤§æ•°ã€‚
            random_split (bool): é›†åˆã‚’ãƒ©ãƒ³ãƒ€ãƒ ãªæ¯”ç‡ã§åˆ†å‰²ã™ã‚‹ã‹ã©ã†ã‹ã€‚
            min_query_items (int): åˆ†å‰²å¾Œã®ã‚¯ã‚¨ãƒªé›†åˆãŒæŒã¤ã¹ãæœ€å°ã‚¢ã‚¤ãƒ†ãƒ æ•°ã€‚
            min_target_items (int): åˆ†å‰²å¾Œã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆé›†åˆãŒæŒã¤ã¹ãæœ€å°ã‚¢ã‚¤ãƒ†ãƒ æ•°ã€‚
            whitening_params (dict): ãƒ›ãƒ¯ã‚¤ãƒˆãƒ‹ãƒ³ã‚°å¤‰æ›ç”¨ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆmean, matrixï¼‰ã€‚
            include_set_ids (bool): ãƒ†ã‚¹ãƒˆæ™‚ã«SetIDã‚’ãƒãƒƒãƒã«å«ã‚ã‚‹ã‹ã€‚
        """
        # åŸºæœ¬ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.split_path = split_path
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.seed = seed
        self.rng = np.random.RandomState(seed)

        # é›†åˆåˆ†å‰²ã«é–¢ã™ã‚‹è¨­å®š
        self.random_split = random_split
        self.min_query_items = max(2, min_query_items)
        self.min_target_items = max(2, min_target_items)
        print(f"ğŸ”’ Set Integrity Guarantee: Query >= {self.min_query_items}, Target >= {self.min_target_items}")

        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        self._load_and_prepare_data()

        # TPaNeg (å€™è£œãƒã‚¬ãƒ†ã‚£ãƒ–) ã®è¨­å®š
        self.use_negatives = use_negatives
        self.candidate_neg_num = candidate_neg_num
        self.negative_cache_ids = None # IDsã‚’ä¿æŒã™ã‚‹
        self.item_feature_map = None # IDã‹ã‚‰ç‰¹å¾´é‡ã‚’å–å¾—ã™ã‚‹ãŸã‚ã®ãƒãƒƒãƒ—
        if self.use_negatives:
            self._load_negative_cache(negative_cache_path)

        # ãƒ›ãƒ¯ã‚¤ãƒˆãƒ‹ãƒ³ã‚°è¨­å®š
        self.whitening_params = whitening_params
        if self.whitening_params:
            self.whitening_mean = self.whitening_params['mean']
            self.whitening_matrix = self.whitening_params['matrix']
            print(f"âœ… Whitening enabled for DataGenerator ({os.path.basename(split_path)}).")
            
        # ãƒ†ã‚¹ãƒˆç”¨è¨­å®š
        self.include_SetIDs = include_set_ids

        # ãƒãƒƒãƒç”Ÿæˆç”¨ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’åˆæœŸåŒ–
        self.indexes = np.arange(len(self.scene_ids))
        if self.shuffle:
            self.rng.shuffle(self.indexes)

    def _load_and_prepare_data(self):
        """ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã€çµ±åˆã€ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’è¡Œã†"""
        print(f"Loading data from {self.split_path}...")
        if self.split_path.endswith('.gz'):
            opener = gzip.open
        else:
            opener = open
        with opener(self.split_path, 'rb') as f:
            data = pickle.load(f)

        # ãƒ‡ãƒ¼ã‚¿å½¢å¼ã®ãƒã‚§ãƒƒã‚¯
        if not (isinstance(data, tuple) and len(data) >= 8):
            raise ValueError(f"Unsupported data format in {self.split_path}.")

        # pklã‹ã‚‰å„è¦ç´ ã‚’æŠ½å‡º
        q_feats, t_feats, scene_ids, q_cats, t_cats, _, q_ids, t_ids = data[:8]

        # ãƒ‡ãƒ¼ã‚¿ã‚’ã‚·ãƒ¼ãƒ³ï¼ˆå®Œå…¨ãªé›†åˆï¼‰ã”ã¨ã«çµ±åˆ
        raw_full_sets = []
        for i in range(len(q_feats)):
            q_valid_mask = q_cats[i] > 0
            t_valid_mask = t_cats[i] > 0

            # çµåˆã—ã¦å®Œå…¨ãªé›†åˆã‚’ä½œæˆ
            full_f = np.concatenate((q_feats[i][q_valid_mask], t_feats[i][t_valid_mask]), axis=0)
            full_c = np.concatenate((q_cats[i][q_valid_mask], t_cats[i][t_valid_mask]), axis=0)
            full_i = np.concatenate((q_ids[i][q_valid_mask], t_ids[i][t_valid_mask]), axis=0)

            raw_full_sets.append({
                'features': full_f.astype(np.float32),
                'categories': full_c.astype(np.int32),
                'ids': full_i.astype(np.int32)
            })
        
        # ã‚·ãƒ¼ãƒ³IDã‚‚ä¿æŒ
        self.scene_ids = list(scene_ids)
        print(f"Loaded {len(raw_full_sets)} raw scenes.")

        # =======================================================
        # â˜…â˜…â˜… çµ±åˆã•ã‚ŒãŸãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãƒ­ã‚¸ãƒƒã‚¯ â˜…â˜…â˜…
        # =======================================================
        min_required_items = self.min_query_items + self.min_target_items
        # â˜… ã‚«ãƒ†ã‚´ãƒªç¨®é¡æ•°ã®æœ€å°å€¤ (è¦ä»¶é€šã‚Š4ã«è¨­å®š)
        min_required_categories = 4
        
        original_count = len(raw_full_sets)
        
        self.full_sets = []
        valid_scene_ids = []
        
        filtered_items_count = 0
        
        for i, s in enumerate(raw_full_sets):
            
            # ã‚«ãƒ†ã‚´ãƒªãŒ0ï¼ˆãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ï¼‰ä»¥å¤–ã®ã‚¢ã‚¤ãƒ†ãƒ ã®ã¿ã‚’è€ƒæ…®
            valid_categories = s['categories'][s['categories'] > 0]
            
            num_unique_categories = len(np.unique(valid_categories))
            num_total_items = len(s['features'])
            
            # ã‚¢ã‚¤ãƒ†ãƒ ç·æ•°ã¨ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚«ãƒ†ã‚´ãƒªæ•°ã®ä¸¡æ–¹ã‚’ãƒã‚§ãƒƒã‚¯
            if (num_total_items >= min_required_items and 
                num_unique_categories >= min_required_categories):
                
                self.full_sets.append(s)
                valid_scene_ids.append(self.scene_ids[i])
            else:
                filtered_items_count += 1
        
        self.scene_ids = valid_scene_ids

        if filtered_items_count > 0:
            print(f"ğŸš« Filtered out {filtered_items_count} scenes. Reasons:")
            print(f"   - Minimum total items required: {min_required_items}")
            print(f"   - Minimum unique categories required: {min_required_categories}")
            
        print(f"âœ… Working with {len(self.full_sets)} valid scenes.")

        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ç”¨ã®æ¬¡å…ƒæƒ…å ±ã‚’ä¿å­˜
        set_sizes = [len(s['features']) for s in self.full_sets]
        self.max_item_num = max(set_sizes) if set_sizes else 0
        self.feature_dim = self.full_sets[0]['features'].shape[1] if self.full_sets else 0
        print(f"Data stats: max_items={self.max_item_num}, feature_dim={self.feature_dim}")

    def _load_negative_cache(self, cache_path):
        """äº‹å‰è¨ˆç®—ã•ã‚ŒãŸå€™è£œãƒã‚¬ãƒ†ã‚£ãƒ–ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’èª­ã¿è¾¼ã‚€"""
        if not cache_path or not os.path.exists(cache_path):
            print(f"âš ï¸ Negative cache not found at '{cache_path}'. TPaNeg is disabled.")
            self.use_negatives = False
            return
        
        try:
            print(f"Loading pre-computed negative cache from {cache_path}...")
            with open(cache_path, 'rb') as f:
                cache_data = pickle.load(f)
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ãƒ¼ã‚¿ã®åŸºæœ¬å½¢å¼ãƒã‚§ãƒƒã‚¯
            if not isinstance(cache_data, dict):
                raise ValueError(f"Cache data is not a dictionary. Type: {type(cache_data)}")
            
            print(f"Available keys in cache: {list(cache_data.keys())}")
            
            # item_feature_mapã®å–å¾—
            if 'item_feature_map' in cache_data:
                self.item_feature_map = cache_data['item_feature_map']
            else:
                raise ValueError("Missing 'item_feature_map' in cache")
            
            # hard_negatives_cacheã®å–å¾—ï¼ˆè¤‡æ•°ã®å½¢å¼ã«å¯¾å¿œï¼‰
            if 'hard_negatives_cache' in cache_data:
                # æ–°ã—ã„å½¢å¼
                self.negative_cache_ids = cache_data['hard_negatives_cache']
                print("âœ… Using standard cache format")
            elif 'negative_cache' in cache_data:
                # å¤ã„å½¢å¼ï¼ˆäº’æ›æ€§å¯¾å¿œï¼‰
                self.negative_cache_ids = cache_data['negative_cache']
                print("âš ï¸ Using legacy cache format")
            else:
                # ãã®ä»–ã®ã‚­ãƒ¼åã®å¯èƒ½æ€§ã‚’ç¢ºèª
                possible_keys = [k for k in cache_data.keys() if 'negative' in k.lower()]
                if possible_keys:
                    # æœ€åˆã«è¦‹ã¤ã‹ã£ãŸnegativeã‚’å«ã‚€ã‚­ãƒ¼ã‚’ä½¿ç”¨
                    fallback_key = possible_keys[0]
                    self.negative_cache_ids = cache_data[fallback_key]
                    print(f"âš ï¸ Using fallback cache key: '{fallback_key}'")
                else:
                    raise ValueError(f"No valid negative cache key found. Available keys: {list(cache_data.keys())}")
            
            # ãƒ‡ãƒ¼ã‚¿ã®å‹ãƒã‚§ãƒƒã‚¯
            if not isinstance(self.item_feature_map, dict):
                raise ValueError(f"item_feature_map is not a dictionary. Type: {type(self.item_feature_map)}")
            
            if not isinstance(self.negative_cache_ids, dict):
                raise ValueError(f"negative_cache_ids is not a dictionary. Type: {type(self.negative_cache_ids)}")
            
            # ã‚­ãƒ¼ã®å‹ãƒã‚§ãƒƒã‚¯ï¼ˆæ–‡å­—åˆ—ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªï¼‰
            if self.item_feature_map and not all(isinstance(k, str) for k in list(self.item_feature_map.keys())[:10]):
                print("âš ï¸ Converting item_feature_map keys to strings...")
                self.item_feature_map = {str(k): v for k, v in self.item_feature_map.items()}
            
            if self.negative_cache_ids and not all(isinstance(k, str) for k in list(self.negative_cache_ids.keys())[:10]):
                print("âš ï¸ Converting negative_cache_ids keys to strings...")
                self.negative_cache_ids = {str(k): v for k, v in self.negative_cache_ids.items()}
            
            print(f"âœ… Negative cache loaded successfully.")
            print(f"   - Item feature map: {len(self.item_feature_map)} items")
            print(f"   - Negative cache: {len(self.negative_cache_ids)} cached relationships")
            
            # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèªï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
            if self.negative_cache_ids:
                sample_key = next(iter(self.negative_cache_ids.keys()))
                sample_value = self.negative_cache_ids[sample_key]
                print(f"   - Sample cache entry: {sample_key} -> {len(sample_value) if isinstance(sample_value, list) else type(sample_value)} negatives")
            
        except Exception as e:
            print(f"âŒ Failed to load negative cache: {e}")
            print(f"   Cache file: {cache_path}")
            if 'cache_data' in locals():
                print(f"   Available keys: {list(cache_data.keys()) if isinstance(cache_data, dict) else 'Not a dict'}")
            print("   TPaNeg is disabled.")
            self.use_negatives = False
            self.negative_cache_ids = {}
            self.item_feature_map = {}
            # ã‚¨ãƒ©ãƒ¼ã‚’å†ã‚¹ãƒ­ãƒ¼ã—ãªã„ï¼ˆãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ç¶šè¡Œï¼‰
            return

    def _random_split_set(self, full_set):
        """é›†åˆã‚’æŒ‡å®šã•ã‚ŒãŸæœ€å°æ•°ã‚’ä¿è¨¼ã—ã¤ã¤ã€ã‚¯ã‚¨ãƒªã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®å·®ã‚’1ä»¥ä¸‹ã«æŠ‘ãˆã¦åˆ†å‰²ã™ã‚‹"""
        features, categories, ids = full_set['features'], full_set['categories'], full_set['ids']
        total_items = len(features)
        
        indices = np.arange(total_items)
        if self.random_split:
            self.rng.shuffle(indices)
        # random_split=Falseã®å ´åˆã¯ã€indicesã®é †åºã¯ãã®ã¾ã¾ï¼ˆæ±ºå®šè«–çš„åˆ†å‰²ï¼‰

        # â˜…â˜…â˜… ã‚¢ã‚¤ãƒ†ãƒ æ•°ã®å·®ã‚’1ä»¥ä¸‹ã«æŠ‘ãˆã‚‹åˆ¶ç´„ï¼ˆrandom_splitã«é–¢ä¿‚ãªãé©ç”¨ï¼‰â˜…â˜…â˜…
        # ã‚¯ã‚¨ãƒªã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®ã‚¢ã‚¤ãƒ†ãƒ æ•°ã®å·®ã‚’1ä»¥ä¸‹ã«æŠ‘ãˆã‚‹ã‚ˆã†ã«query_countã‚’æ±ºå®š
        base_query_count = total_items // 2
        
        # å¯èƒ½ãªquery_countã®å€™è£œï¼ˆå·®ãŒ1ä»¥ä¸‹ã«ãªã‚‹å€™è£œï¼‰
        possible_query_counts = []
        
        # +-1 ã®ç¯„å›²ã§è©¦ã™
        for qc_candidate in [base_query_count - 1, base_query_count, base_query_count + 1]:
            target_count = total_items - qc_candidate
            # æœ€å°ã‚¢ã‚¤ãƒ†ãƒ æ•°åˆ¶ç´„ã‚’æº€ãŸã—ã€ã‹ã¤ã‚¢ã‚¤ãƒ†ãƒ æ•°ã®å·®ãŒ1ä»¥ä¸‹ã‹ç¢ºèª
            if (self.min_query_items <= qc_candidate <= total_items - self.min_target_items) and \
            (self.min_target_items <= target_count <= total_items - self.min_query_items) and \
            (abs(qc_candidate - target_count) <= 1):  # å·®ãŒ1ä»¥ä¸‹ã®åˆ¶ç´„
                possible_query_counts.append(qc_candidate)
        
        if not possible_query_counts:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æœ€å°åˆ¶ç´„ã‚’æº€ãŸã™ç¯„å›²ã§ã€æœ€ã‚‚å‡ç­‰ã«è¿‘ã„åˆ†å‰²ã‚’é¸æŠ
            min_q_fallback = self.min_query_items
            max_q_fallback = total_items - self.min_target_items
            if min_q_fallback > max_q_fallback:
                raise ValueError(f"Cannot split set of size {total_items} with min_query={self.min_query_items}, min_target={self.min_target_items}")
            
            # æœ€ã‚‚å‡ç­‰ã«è¿‘ã„åˆ†å‰²ã‚’é¸æŠï¼ˆtotal_items // 2 ã«æœ€ã‚‚è¿‘ã„ã‚‚ã®ï¼‰
            best_query_count = min_q_fallback
            best_diff = abs((min_q_fallback) - (total_items - min_q_fallback))
            
            for qc in range(min_q_fallback, max_q_fallback + 1):
                tc = total_items - qc
                diff = abs(qc - tc)
                if diff < best_diff:
                    best_diff = diff
                    best_query_count = qc
            
            query_count = best_query_count
        else:
            if self.random_split:
                # ãƒ©ãƒ³ãƒ€ãƒ åˆ†å‰²ã®å ´åˆï¼šå¯èƒ½ãªå€™è£œã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã«é¸æŠ
                query_count = self.rng.choice(possible_query_counts)
            else:
                # æ±ºå®šè«–çš„åˆ†å‰²ã®å ´åˆï¼šæœ€ã‚‚å‡ç­‰ã«è¿‘ã„å€™è£œã‚’é¸æŠ
                # base_query_countã«æœ€ã‚‚è¿‘ã„å€™è£œã‚’é¸ã¶
                query_count = min(possible_query_counts, key=lambda x: abs(x - base_query_count))

        # â˜…â˜…â˜… ã“ã“ã¾ã§ä¿®æ­£ â˜…â˜…â˜…

        query_indices = indices[:query_count]
        target_indices = indices[query_count:]

        query_set = {k: v[query_indices] for k, v in full_set.items()}
        target_set = {k: v[target_indices] for k, v in full_set.items()}

        # æœ€çµ‚ãƒã‚§ãƒƒã‚¯
        assert len(query_set['features']) >= self.min_query_items
        assert len(target_set['features']) >= self.min_target_items
        
        # å·®ãŒ1ä»¥ä¸‹ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
        query_count_final = len(query_set['features'])
        target_count_final = len(target_set['features'])
        assert abs(query_count_final - target_count_final) <= 1, f"Item count difference too large: query={query_count_final}, target={target_count_final}"
        
        return query_set, target_set

    def __len__(self):
        """1ã‚¨ãƒãƒƒã‚¯ã‚ãŸã‚Šã®ãƒãƒƒãƒæ•°ã‚’è¿”ã™"""
        return int(np.ceil(len(self.full_sets) / self.batch_size))

    def on_epoch_end(self):
        """ã‚¨ãƒãƒƒã‚¯çµ‚äº†æ™‚ã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«"""
        if self.shuffle:
            self.rng.shuffle(self.indexes)
            
    def __getitem__(self, idx):
        """1ãƒãƒƒãƒåˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã™ã‚‹"""
        batch_indices = self.indexes[idx * self.batch_size:(idx + 1) * self.batch_size]
        
        # ãƒãƒƒãƒç”¨ã®é…åˆ—ã‚’åˆæœŸåŒ–
        batch = {
            'query_features': np.zeros((len(batch_indices), self.max_item_num, self.feature_dim), dtype=np.float32),
            'target_features': np.zeros((len(batch_indices), self.max_item_num, self.feature_dim), dtype=np.float32),
            'query_categories': np.zeros((len(batch_indices), self.max_item_num), dtype=np.int32),
            'target_categories': np.zeros((len(batch_indices), self.max_item_num), dtype=np.int32),
            'query_item_ids': np.zeros((len(batch_indices), self.max_item_num), dtype=np.int32),
            'target_item_ids': np.zeros((len(batch_indices), self.max_item_num), dtype=np.int32),
        }
        
        # TPaNegç”¨ã®å€™è£œãƒã‚¬ãƒ†ã‚£ãƒ–ã®åˆæœŸåŒ–ï¼ˆuse_negativesãŒTrueã®å ´åˆã®ã¿ï¼‰
        if self.use_negatives:
            batch['candidate_negative_features'] = np.zeros((len(batch_indices), self.max_item_num, self.candidate_neg_num, self.feature_dim), dtype=np.float32)
            batch['candidate_negative_masks'] = np.zeros((len(batch_indices), self.max_item_num, self.candidate_neg_num), dtype=bool)
            batch['query_candidate_negative_features'] = np.zeros_like(batch['candidate_negative_features'])
            batch['query_candidate_negative_masks'] = np.zeros_like(batch['candidate_negative_masks'])

        if self.include_SetIDs:
            batch['set_ids'] = np.empty(len(batch_indices), dtype=object)

        # å„ã‚·ãƒ¼ãƒ³ã«ã¤ã„ã¦ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
        for i, scene_idx in enumerate(batch_indices):
            full_set = self.full_sets[scene_idx]
            query_set, target_set = self._random_split_set(full_set)

            # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã—ã¦ãƒãƒƒãƒã«æ ¼ç´
            q_len, t_len = len(query_set['ids']), len(target_set['ids'])
            batch['query_features'][i, :q_len] = query_set['features']
            batch['query_categories'][i, :q_len] = query_set['categories']
            batch['query_item_ids'][i, :q_len] = query_set['ids']
            
            batch['target_features'][i, :t_len] = target_set['features']
            batch['target_categories'][i, :t_len] = target_set['categories']
            batch['target_item_ids'][i, :t_len] = target_set['ids']

            if self.include_SetIDs:
                batch['set_ids'][i] = self.scene_ids[scene_idx]

            # TPaNegç”¨ã®å€™è£œãƒã‚¬ãƒ†ã‚£ãƒ–ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—ã—ã€ãƒãƒƒãƒã«åŸ‹ã‚è¾¼ã‚€
            # ã“ã®ãƒ­ã‚¸ãƒƒã‚¯ã¯ãƒãƒƒãƒã‚’åŸ‹ã‚ã‚‹ãƒ«ãƒ¼ãƒ—ã®å†…å´ã«ç§»å‹•
            if self.use_negatives: # <--- å¤–å´ã®if self.use_negatives ãŒ True ã®å ´åˆã®ã¿å®Ÿè¡Œ
                # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ(Y)ã«å¯¾ã™ã‚‹ãƒã‚¬ãƒ†ã‚£ãƒ–å€™è£œã‚’å–å¾—
                for item_idx_in_target_set, item_id in enumerate(target_set['ids']):
                    hard_negative_pool_ids = self.negative_cache_ids.get(str(item_id), []) 
                    neg_ids = []
                    if hard_negative_pool_ids:
                        num_to_sample = min(len(hard_negative_pool_ids), self.candidate_neg_num)
                        sampled_indices = self.rng.choice(len(hard_negative_pool_ids), size=num_to_sample, replace=False)
                        neg_ids = [hard_negative_pool_ids[j] for j in sampled_indices]
                    
                    if neg_ids:
                        sampled_neg_feats = np.array([self.item_feature_map[str(nid)] for nid in neg_ids], dtype=np.float32)
                        current_num_negs = len(sampled_neg_feats)
                        batch['candidate_negative_features'][i, item_idx_in_target_set, :current_num_negs] = sampled_neg_feats
                        batch['candidate_negative_masks'][i, item_idx_in_target_set, :current_num_negs] = True
                
                # ã‚¯ã‚¨ãƒª(X)ã«å¯¾ã™ã‚‹ãƒã‚¬ãƒ†ã‚£ãƒ–å€™è£œã‚’å–å¾—
                for item_idx_in_query_set, item_id in enumerate(query_set['ids']):
                    hard_negative_pool_ids = self.negative_cache_ids.get(str(item_id), []) 
                    neg_ids = []
                    if hard_negative_pool_ids:
                        num_to_sample = min(len(hard_negative_pool_ids), self.candidate_neg_num)
                        sampled_indices = self.rng.choice(len(hard_negative_pool_ids), size=num_to_sample, replace=False)
                        neg_ids = [hard_negative_pool_ids[j] for j in sampled_indices]
                    
                    if neg_ids:
                        sampled_neg_feats = np.array([self.item_feature_map[str(nid)] for nid in neg_ids], dtype=np.float32)
                        current_num_negs = len(sampled_neg_feats)
                        batch['query_candidate_negative_features'][i, item_idx_in_query_set, :current_num_negs] = sampled_neg_feats
                        batch['query_candidate_negative_masks'][i, item_idx_in_query_set, :current_num_negs] = True


        # â˜…â˜…â˜… ä¿®æ­£ã•ã‚ŒãŸãƒ›ãƒ¯ã‚¤ãƒˆãƒ‹ãƒ³ã‚°å‡¦ç† â˜…â˜…â˜…
        if self.whitening_params:
            # 3æ¬¡å…ƒé…åˆ—ç”¨ã®ãƒ›ãƒ¯ã‚¤ãƒˆãƒ‹ãƒ³ã‚°ï¼ˆquery_features, target_featuresï¼‰
            for key in ['query_features', 'target_features']:
                if key in batch:
                    # (B, S, D) å½¢çŠ¶ã®3æ¬¡å…ƒé…åˆ—
                    data = batch[key]
                    batch_size, seq_len, feature_dim = data.shape
                    
                    # æœ‰åŠ¹ãªï¼ˆéã‚¼ãƒ­ï¼‰è¦ç´ ã®ãƒã‚¹ã‚¯ã‚’ä½œæˆ
                    valid_mask = np.sum(np.abs(data), axis=-1) > 0  # (B, S)
                    
                    if valid_mask.any():
                        # æœ‰åŠ¹ãªè¦ç´ ã®ã¿ã‚’å¹³å¦åŒ–ã—ã¦ãƒ›ãƒ¯ã‚¤ãƒˆãƒ‹ãƒ³ã‚°é©ç”¨
                        valid_data = data[valid_mask]  # (N_valid, D)
                        whitened_data = np.dot(valid_data - self.whitening_mean, self.whitening_matrix)
                        
                        # å…ƒã®é…åˆ—ã«æ›¸ãæˆ»ã—
                        data[valid_mask] = whitened_data
                        batch[key] = data
        
        # ãƒ†ã‚¹ãƒˆç”¨ã« set_ids ã‚’é©åˆ‡ãªStringå‹ã«å¤‰æ›
        if self.include_SetIDs:
            batch['set_ids'] = np.array(batch['set_ids'], dtype=str)

        # æœ€çµ‚çš„ã«ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›ã—ã¦è¿”ã™ï¼ˆNumPyã®ã¾ã¾æ¸¡ã—ã¦ã‚‚TFãŒå¤‰æ›ã—ã¦ãã‚Œã‚‹ãŒã€æ˜ç¤ºçš„ã«è¡Œã†ã®ãŒå®‰å…¨ï¼‰
        final_batch = {}
        for key, value in batch.items():
            # TF.data.Datasetã¯stringå‹ã®numpy arrayã‚’ãã®ã¾ã¾å‡¦ç†ã§ããªã„å ´åˆãŒã‚ã‚‹ãŸã‚ã€
            # set_idsã¯objectå‹ã€ãã‚Œä»¥å¤–ã¯Tensorã«å¤‰æ›
            if key == 'set_ids' and self.include_SetIDs:
                final_batch[key] = value # object dtype array
            else:
                final_batch[key] = tf.convert_to_tensor(value) 
        
        return final_batch