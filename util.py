"""
util.py - Core evaluation logic for Set Retrieval.
"""
import os
import gc
from typing import Dict, List, Tuple, Optional, Any
from collections import defaultdict
import numpy as np
import random
import tensorflow as tf
try:
    from tqdm import tqdm
except ImportError:
    def tqdm(iterator, *args, **kwargs): return iterator

from config import get_dataset_config
from results import save_evaluation_results, display_evaluation_results
from plot import generate_all_visualizations

def setup_gpu_memory():
    gpus = tf.config.experimental.list_physical_devices('GPU')
    if gpus:
        try:
            for gpu in gpus: tf.config.experimental.set_memory_growth(gpu, True)
        except RuntimeError as e: print(f"GPU setup error: {e}")

def clear_memory():
    gc.collect()
    if tf.config.list_physical_devices('GPU'):
        try: tf.keras.backend.clear_session()
        except: pass

def detect_dataset_type(test_data) -> str:
    try:
        sample_batch = next(iter(test_data.take(1)))
        if 'target_categories' in sample_batch:
            categories = sample_batch['target_categories']
            if hasattr(categories, 'numpy'): categories = categories.numpy()
            max_cat = int(np.max(categories[categories > 0])) if np.any(categories > 0) else 0
            # IQON3000ã®ã‚«ãƒ†ã‚´ãƒªæ•°16ã¨DeepFurnitureã®ã‚«ãƒ†ã‚´ãƒªæ•°11ã‚’è€ƒæ…®
            if max_cat <= len(IQON3000_CATEGORIES): # 16ä»¥ä¸‹ãªã‚‰IQON3000ã¨åˆ¤å®š
                return 'IQON3000'
            elif max_cat <= len(DEEPFURNITURE_CATEGORIES): # 11ä»¥ä¸‹ãªã‚‰DeepFurnitureã¨åˆ¤å®š (ã“ã®é †åºãŒé‡è¦)
                return 'DeepFurniture'
            else:
                return 'DeepFurniture' # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯DeepFurniture
    except Exception as e:
        print(f"[WARN] Could not detect dataset type: {e}")
    return 'IQON3000' # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ


def collect_test_data(test_data) -> Tuple[List[Dict], Dict, str]:
    """ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿åé›†ï¼ˆDataGeneratorã‹ã‚‰ç›´æ¥å–å¾—ï¼‰"""
    print("[INFO] ğŸ“¥ Collecting test data and building gallery...")
    
    # â˜…â˜…â˜… ä¿®æ­£: DataGeneratorã‹ã‚‰ç›´æ¥å–å¾—ã‚’å„ªå…ˆ â˜…â˜…â˜…
    if hasattr(test_data, '_data_generator'):
        print("[INFO] Using DataGenerator for test data collection...")
        data_gen = test_data._data_generator
        dataset_type = data_gen.dataset_name or 'Unknown'
        
        # DataGeneratorã‹ã‚‰ç›´æ¥å…¨ãƒãƒƒãƒã‚’å–å¾—
        all_batches = []
        print(f"[INFO] Collecting {len(data_gen)} batches from DataGenerator...")
        for i in range(len(data_gen)):
            batch = data_gen[i]  # DataGeneratorã‹ã‚‰ç›´æ¥å–å¾—ï¼ˆset_idsã‚’å«ã‚€ï¼‰
            if batch is not None:
                all_batches.append(batch)
                
    else:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼štf.data.DatasetçµŒç”±
        print("[INFO] Using tf.data.Dataset iterator...")
        dataset_type = detect_dataset_type(test_data)
        all_batches = list(tqdm(test_data.as_numpy_iterator(), desc="Collecting batches"))
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®šã‚’å–å¾—
    config = get_dataset_config(dataset_type)
    min_cat, max_cat = config['category_range']
    
    test_items = []
    gallery_by_category = defaultdict(dict)
    
    print("[INFO] Processing sets and building gallery...")
    for batch in tqdm(all_batches, desc="Processing batches"):
        batch_size = len(batch['query_features'])
        
        # â˜…â˜…â˜… ãƒ‡ãƒãƒƒã‚°: ãƒãƒƒãƒã«å«ã¾ã‚Œã‚‹ã‚­ãƒ¼ã‚’ç¢ºèª â˜…â˜…â˜…
        if len(test_items) == 0:  # æœ€åˆã®ãƒãƒƒãƒã§ã®ã¿è¡¨ç¤º
            print(f"[DEBUG] Available keys in batch: {list(batch.keys())}")
        
        for i in range(batch_size):
            # åŸºæœ¬çš„ãªã‚¢ã‚¤ãƒ†ãƒ ã‚»ãƒƒãƒˆã‚’ä½œæˆ
            item_set = {
                'query_features': batch['query_features'][i],
                'target_features': batch['target_features'][i],
                'query_categories': batch['query_categories'][i],
                'target_categories': batch['target_categories'][i],
                'query_item_ids': batch['query_item_ids'][i],
                'target_item_ids': batch['target_item_ids'][i],
            }
            
            # â˜…â˜…â˜… ä¿®æ­£: set_idsã®å®‰å…¨ãªå–å¾— â˜…â˜…â˜…
            if 'set_ids' in batch:
                set_id_raw = batch['set_ids'][i]
                if isinstance(set_id_raw, bytes):
                    item_set['set_id'] = set_id_raw.decode('utf-8')
                elif isinstance(set_id_raw, (np.str_, np.bytes_)):
                    item_set['set_id'] = str(set_id_raw)
                else:
                    item_set['set_id'] = str(set_id_raw)
            else:
                # set_idsãŒãªã„å ´åˆã¯ä»£æ›¿IDã‚’ç”Ÿæˆ
                item_set['set_id'] = f"test_set_{len(test_items)}"
                if len(test_items) == 0:
                    print("[WARNING] set_ids not found in batch, using generated IDs")
            
            test_items.append(item_set)

            # ã‚®ãƒ£ãƒ©ãƒªãƒ¼ã‚’æ§‹ç¯‰
            for feat, cat, item_id in zip(item_set['target_features'], item_set['target_categories'], item_set['target_item_ids']):
                cat_int = int(cat)
                if min_cat <= cat_int <= max_cat and np.any(feat):
                    gallery_by_category[cat_int][str(item_id)] = feat.astype(np.float32)

    # ã‚®ãƒ£ãƒ©ãƒªãƒ¼ã®æœ€çµ‚å‡¦ç†
    for cat_id in list(gallery_by_category.keys()):
        items = gallery_by_category[cat_id]
        if items:
            gallery_by_category[cat_id] = {
                'ids': np.array(list(items.keys())), 
                'features': np.array(list(items.values()))
            }
    
    print(f"âœ… Collected {len(test_items)} test items and {len(gallery_by_category)} categories")
    return test_items, dict(gallery_by_category), dataset_type


def collect_training_items_by_category(train_data_path):
    """å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å„ã‚«ãƒ†ã‚´ãƒªã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’åé›†"""
    
    with open(train_data_path, 'rb') as f:
        data = pickle.load(f)
    
    query_features = np.array(data[0], dtype=np.float32)
    positive_features = np.array(data[1], dtype=np.float32)
    query_categories = np.array(data[3], dtype=np.int32)
    positive_categories = np.array(data[4], dtype=np.int32)
    
    # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚¢ã‚¤ãƒ†ãƒ è¾æ›¸
    items_by_category = defaultdict(list)
    
    # å…¨ã‚·ãƒ¼ãƒ³ã‹ã‚‰å…¨ã‚¢ã‚¤ãƒ†ãƒ ã‚’åé›†
    for scene_idx in range(len(query_features)):
        # queryã‚¢ã‚¤ãƒ†ãƒ 
        for item_idx in range(len(query_features[scene_idx])):
            if query_categories[scene_idx, item_idx] > 0:
                cat = int(query_categories[scene_idx, item_idx])
                feat = query_features[scene_idx, item_idx]
                item_id = f"q_{scene_idx}_{item_idx}"
                items_by_category[cat].append({
                    'id': item_id,
                    'features': feat,
                    'scene_idx': scene_idx,
                    'item_idx': item_idx,
                    'type': 'query'
                })
        
        # positiveã‚¢ã‚¤ãƒ†ãƒ 
        for item_idx in range(len(positive_features[scene_idx])):
            if positive_categories[scene_idx, item_idx] > 0:
                cat = int(positive_categories[scene_idx, item_idx])
                feat = positive_features[scene_idx, item_idx]
                item_id = f"p_{scene_idx}_{item_idx}"
                items_by_category[cat].append({
                    'id': item_id,
                    'features': feat,
                    'scene_idx': scene_idx,
                    'item_idx': item_idx,
                    'type': 'positive'
                })
    
    return items_by_category



# util.py ã«è¿½åŠ ã™ã‚‹é–¢æ•°ç¾¤

class HardNegativeMiner:
    """é«˜é€ŸåŒ–ã•ã‚ŒãŸHard Negativeäº‹å‰è¨ˆç®—ã¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†"""
    
    def __init__(self, train_data_path, whitening_params=None, 
                 max_items_per_category=1500, progress_interval=100, 
                 max_negatives_per_item=15):
        self.whitening_params = whitening_params
        self.max_items_per_category = max_items_per_category
        self.progress_interval = progress_interval
        self.max_negatives_per_item = max_negatives_per_item
        
        self.items_by_category = {}
        self.hard_negatives_cache = {}
        self.scheduler = CurriculumScheduler()
        
        print(f"ğŸš€ Starting Optimized Hard Negative computation...")
        print(f"   Max items per category: {max_items_per_category}")
        print(f"   Max negatives per item: {max_negatives_per_item}")
        print(f"   Progress interval: {progress_interval}")
        
        # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰äº‹å‰è¨ˆç®—å®Ÿè¡Œ
        self._precompute_from_training_data(train_data_path)
        
        print(f"âœ… Optimized Hard Negative mining completed!")
        print(f"   Categories: {len(self.items_by_category)}")
        print(f"   Cached relationships: {len(self.hard_negatives_cache):,}")
    
    def _precompute_from_training_data(self, train_data_path):
        """å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å…¨Hard Negativeé–¢ä¿‚ã‚’äº‹å‰è¨ˆç®—"""
        # Step 1: å­¦ç¿’ãƒ‡ãƒ¼ã‚¿åé›†ï¼ˆé«˜é€Ÿç‰ˆï¼‰
        print("  Step 1: Fast data collection with sampling...")
        
        self.items_by_category = collect_training_items_by_category_fast(
            train_data_path, self.max_items_per_category
        )
        
        # Step 2: ãƒ›ãƒ¯ã‚¤ãƒˆãƒ‹ãƒ³ã‚°é©ç”¨
        if self.whitening_params:
            print("  Step 2: Applying whitening...")
            self._apply_whitening_to_items()
        
        # Step 3: é«˜é€Ÿé¡ä¼¼åº¦è¨ˆç®—
        print("  Step 3: Fast similarity computation...")
        self._compute_all_similarities_fast()
    
    def _apply_whitening_to_items(self):
        """ãƒ›ãƒ¯ã‚¤ãƒˆãƒ‹ãƒ³ã‚°é©ç”¨ï¼ˆãƒãƒƒãƒå‡¦ç†ï¼‰"""
        for cat_id, items in self.items_by_category.items():
            if len(items) > 0:
                print(f"    Applying whitening to category {cat_id}: {len(items)} items")
                
                # ãƒãƒƒãƒå‡¦ç†ã§ãƒ›ãƒ¯ã‚¤ãƒˆãƒ‹ãƒ³ã‚°
                features = np.array([item['features'] for item in items])
                whitened_features = np.dot(features - self.whitening_params['mean'], 
                                         self.whitening_params['matrix'])
                
                # çµæœã‚’å…ƒã®ã‚¢ã‚¤ãƒ†ãƒ ã«æ›¸ãæˆ»ã—
                for i, item in enumerate(items):
                    item['features'] = whitened_features[i]
    
    def _compute_all_similarities_fast(self):
        """é«˜é€ŸåŒ–ã•ã‚ŒãŸé¡ä¼¼åº¦è¨ˆç®—"""
        total_processed = 0
        
        for cat_id, items in self.items_by_category.items():
            if len(items) < 2:
                continue
            
            print(f"    Processing category {cat_id}: {len(items)} items")
            
            # â˜… ä»£è¡¨çš„ãªã‚¢ã‚¤ãƒ†ãƒ ã®ã¿å‡¦ç†ï¼ˆã•ã‚‰ãªã‚‹é«˜é€ŸåŒ–ï¼‰
            if len(items) > 1000:
                # å¤§ããªã‚«ãƒ†ã‚´ãƒªã¯ä»£è¡¨ã‚µãƒ³ãƒ—ãƒ«ã®ã¿å‡¦ç†
                sample_size = min(500, len(items) // 3)
                sample_indices = np.random.choice(len(items), sample_size, replace=False)
                process_items = [items[i] for i in sample_indices]
                print(f"      Large category detected, processing {len(process_items)} representative items")
            else:
                process_items = items
            
            # å„ã‚¢ã‚¤ãƒ†ãƒ ã®Hard Negativeè¨ˆç®—
            for item in process_items:
                hard_negatives = compute_hard_negatives_for_item_fast(
                    item, items, 
                    similarity_threshold=0.1,  # æœ€ä½é–¾å€¤
                    max_negatives=self.max_negatives_per_item
                )
                
                self.hard_negatives_cache[item['id']] = hard_negatives
                total_processed += 1
                
                # â˜… é«˜é »åº¦é€²æ—è¡¨ç¤º
                if total_processed % self.progress_interval == 0:
                    print(f"      Processed {total_processed:,} items...")
            
            # ãƒ¡ãƒ¢ãƒªè§£æ”¾
            if len(items) > 1000:
                # å¤§ããªã‚«ãƒ†ã‚´ãƒªã®å‡¦ç†å¾Œã¯ãƒ¡ãƒ¢ãƒªè§£æ”¾
                gc.collect()
        
        print(f"    Total processed items: {total_processed:,}")
    
    def get_hard_negatives_for_item(self, item_id, epoch, max_samples=60):
        """æŒ‡å®šã‚¢ã‚¤ãƒ†ãƒ ãƒ»ã‚¨ãƒãƒƒã‚¯ã®Hard Negativeã‚’å–å¾—"""
        current_threshold = self.scheduler.get_threshold(epoch)
        all_hard_negatives = self.hard_negatives_cache.get(item_id, [])
        
        # ç¾åœ¨ã‚¨ãƒãƒƒã‚¯ã«é©ã—ãŸå›°é›£åº¦ã®ã‚‚ã®ã‚’é¸æŠ
        epoch_hard_negatives = [
            hn for hn in all_hard_negatives 
            if hn['similarity'] >= current_threshold
        ]
        
        # ä¸è¶³ã—ã¦ã„ã‚‹å ´åˆã¯ä½ã„é–¾å€¤ã®ã‚‚ã®ã‚‚è¿½åŠ 
        if len(epoch_hard_negatives) < max_samples // 2 and len(all_hard_negatives) > 0:
            remaining = max_samples - len(epoch_hard_negatives)
            lower_threshold_negatives = [
                hn for hn in all_hard_negatives 
                if hn['similarity'] < current_threshold
            ]
            if lower_threshold_negatives:
                additional = random.sample(
                    lower_threshold_negatives, 
                    min(remaining, len(lower_threshold_negatives))
                )
                epoch_hard_negatives.extend(additional)
        
        return epoch_hard_negatives[:max_samples]


def save_hard_negative_cache(cache_obj, output_path):
    """Hard Negativeã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜"""
    if hasattr(cache_obj, 'hard_negatives_cache'):
        # æ–°å½¢å¼ï¼ˆã‚¯ãƒ©ã‚¹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆï¼‰
        save_data = {
            'hard_negatives_cache': cache_obj.hard_negatives_cache,
            'scheduler': cache_obj.scheduler,
            'max_items_per_category': getattr(cache_obj, 'max_items_per_category', 3000),
            'max_negatives_per_item': getattr(cache_obj, 'max_negatives_per_item', 30)
        }
    else:
        # æ—§å½¢å¼ï¼ˆè¾æ›¸ï¼‰
        save_data = cache_obj
    
    with open(output_path, 'wb') as f:
        pickle.dump(save_data, f)
    
    if hasattr(cache_obj, 'hard_negatives_cache'):
        print(f"[INFO] Saved {len(cache_obj.hard_negatives_cache):,} hard negative relationships")
    else:
        print(f"[INFO] Saved hard negative cache")


def compute_hard_negatives_for_item(positive_item, category_items, similarity_threshold=0.3):
    """æŒ‡å®šã‚¢ã‚¤ãƒ†ãƒ ã«å¯¾ã™ã‚‹Hard Negativeã‚’è¨ˆç®—"""
    
    positive_features = positive_item['features']
    hard_negatives = []
    
    for candidate_item in category_items:
        # è‡ªåˆ†è‡ªèº«ã¯é™¤å¤–
        if candidate_item['id'] == positive_item['id']:
            continue
        
        # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—
        candidate_features = candidate_item['features']
        similarity = np.dot(positive_features, candidate_features) / (
            np.linalg.norm(positive_features) * np.linalg.norm(candidate_features)
        )
        
        # é–¾å€¤ä»¥ä¸Šã‚’Hard Negativeã¨ã—ã¦é¸æŠ
        if similarity >= similarity_threshold:
            hard_negatives.append({
                'item': candidate_item,
                'similarity': similarity
            })
    
    # é¡ä¼¼åº¦é™é †ã§ã‚½ãƒ¼ãƒˆ
    hard_negatives.sort(key=lambda x: x['similarity'], reverse=True)
    
    return hard_negatives

    
class CurriculumScheduler:
    """æ®µéšçš„å›°é›£åº¦èª¿æ•´"""
    
    def __init__(self, init_threshold=0.3, max_threshold=0.8, increment=0.05):
        self.init_threshold = init_threshold
        self.max_threshold = max_threshold
        self.increment = increment
    
    def get_threshold(self, epoch):
        return min(self.max_threshold, self.init_threshold + epoch * self.increment)

def compute_similarities_vectorized(features_a, features_b, batch_size=2500):
    """
    ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã•ã‚ŒãŸé«˜é€Ÿé¡ä¼¼åº¦è¨ˆç®—
    
    Args:
        features_a: (N, D) ç‰¹å¾´é‡é…åˆ—A
        features_b: (M, D) ç‰¹å¾´é‡é…åˆ—B
        batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º
    
    Returns:
        similarities: (N, M) é¡ä¼¼åº¦è¡Œåˆ—
    """
    N, D = features_a.shape
    M = features_b.shape[0]
    
    # æ­£è¦åŒ–
    features_a_norm = features_a / (np.linalg.norm(features_a, axis=1, keepdims=True) + 1e-8)
    features_b_norm = features_b / (np.linalg.norm(features_b, axis=1, keepdims=True) + 1e-8)
    
    # GPUä½¿ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯
    if tf.config.list_physical_devices('GPU') and N > 500 and M > 500:
        try:
            # GPUç‰ˆï¼šãƒãƒƒãƒå‡¦ç†
            similarities = np.zeros((N, M), dtype=np.float32)
            
            for i in range(0, N, batch_size):
                end_i = min(i + batch_size, N)
                batch_a = tf.constant(features_a_norm[i:end_i])
                batch_sim = tf.matmul(batch_a, tf.constant(features_b_norm), transpose_b=True)
                similarities[i:end_i] = batch_sim.numpy()
            
            return similarities
        except:
            # GPUè¨ˆç®—å¤±æ•—æ™‚ã¯CPUã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            pass
    
    # CPUç‰ˆï¼šNumPyï¼ˆé«˜é€Ÿï¼‰
    return np.dot(features_a_norm, features_b_norm.T)

def collect_training_items_by_category_fast(train_data_path, max_items_per_category=2000):
    """é«˜é€ŸåŒ–ã•ã‚ŒãŸå­¦ç¿’ãƒ‡ãƒ¼ã‚¿åé›†ï¼ˆã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ä»˜ãï¼‰"""
    
    with open(train_data_path, 'rb') as f:
        data = pickle.load(f)
    
    query_features = np.array(data[0], dtype=np.float32)
    positive_features = np.array(data[1], dtype=np.float32)
    query_categories = np.array(data[3], dtype=np.int32)
    positive_categories = np.array(data[4], dtype=np.int32)
    
    # â˜… ä¿®æ­£ï¼šå®Ÿéš›ã®ã‚¢ã‚¤ãƒ†ãƒ IDã‚’ä½¿ç”¨
    query_item_ids = np.array(data[6])      # å®Ÿéš›ã®ID
    positive_item_ids = np.array(data[7])   # å®Ÿéš›ã®ID

    # ä¸€æ™‚çš„ãªåé›†ç”¨è¾æ›¸
    temp_features = defaultdict(list)
    temp_metadata = defaultdict(list)
    
    print("  Collecting items by category...")
    
    # å…¨ã‚·ãƒ¼ãƒ³ã‹ã‚‰å…¨ã‚¢ã‚¤ãƒ†ãƒ ã‚’åé›†
    for scene_idx in range(len(query_features)):
        # queryã‚¢ã‚¤ãƒ†ãƒ 
        for item_idx in range(len(query_features[scene_idx])):
            if query_categories[scene_idx, item_idx] > 0:
                cat = int(query_categories[scene_idx, item_idx])
                feat = query_features[scene_idx, item_idx]
                item_id = str(query_item_ids[scene_idx, item_idx])
                
                temp_features[cat].append(feat)
                temp_metadata[cat].append({
                    'id': item_id, 'scene_idx': scene_idx, 'item_idx': item_idx, 'type': 'query'
                })
        
        # positiveã‚¢ã‚¤ãƒ†ãƒ 
        for item_idx in range(len(positive_features[scene_idx])):
            if positive_categories[scene_idx, item_idx] > 0:
                cat = int(positive_categories[scene_idx, item_idx])
                feat = positive_features[scene_idx, item_idx]
                item_id = str(positive_item_ids[scene_idx, item_idx])
                
                temp_features[cat].append(feat)
                temp_metadata[cat].append({
                    'id': item_id, 'scene_idx': scene_idx, 'item_idx': item_idx, 'type': 'positive'
                })
    
    # â˜… ã‚¹ãƒãƒ¼ãƒˆã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é©ç”¨
    items_by_category = {}
    
    for cat_id, features in temp_features.items():
        original_count = len(features)
        
        if original_count > max_items_per_category:
            print(f"    Category {cat_id}: Sampling {max_items_per_category} from {original_count} items")
            
            # ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ã§é«˜é€Ÿï¼‰
            selected_indices = np.random.choice(
                original_count, max_items_per_category, replace=False
            )
            
            # é¸æŠã•ã‚ŒãŸã‚¢ã‚¤ãƒ†ãƒ ã®ã¿ä¿æŒ
            sampled_items = []
            for idx in selected_indices:
                sampled_items.append({
                    'id': temp_metadata[cat_id][idx]['id'],
                    'features': features[idx],
                    'scene_idx': temp_metadata[cat_id][idx]['scene_idx'],
                    'item_idx': temp_metadata[cat_id][idx]['item_idx'],
                    'type': temp_metadata[cat_id][idx]['type']
                })
            
            items_by_category[cat_id] = sampled_items
        else:
            # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ä¸è¦
            full_items = []
            for idx, feat in enumerate(features):
                full_items.append({
                    'id': temp_metadata[cat_id][idx]['id'],
                    'features': feat,
                    'scene_idx': temp_metadata[cat_id][idx]['scene_idx'],
                    'item_idx': temp_metadata[cat_id][idx]['item_idx'],
                    'type': temp_metadata[cat_id][idx]['type']
                })
            
            items_by_category[cat_id] = full_items
        
        print(f"    Category {cat_id}: {len(items_by_category[cat_id])} items (from {original_count})")
    
    return items_by_category

def compute_hard_negatives_for_item_fast(positive_item, category_items, similarity_threshold=0.3, max_negatives=50):
    """é«˜é€ŸåŒ–ã•ã‚ŒãŸHard Negativeè¨ˆç®—"""
    
    if len(category_items) <= 1:
        return []
    
    positive_features = positive_item['features']
    positive_id = positive_item['id']
    
    # å€™è£œç‰¹å¾´é‡ã‚’æŠ½å‡ºï¼ˆè‡ªåˆ†ä»¥å¤–ï¼‰
    candidate_features = []
    candidate_items = []
    
    for item in category_items:
        if item['id'] != positive_id:
            candidate_features.append(item['features'])
            candidate_items.append(item)
    
    if len(candidate_features) == 0:
        return []
    
    candidate_features = np.array(candidate_features)
    
    # ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã•ã‚ŒãŸã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—
    positive_norm = positive_features / (np.linalg.norm(positive_features) + 1e-8)
    candidate_norms = candidate_features / (np.linalg.norm(candidate_features, axis=1, keepdims=True) + 1e-8)
    similarities = np.dot(candidate_norms, positive_norm)
    
    # é–¾å€¤ä»¥ä¸Šã®ã‚‚ã®ã®ã¿é¸æŠ
    valid_mask = similarities >= similarity_threshold
    valid_indices = np.where(valid_mask)[0]
    valid_similarities = similarities[valid_mask]
    
    if len(valid_indices) == 0:
        return []
    
    # Top-Ké¸æŠï¼ˆé¡ä¼¼åº¦é™é †ï¼‰
    k = min(max_negatives, len(valid_indices))
    top_k_indices = np.argsort(valid_similarities)[::-1][:k]
    
    hard_negatives = []
    for i in top_k_indices:
        original_idx = valid_indices[i]
        hard_negatives.append({
            'item': candidate_items[original_idx],
            'similarity': float(valid_similarities[i])
        })
    
    return hard_negatives


def load_hard_negative_cache(cache_path):
    """Hard Negativeã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿"""
    if os.path.exists(cache_path):
        with open(cache_path, 'rb') as f:
            save_data = pickle.load(f)
        
        # æ–°å½¢å¼ã®å ´åˆ
        if isinstance(save_data, dict) and 'hard_negatives_cache' in save_data:
            class CacheWrapper:
                def __init__(self, data):
                    self.hard_negatives_cache = data['hard_negatives_cache']
                    self.scheduler = data.get('scheduler', CurriculumScheduler())
                    self.max_items_per_category = data.get('max_items_per_category', 3000)
                    self.max_negatives_per_item = data.get('max_negatives_per_item', 30)
                
                def get_hard_negatives_for_item(self, item_id, epoch, max_samples=60):
                    current_threshold = self.scheduler.get_threshold(epoch)
                    all_hard_negatives = self.hard_negatives_cache.get(item_id, [])
                    
                    epoch_hard_negatives = [
                        hn for hn in all_hard_negatives 
                        if hn['similarity'] >= current_threshold
                    ]
                    
                    # ä¸è¶³åˆ†ã‚’è£œå®Œ
                    if len(epoch_hard_negatives) < max_samples // 2 and len(all_hard_negatives) > 0:
                        remaining = max_samples - len(epoch_hard_negatives)
                        lower_threshold_negatives = [
                            hn for hn in all_hard_negatives 
                            if hn['similarity'] < current_threshold
                        ]
                        if lower_threshold_negatives:
                            additional = random.sample(
                                lower_threshold_negatives, 
                                min(remaining, len(lower_threshold_negatives))
                            )
                            epoch_hard_negatives.extend(additional)
                    
                    return epoch_hard_negatives[:max_samples]
            
            cache_obj = CacheWrapper(save_data)
            print(f"[INFO] Loaded {len(cache_obj.hard_negatives_cache):,} hard negative relationships")
            return cache_obj
        else:
            # æ—§å½¢å¼ï¼ˆå¾Œæ–¹äº’æ›æ€§ï¼‰
            return save_data
    
    return None

def compute_training_whitening_params(train_generator, model, num_categories=7):
    """
    è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒ›ãƒ¯ã‚¤ãƒˆãƒ‹ãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨ˆç®—
    
    Args:
        train_generator: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿
        model: å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
        num_categories: ã‚«ãƒ†ã‚´ãƒªæ•°
    
    Returns:
        ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒ›ãƒ¯ã‚¤ãƒˆãƒ‹ãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    """
    print("[INFO] Computing whitening parameters from TRAINING data...")
    
    # ã‚«ãƒ†ã‚´ãƒªåˆ¥ç‰¹å¾´é‡åé›†
    category_embeddings = {cat_id: [] for cat_id in range(1, num_categories + 1)}
    
    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç‰¹å¾´é‡åé›†
    batch_count = 0
    for batch in train_generator.take(100):  # æœ€åˆã®100ãƒãƒƒãƒä½¿ç”¨
        query_features = batch['query_features']
        query_categories = batch['query_categories']
        target_features = batch['target_features']
        target_categories = batch['target_categories']
        
        # ãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬ï¼ˆå„ã‚«ãƒ†ã‚´ãƒªã®åŸ‹ã‚è¾¼ã¿ã‚’å–å¾—ï¼‰
        predictions = model({'query_features': query_features}, training=False)
        
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç‰¹å¾´é‡ã‹ã‚‰å„ã‚«ãƒ†ã‚´ãƒªã®åŸ‹ã‚è¾¼ã¿ã‚’åé›†
        for sample_idx in range(target_features.shape[0]):
            for item_idx in range(target_features.shape[1]):
                cat_id = int(target_categories[sample_idx, item_idx])
                if 1 <= cat_id <= num_categories:
                    # ã“ã®ç‰¹å¾´é‡ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ãŒç”Ÿæˆã™ã‚‹ã§ã‚ã‚ã†åŸ‹ã‚è¾¼ã¿
                    item_features = target_features[sample_idx, item_idx:item_idx+1]
                    item_embedding = model.infer_single_set(item_features)[cat_id - 1]
                    category_embeddings[cat_id].append(item_embedding.numpy())
        
        batch_count += 1
        if batch_count % 20 == 0:
            print(f"  Processed {batch_count} training batches...")
    
    # ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒ›ãƒ¯ã‚¤ãƒˆãƒ‹ãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨ˆç®—
    category_whitening_params = {}
    
    for cat_id in range(1, num_categories + 1):
        if cat_id in category_embeddings and len(category_embeddings[cat_id]) > 0:
            embeddings = np.array(category_embeddings[cat_id])
            mean_vec, whitening_matrix = compute_whitening_params(embeddings)
            category_whitening_params[cat_id] = {
                'mean': mean_vec, 
                'matrix': whitening_matrix
            }
            print(f"  Category {cat_id}: {len(embeddings)} training samples")
        else:
            print(f"  Category {cat_id}: No training samples, skipping")
    
    print(f"[INFO] Training-based whitening parameters computed for {len(category_whitening_params)} categories")
    return category_whitening_params


def save_whitening_params(whitening_params, filepath):
    """ãƒ›ãƒ¯ã‚¤ãƒˆãƒ‹ãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
    import pickle
    with open(filepath, 'wb') as f:
        pickle.dump(whitening_params, f)
    print(f"[INFO] Whitening parameters saved to {filepath}")


def load_whitening_params(filepath):
    """ãƒ›ãƒ¯ã‚¤ãƒˆãƒ‹ãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
    import pickle
    with open(filepath, 'rb') as f:
        whitening_params = pickle.load(f)
    print(f"[INFO] Whitening parameters loaded from {filepath}")
    return whitening_params


def compute_whitening_params(features, epsilon=1e-6):
    """ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ›ãƒ¯ã‚¤ãƒˆãƒ‹ãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨ˆç®—"""
    mean_vec = np.mean(features, axis=0)
    centered = features - mean_vec
    cov_matrix = np.cov(centered.T)
    eigenvals, eigenvecs = np.linalg.eigh(cov_matrix)
    # æ•°å€¤å®‰å®šåŒ–ï¼š0ã«è¿‘ã„å›ºæœ‰å€¤ã‚’ä¿®æ­£
    eigenvals = np.maximum(eigenvals, epsilon)
    whitening_matrix = eigenvecs @ np.diag(1.0 / np.sqrt(eigenvals)) @ eigenvecs.T
    return mean_vec, whitening_matrix

def apply_whitening(features, mean_vec, whitening_matrix):
    """ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ›ãƒ¯ã‚¤ãƒˆãƒ‹ãƒ³ã‚°é©ç”¨"""
    centered = features - mean_vec
    whitened = centered @ whitening_matrix.T
    norms = np.linalg.norm(whitened, axis=1, keepdims=True)
    return whitened / np.maximum(norms, 1e-8)

# def compute_training_whitening_stats(model, train_dataset):
#     """å­¦ç¿’ãƒ‡ãƒ¼ã‚¿å…¨ä½“ã‹ã‚‰çµ±è¨ˆé‡è¨ˆç®—"""
#     print("[INFO] Computing whitening statistics from ALL training data...")
    
#     all_embeddings = []
#     batch_count = 0
    
#     for batch in train_dataset:
#         # ãƒãƒƒãƒå…¨ä½“ã‚’ä¸€åº¦ã«æ¨è«–
#         predictions = model({'query_features': batch['query_features']}, training=False)
#         # å…¨ã‚«ãƒ†ã‚´ãƒªã®åŸ‹ã‚è¾¼ã¿ã‚’åé›†ï¼ˆå½¢çŠ¶: [batch_size, num_categories, embed_dim]ï¼‰
#         batch_embeddings = predictions.numpy().reshape(-1, predictions.shape[-1])
#         all_embeddings.append(batch_embeddings)
        
#         batch_count += 1
#         if batch_count % 10 == 0:  # 10ãƒãƒƒãƒã”ã¨ã«é€²æ—è¡¨ç¤º
#             print(f"  Processed {batch_count} batches...")
    
#     # å…¨åŸ‹ã‚è¾¼ã¿ã‚’çµåˆ
#     all_embeddings = np.concatenate(all_embeddings, axis=0)
#     print(f"  Collected {len(all_embeddings):,} embeddings for whitening from {batch_count} batches")
    
#     # çµ±è¨ˆé‡è¨ˆç®—
#     mean_vec, whitening_matrix = compute_whitening_params(all_embeddings)
    
#     return {'mean': mean_vec, 'matrix': whitening_matrix}


# def apply_whitening_to_evaluation(model, test_items, gallery_by_category, whitening_params):
#     """ãƒ†ã‚¹ãƒˆæ™‚ã«ãƒ›ãƒ¯ã‚¤ãƒˆãƒ‹ãƒ³ã‚°é©ç”¨"""
#     # ã‚®ãƒ£ãƒ©ãƒªãƒ¼ã«ãƒ›ãƒ¯ã‚¤ãƒˆãƒ‹ãƒ³ã‚°é©ç”¨
#     for cat_id, gallery in gallery_by_category.items():
#         if 'features' in gallery:
#             # ã‚®ãƒ£ãƒ©ãƒªãƒ¼ç‰¹å¾´é‡ â†’ ãƒ¢ãƒ‡ãƒ« â†’ ãƒ›ãƒ¯ã‚¤ãƒˆãƒ‹ãƒ³ã‚°
#             gallery_embeddings = []
#             for feat in gallery['features']:
#                 pred = model({'query_features': feat.reshape(1, 1, -1)}, training=False)
#                 gallery_embeddings.append(pred[0, cat_id-1].numpy())
            
#             gallery_embeddings = np.array(gallery_embeddings)
#             gallery['features'] = apply_whitening(gallery_embeddings, whitening_params['mean'], whitening_params['matrix'])
    
#     return gallery_by_category

def apply_whitening_to_predictions(predictions, whitening_params):
    """äºˆæ¸¬ãƒ™ã‚¯ãƒˆãƒ«ã«ãƒ›ãƒ¯ã‚¤ãƒˆãƒ‹ãƒ³ã‚°é©ç”¨ï¼ˆé«˜é€Ÿç‰ˆï¼‰"""
    if whitening_params is None:
        return predictions
        
    original_shape = predictions.shape
    # [batch_size, num_categories, embed_dim] -> [batch_size * num_categories, embed_dim]
    flat_predictions = predictions.reshape(-1, original_shape[-1])
    
    # ãƒ›ãƒ¯ã‚¤ãƒˆãƒ‹ãƒ³ã‚°é©ç”¨
    whitened = apply_whitening(flat_predictions, whitening_params['mean'], whitening_params['matrix'])
    
    # å…ƒã®å½¢çŠ¶ã«æˆ»ã™
    return whitened.reshape(original_shape)


import gzip
import pickle


def compute_input_whitening_stats(dataset_path: str, feature_dim: int):
    """
    ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®pklãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç”Ÿã®å…¥åŠ›ç‰¹å¾´é‡ã‚’èª­ã¿è¾¼ã¿ã€
    ãƒ›ãƒ¯ã‚¤ãƒˆãƒ‹ãƒ³ã‚°ã«å¿…è¦ãªçµ±è¨ˆé‡ï¼ˆå¹³å‡ã¨å…±åˆ†æ•£è¡Œåˆ—ï¼‰ã‚’è¨ˆç®—ã™ã‚‹ã€‚
    """
    import gzip
    import pickle
    import numpy as np
    from tqdm import tqdm
    import os

    print("ğŸ”§ Collecting all raw features from training data for whitening stats...")
    
    train_split_path = os.path.join(dataset_path, 'train.pkl')
    
    # â˜…â˜…â˜… ä¿®æ­£: ãƒ•ã‚¡ã‚¤ãƒ«ãŒ.gzã§çµ‚ã‚ã‚‹ã‹ç¢ºèªã—ã¦é–‹ãæ–¹æ³•ã‚’å¤‰ãˆã‚‹ â˜…â˜…â˜…
    if train_split_path.endswith('.gz'):
        opener = gzip.open
    else:
        # .gzã§ãªã‘ã‚Œã°é€šå¸¸ã®openã‚’ä½¿ç”¨
        opener = open
        
    try:
        with opener(train_split_path, 'rb') as f:
            # data_generator._load_data ã¨åŒã˜å½¢å¼ã§ãƒ­ãƒ¼ãƒ‰
            data = pickle.load(f)
    except FileNotFoundError:
        print(f"Error: File not found at {train_split_path}")
        return None
    except Exception as e:
        print(f"Error loading data from {train_split_path}: {e}")
        raise # ä»–ã®ã‚¨ãƒ©ãƒ¼ã¯å†ã‚¹ãƒ­ãƒ¼ã—ã¦ãƒ‡ãƒãƒƒã‚°ã‚’åŠ©ã‘ã‚‹
    
    # æ—¢å­˜ã®_load_dataãƒ­ã‚¸ãƒƒã‚¯ã‚’ç°¡ç•¥åŒ–ã—ã¦ç‰¹å¾´é‡ã ã‘ã‚’åé›†
    if not (isinstance(data, tuple) and len(data) >= 8):
        raise ValueError(f"Unsupported data format in {train_split_path}. Expected a tuple with at least 8 elements.")

    query_features = np.array(data[0], dtype=np.float32)
    positive_features = np.array(data[1], dtype=np.float32)

    # query featuresã¨positive featuresã‚’å…¨ã¦çµ±åˆ
    all_features = np.concatenate([query_features.reshape(-1, feature_dim), positive_features.reshape(-1, feature_dim)], axis=0)

    # 0ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆãƒ‘ãƒ‡ã‚£ãƒ³ã‚°éƒ¨åˆ†ï¼‰ã‚’é™¤å¤–
    # norm = np.linalg.norm(all_features, axis=1)
    # all_features = all_features[norm > 1e-8]
    
    print(f"âœ… Collected {len(all_features)} features with shape {all_features.shape}")
    
    # å¹³å‡ã¨å…±åˆ†æ•£è¡Œåˆ—ã‚’è¨ˆç®—
    mean_vector = np.mean(all_features, axis=0)
    
    # ãƒ‡ãƒ¼ã‚¿ç‚¹æ•°ãŒæ¬¡å…ƒæ•°ã‚ˆã‚Šå¤šã„ã“ã¨ã‚’ç¢ºèª
    num_samples, num_dims = all_features.shape
    if num_samples <= num_dims:
        print(f"âš ï¸ Warning: Number of samples ({num_samples}) is not greater than feature dimension ({num_dims}).")
        print("         Covariance matrix will be singular. Consider using a smaller dimension or more data.")

    # å…±åˆ†æ•£è¡Œåˆ—ã®è¨ˆç®— (ãƒ‡ãƒ¼ã‚¿æ•°ãŒæ¬¡å…ƒæ•°ã‚ˆã‚Šå¤šã„ã“ã¨ã‚’ä»®å®š)
    cov_matrix = np.cov(all_features, rowvar=False, bias=False)
    
    # å›ºæœ‰å€¤åˆ†è§£ (np.linalg.eigh ã¯å¯¾ç§°è¡Œåˆ—ç”¨ã§å®‰å®š)
    eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)
    
    # å›ºæœ‰å€¤ãŒè² ã®å€¤ã«ãªã‚‰ãªã„ã‚ˆã†ã«ã‚¯ãƒªãƒƒãƒ—ï¼ˆæ•°å€¤å®‰å®šæ€§ã®ãŸã‚ï¼‰
    eigenvalues[eigenvalues < 1e-8] = 1e-8
    
    # ãƒ›ãƒ¯ã‚¤ãƒˆãƒ‹ãƒ³ã‚°å¤‰æ›è¡Œåˆ—ï¼ˆZCA Whiteningï¼‰
    # ZCA = V * D^-0.5 * V^T
    whitening_matrix = eigenvectors @ np.diag(1. / np.sqrt(eigenvalues)) @ eigenvectors.T
    
    return {'mean': mean_vector, 'matrix': whitening_matrix}



def precompute_gallery_embeddings(model, gallery_by_category, whitening_params=None):
    """ã‚®ãƒ£ãƒ©ãƒªãƒ¼ã®åŸ‹ã‚è¾¼ã¿ã‚’äº‹å‰è¨ˆç®—ï¼ˆé«˜é€Ÿç‰ˆï¼‰"""
    print("[INFO] Pre-computing gallery embeddings...")
    
    processed_gallery = {}
    
    for cat_id, gallery in gallery_by_category.items():
        if 'features' not in gallery or len(gallery['features']) == 0:
            continue
            
        gallery_features = gallery['features']  # æ—¢ã«CLIPç‰¹å¾´é‡
        print(f"  Category {cat_id}: Processing {len(gallery_features)} items")
        
        # ãƒãƒƒãƒå‡¦ç†ã§ã‚®ãƒ£ãƒ©ãƒªãƒ¼ç‰¹å¾´é‡ã‚’ãƒ¢ãƒ‡ãƒ«åŸ‹ã‚è¾¼ã¿ã«å¤‰æ›
        batch_size = 64
        all_embeddings = []
        
        for i in range(0, len(gallery_features), batch_size):
            batch_features = gallery_features[i:i + batch_size]
            # [batch_size, 1, feature_dim] ã®å½¢çŠ¶ã§ãƒ¢ãƒ‡ãƒ«ã«å…¥åŠ›
            batch_input = np.expand_dims(batch_features, axis=1)
            
            # ãƒ¢ãƒ‡ãƒ«ã§åŸ‹ã‚è¾¼ã¿è¨ˆç®—
            batch_predictions = model({'query_features': tf.constant(batch_input)}, training=False)
            # è©²å½“ã‚«ãƒ†ã‚´ãƒªã®åŸ‹ã‚è¾¼ã¿ã‚’å–å¾—
            batch_embeddings = batch_predictions[:, cat_id - 1, :].numpy()
            all_embeddings.append(batch_embeddings)
        
        gallery_embeddings = np.concatenate(all_embeddings, axis=0)
        
        # ãƒ›ãƒ¯ã‚¤ãƒˆãƒ‹ãƒ³ã‚°é©ç”¨ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        if whitening_params is not None:
            gallery_embeddings = apply_whitening(
                gallery_embeddings, 
                whitening_params['mean'], 
                whitening_params['matrix']
            )
            print(f"    Applied whitening to category {cat_id}")
        
        processed_gallery[cat_id] = {
            'ids': gallery['ids'],
            'features': gallery_embeddings  # ã“ã‚Œã§åŸ‹ã‚è¾¼ã¿æ¸ˆã¿
        }
    
    return processed_gallery


# def evaluate_model_comprehensive(model, test_items, gallery_by_category, dataset_type, use_weighted_topk=False, category_centers=None):
#     print("[INFO] ğŸ¯ Starting comprehensive evaluation...")
#     config = get_dataset_config(dataset_type)
#     min_cat, max_cat = config['category_range']

#     # é‡ã¿ä»˜ããƒ¡ãƒˆãƒªã‚¯ã‚¹ç”¨ã®é–¾å€¤
#     ALPHA_RANK_THRESHOLD_PERCENT = 5  # è¨±å®¹ã‚¢ã‚¤ãƒ†ãƒ åŸºæº–ï¼ˆæ­£è§£ã¨ã®é¡ä¼¼åº¦é †ä½ï¼‰
#     BETA_THRESHOLD = 0.90             # äºˆæ¸¬æˆåŠŸåŸºæº–ï¼ˆäºˆæ¸¬ã¨ã®é¡ä¼¼åº¦ï¼‰
#     EPSILON = 1e-8

#     evaluated_queries_details = []

#     queries_by_category = defaultdict(list)

#     for item in tqdm(test_items, desc="Organizing queries"):
#         for target_cat, target_id in zip(item['target_categories'], item['target_item_ids']):
#             target_cat_int = int(target_cat)
#             if not (min_cat <= target_cat_int <= max_cat): continue
#             target_id_str = str(target_id)
#             if target_cat_int in gallery_by_category and target_id_str in gallery_by_category[target_cat_int]['ids']:
#                 query_data = {'query_features': item['query_features'], 'target_id': target_id_str}
#                 queries_by_category[target_cat_int].append(query_data)

#     # # ãƒ›ãƒ¯ã‚¤ãƒˆãƒ‹ãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
#     # whitening_params_path = 'training_whitening_params.pkl'
#     # if os.path.exists(whitening_params_path):
#     #     whitening_params = load_whitening_params(whitening_params_path)
#     #     print("[INFO] âœ… Loaded training-based whitening parameters")
#     #     processed_gallery = precompute_gallery_embeddings(model, gallery_by_category, whitening_params)
#     #     use_whitening = True
#     # else:
#     #     print("[INFO] âš ï¸ No whitening parameters found, using original features")
#     #     whitening_params = None
#     #     processed_gallery = None
#     #     use_whitening = False
    
#     category_results = {}
#     all_ranks = []
#     all_gallery_sizes = []

#     predictions_by_category = defaultdict(list)

#     # é‡ã¿ä»˜ããƒ¡ãƒˆãƒªã‚¯ã‚¹ç”¨ã®è“„ç©å¤‰æ•°
#     if use_weighted_topk:
#         all_weighted_scores = {k: [] for k in [1, 5, 10, 20]}
#         all_acceptance_counts = []
#         all_success_counts = []
    
#     # ãƒ‡ãƒãƒƒã‚°ç”¨
#     debug_count = 0
#     total_processed = 0
    
#     for cat_id, queries in tqdm(queries_by_category.items(), desc="Evaluating categories"):
#         if not queries: 
#             continue
        
#         gallery = gallery_by_category[cat_id]
            
#         gallery_size = len(gallery['ids'])
#         all_gallery_sizes.append(gallery_size)
        
#         # K%ã®é–¾å€¤ã‚’è¨ˆç®—ï¼ˆStandardç”¨ï¼‰
#         top_1_percent_threshold = max(1, int(gallery_size * 0.01))
#         top_5_percent_threshold = max(1, int(gallery_size * 0.05))
#         top_10_percent_threshold = max(1, int(gallery_size * 0.10))
#         top_20_percent_threshold = max(1, int(gallery_size * 0.20))
        
#         query_features_batch = np.array([q['query_features'] for q in queries])
#         predictions_batch = model({'query_features': tf.constant(query_features_batch)}, training=False).numpy()
#         pred_vectors = predictions_batch[:, cat_id - 1, :]

#         # ãƒ›ãƒ¯ã‚¤ãƒˆãƒ‹ãƒ³ã‚°æœ‰åŠ¹æ™‚ã®ã¿äºˆæ¸¬ãƒ™ã‚¯ãƒˆãƒ«ã«ãƒ›ãƒ¯ã‚¤ãƒˆãƒ‹ãƒ³ã‚°é©ç”¨
#         # if use_whitening and whitening_params is not None:
#         #     pred_vectors = apply_whitening(pred_vectors, whitening_params['mean'], whitening_params['matrix'])

#         # äºˆæ¸¬ãƒ™ã‚¯ãƒˆãƒ«ã‚’æ­£è¦åŒ–
#         # pred_norms = np.linalg.norm(pred_vectors, axis=1, keepdims=True)
#         # pred_vectors_normalized = pred_vectors / np.maximum(pred_norms, EPSILON)

#         # â˜…â˜…â˜… è¿½åŠ : æ­£è¦åŒ–ã•ã‚ŒãŸäºˆæ¸¬ãƒ™ã‚¯ãƒˆãƒ«ã‚’ã‚«ãƒ†ã‚´ãƒªã”ã¨ã«åé›† â˜…â˜…â˜…
#         predictions_by_category[cat_id].extend(pred_vectors)

#         # ==========================================================
#         # â˜…â˜…â˜… ã“ã“ã‹ã‚‰ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›ã®è¿½åŠ  (ä¿®æ­£ç‰ˆ) â˜…â˜…â˜…
#         # ==========================================================
#         if cat_id in predictions_by_category and len(predictions_by_category[cat_id]) > 1:
#             cat_pred_vectors = np.array(predictions_by_category[cat_id])
#             num_samples = len(cat_pred_vectors)
#             num_dimensions = cat_pred_vectors.shape[1]
            
#             # â˜…â˜…â˜… ä¿®æ­£: ä¸­å¿ƒåŒ–ã•ã‚ŒãŸãƒ™ã‚¯ãƒˆãƒ«ã‚’è¨ˆç®— â˜…â˜…â˜…
#             cat_mean = np.mean(cat_pred_vectors, axis=0)
#             centered_vectors = cat_pred_vectors - cat_mean
            
#             # äºˆæ¸¬ãƒ™ã‚¯ãƒˆãƒ«ã®ãƒãƒ«ãƒ ã®å¹³å‡ã¨æ¨™æº–åå·®
#             pred_vector_norms = np.linalg.norm(cat_pred_vectors, axis=1)
#             # print(f"\n[DEBUG-CAT-{cat_id}] ----------------------------------------")
#             # print(f"[DEBUG-CAT-{cat_id}] Analysis of Predicted Vectors ({num_samples} samples, {num_dimensions} dims)")
#             # print(f"[DEBUG-CAT-{cat_id}] - Average Norm (Original): {np.mean(pred_vector_norms):.6f}")
#             # print(f"[DEBUG-CAT-{cat_id}] - Norm Std Dev (Original): {np.std(pred_vector_norms):.6f}")
            
#             # å„æ¬¡å…ƒã®å¹³å‡ (ä¸­å¿ƒåŒ–å¾Œ)
#             # centered_mean = np.mean(centered_vectors, axis=0) # ã“ã‚Œã¯ã»ã¼0ã«ãªã‚‹ã¯ãš
#             # print(f"[DEBUG-CAT-{cat_id}] - Overall Mean Norm (Original): {np.linalg.norm(cat_mean):.6f}")
            
#             # å…±åˆ†æ•£è¡Œåˆ—ã®ç¢ºèª (ä¸­å¿ƒåŒ–ã•ã‚ŒãŸãƒ™ã‚¯ãƒˆãƒ«ã‚’ä½¿ç”¨)
#             if num_samples > num_dimensions:
#                 try:
#                     # å…±åˆ†æ•£è¡Œåˆ—ã‚’è¨ˆç®— (ä¸­å¿ƒåŒ–æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨)
#                     cov_matrix = np.cov(centered_vectors, rowvar=False, bias=False)
#                     # print(f"[DEBUG-CAT-{cat_id}] - Covariance Matrix (top-left 5x5, from centered vectors):\n{cov_matrix[:5, :5]}")
                    
#                     # å›ºæœ‰å€¤ã®ç¢ºèª
#                     eigenvalues = np.linalg.eigvalsh(cov_matrix)
#                     # print(f"[DEBUG-CAT-{cat_id}] - Top 5 Eigenvalues: {np.sort(eigenvalues)[::-1][:5]}")
#                     # print(f"[DEBUG-CAT-{cat_id}] - Smallest 5 Eigenvalues: {np.sort(eigenvalues)[:5]}")

#                     # è¡Œåˆ—å¼
#                     generalized_variance = np.linalg.det(cov_matrix)
#                     # print(f"[DEBUG-CAT-{cat_id}] - Generalized Variance (Determinant): {generalized_variance:.6e}")
#                     sign, log_det = np.linalg.slogdet(cov_matrix)
#                     # print(f"[DEBUG-CAT-{cat_id}] - Log Determinant: {log_det:.6f}")
#                 except np.linalg.LinAlgError as e:
#                     print(f"[DEBUG-CAT-{cat_id}] âš ï¸ LinAlgError: Could not compute covariance matrix/eigenvalues. Error: {e}")
#             else:
#                 print(f"[DEBUG-CAT-{cat_id}] â„¹ï¸ Not enough samples ({num_samples}) to compute covariance matrix for {num_dimensions} dimensions.")
#         # ==========================================================
#         # â˜…â˜…â˜… ã“ã“ã¾ã§ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›ãƒ–ãƒ­ãƒƒã‚¯ã‚’ä¿®æ­£ â˜…â˜…â˜…
#         # ==========================================================
        
#         # é¡ä¼¼åº¦è¨ˆç®—ï¼ˆæ­£è¦åŒ–ã•ã‚ŒãŸäºˆæ¸¬ãƒ™ã‚¯ãƒˆãƒ«ã¨ã‚®ãƒ£ãƒ©ãƒªãƒ¼ã®é¡ä¼¼åº¦ï¼‰
#         pred_similarities_batch = np.dot(pred_vectors, gallery['features'].T)
#         sorted_indices_batch = np.argsort(pred_similarities_batch, axis=1)[:, ::-1]
        
#         # ... (ä»¥é™ã®ã‚³ãƒ¼ãƒ‰ã¯å¤‰æ›´ãªã—) ...
#         # (ä»¥é™ã¯å…ƒã®ã‚³ãƒ¼ãƒ‰ã¨åŒã˜ãªã®ã§çœç•¥ã—ã¾ã™ã€‚ä¸Šè¨˜ã§è¿½åŠ ã—ãŸãƒ‡ãƒãƒƒã‚°å‡ºåŠ›ãŒã€
#         # å„ã‚«ãƒ†ã‚´ãƒªã®è©•ä¾¡ãƒ«ãƒ¼ãƒ—å†…ã§å®Ÿè¡Œã•ã‚Œã¾ã™ã€‚)

#         # é‡ã¿ä»˜ããƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—ç”¨
#         if use_weighted_topk:
#             category_weighted_scores = {k: [] for k in [1, 5, 10, 20]}
#             category_acceptance_counts = []
#             category_success_counts = []
        
#         ranks = []
#         for i, query in enumerate(queries):
#             sorted_gallery_ids = gallery['ids'][sorted_indices_batch[i]]
#             pred_similarities = pred_similarities_batch[i]
            
#             # æ­£è§£ã‚¢ã‚¤ãƒ†ãƒ ã®ãƒ©ãƒ³ã‚¯ã‚’å–å¾—ï¼ˆStandardç”¨ï¼‰
#             rank_list = np.where(sorted_gallery_ids == query['target_id'])[0]
#             if len(rank_list) > 0: 
#                 true_rank = rank_list[0] + 1
#                 ranks.append(true_rank)
#                 total_processed += 1
                
#                 # â˜…â˜…â˜… é‡ã¿ä»˜ããƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—ï¼ˆé †ä½ãƒ™ãƒ¼ã‚¹ã«ä¿®æ­£ï¼‰ â˜…â˜…â˜…
#                 if use_weighted_topk:
#                     # æ­£è§£ã‚¢ã‚¤ãƒ†ãƒ ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨ç‰¹å¾´é‡ã‚’å–å¾—
#                     correct_idx = np.where(gallery['ids'] == query['target_id'])[0][0]
#                     correct_feature = gallery['features'][correct_idx]
                    
#                     # Step 1: æ­£è§£ã‚¢ã‚¤ãƒ†ãƒ ã¨ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—ã—ã€é¡ä¼¼åº¦é †ã«ä¸¦ã³æ›¿ãˆ
#                     correct_similarities = np.dot(gallery['features'], correct_feature)
                    
#                     # â˜…â˜…â˜… ä¿®æ­£ç®‡æ‰€: é¡ä¼¼åº¦ã®çµ¶å¯¾å€¤ã§ã¯ãªãé †ä½ã§è¨±å®¹ã‚¢ã‚¤ãƒ†ãƒ ã‚’å®šç¾© â˜…â˜…â˜…
#                     # Step 2: é¡ä¼¼åº¦ã§ã‚½ãƒ¼ãƒˆã—ã€Top K% ã®é–¾å€¤ã‚’æ±ºå®š
#                     acceptable_rank_threshold = max(1, int(gallery_size * ALPHA_RANK_THRESHOLD_PERCENT / 100))
                    
#                     # é¡ä¼¼åº¦ãŒé«˜ã„é †ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
#                     sorted_correct_indices = np.argsort(correct_similarities)[::-1]
                    
#                     # Top K% ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¨±å®¹ã‚¢ã‚¤ãƒ†ãƒ ã¨ã™ã‚‹
#                     acceptable_indices = sorted_correct_indices[:acceptable_rank_threshold]
#                     acceptable_similarities = correct_similarities[acceptable_indices]
#                     # â˜…â˜…â˜… ä¿®æ­£ç®‡æ‰€ã“ã“ã¾ã§ â˜…â˜…â˜…
                    
#                     if len(acceptable_indices) > 0:
#                         # Step 4: äºˆæ¸¬ãƒ™ã‚¯ãƒˆãƒ«ã¨è¨±å®¹ã‚¢ã‚¤ãƒ†ãƒ ã®é¡ä¼¼åº¦
#                         pred_to_acceptable = pred_similarities[acceptable_indices]
                        
#                         # Step 5: å„TopK%ã«å¯¾ã—ã¦Weightedè¨ˆç®—
#                         for k in [1, 5, 10, 20]:
#                             # Standard TopKé–¾å€¤
#                             standard_threshold = eval(f"top_{k}_percent_threshold")
                            
#                             # äºˆæ¸¬é¡ä¼¼åº¦ã®é †ä½ã«åŸºã¥ãé–¾å€¤
#                             sorted_pred_sims = np.sort(pred_similarities)[::-1]
#                             topk_count = max(1, int(gallery_size * k / 100))
#                             if topk_count <= len(sorted_pred_sims):
#                                 similarity_threshold = sorted_pred_sims[topk_count - 1]
#                             else:
#                                 similarity_threshold = 0.0
                            
#                             # æˆåŠŸã—ãŸã‚¢ã‚¤ãƒ†ãƒ ã®ãƒã‚¹ã‚¯
#                             topk_success_mask = pred_to_acceptable >= similarity_threshold
                            
#                             # æˆåŠŸã—ãŸã‚¢ã‚¤ãƒ†ãƒ ã®é‡ã¿ã‚’è¨ˆç®—
#                             success_weights = acceptable_similarities[topk_success_mask]
#                             total_weights = acceptable_similarities
                            
#                             if np.sum(total_weights) > EPSILON:
#                                 weighted_score = np.sum(success_weights) / np.sum(total_weights)
#                             else:
#                                 weighted_score = 0.0
                            
#                             category_weighted_scores[k].append(weighted_score)
                        
#                         category_acceptance_counts.append(len(acceptable_indices))
                        
#                         # Î²=-1.0ãªã‚‰å¿…ãšæˆåŠŸ
#                         if BETA_THRESHOLD <= -0.999:
#                             category_success_counts.append(len(acceptable_indices))
#                         else:
#                             category_success_counts.append(np.sum(pred_to_acceptable >= BETA_THRESHOLD))
#                     else:
#                         # è¨±å®¹ã‚¢ã‚¤ãƒ†ãƒ ãŒãªã„å ´åˆ
#                         for k in [1, 5, 10, 20]:
#                             category_weighted_scores[k].append(0.0)
#                         category_acceptance_counts.append(0)
#                         category_success_counts.append(0)
            
#         if ranks:
#             all_ranks.extend(ranks)
#             ranks_np = np.array(ranks)
            
#             # åŸºæœ¬çš„ãªã‚«ãƒ†ã‚´ãƒªçµæœã‚’ä½œæˆ
#             cat_result = {
#                 'count': len(ranks_np), 
#                 'gallery_size': gallery_size,
#                 'mrr': np.mean(1.0 / ranks_np), 
#                 'r_at_1': np.mean(ranks_np <= top_1_percent_threshold),
#                 'r_at_5': np.mean(ranks_np <= top_5_percent_threshold),
#                 'r_at_10': np.mean(ranks_np <= top_10_percent_threshold),
#                 'r_at_20': np.mean(ranks_np <= top_20_percent_threshold),
#                 'mnr': np.mean(ranks_np), 
#                 'mdr': np.median(ranks_np),
#                 'rsum': np.sum(ranks_np),
#                 'top_1_percent_threshold': top_1_percent_threshold,
#                 'top_5_percent_threshold': top_5_percent_threshold,
#                 'top_10_percent_threshold': top_10_percent_threshold,
#                 'top_20_percent_threshold': top_20_percent_threshold,
#             }
#             # â˜…â˜…â˜… ä¿®æ­£ç®‡æ‰€: ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã‹ã‚‰ã®è·é›¢åˆ†æ•£ã®ä»£ã‚ã‚Šã«ã€äºˆæ¸¬ãƒ™ã‚¯ãƒˆãƒ«ã®å…±åˆ†æ•£è¡Œåˆ—ã®è¡Œåˆ—å¼ã‚’è¨ˆç®— â˜…â˜…â˜…
#             if cat_id in predictions_by_category:
#                 cat_pred_vectors = np.array(predictions_by_category[cat_id])
                
#                 # â˜…â˜…â˜… ä¿®æ­£: ä¸­å¿ƒåŒ–ã•ã‚ŒãŸãƒ™ã‚¯ãƒˆãƒ«ã‚’ä½¿ç”¨ â˜…â˜…â˜…
#                 if len(cat_pred_vectors) > 1:
#                     cat_mean = np.mean(cat_pred_vectors, axis=0)
#                     centered_vectors = cat_pred_vectors - cat_mean
#                 else:
#                     centered_vectors = cat_pred_vectors
                
#                 num_pred_dimensions = centered_vectors.shape[1] 

#                 if len(centered_vectors) > num_pred_dimensions: 
#                     try:
#                         cov_matrix = np.cov(centered_vectors, rowvar=False, bias=False)
#                         generalized_variance = np.linalg.det(cov_matrix)
#                         sign, log_generalized_variance = np.linalg.slogdet(cov_matrix)

#                         cat_result['generalized_variance'] = float(generalized_variance)
#                         cat_result['log_generalized_variance'] = float(log_generalized_variance)
#                     except np.linalg.LinAlgError as e:
#                         print(f"âš ï¸ Warning: Could not compute covariance determinant for category {cat_id}: {e}")
#                         cat_result['generalized_variance'] = 0.0
#                         cat_result['log_generalized_variance'] = -np.inf
#                 else:
#                     cat_result['generalized_variance'] = 0.0
#                     cat_result['log_generalized_variance'] = -np.inf
#             else:
#                 cat_result['generalized_variance'] = 0.0
#                 cat_result['log_generalized_variance'] = -np.inf
#             # â˜…â˜…â˜… ä¿®æ­£ç®‡æ‰€ã“ã“ã¾ã§ â˜…â˜…â˜…

#             # é‡ã¿ä»˜ããƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¿½åŠ 
#             if use_weighted_topk and any(len(scores) > 0 for scores in category_weighted_scores.values()):
#                 # å„TopK%ã®é‡ã¿ä»˜ãæ­£è§£ç‡ã‚’ã‚«ãƒ†ã‚´ãƒªçµæœã«è¿½åŠ 
#                 for k in [1, 5, 10, 20]:
#                     if len(category_weighted_scores[k]) > 0:
#                         cat_result[f'weighted_r_at_{k}'] = np.mean(category_weighted_scores[k])
#                         all_weighted_scores[k].extend(category_weighted_scores[k])
#                     else:
#                         cat_result[f'weighted_r_at_{k}'] = 0.0
                
#                 # ãã®ä»–ã®çµ±è¨ˆ
#                 acceptance_counts_np = np.array(category_acceptance_counts)
#                 success_counts_np = np.array(category_success_counts)
                
#                 cat_result.update({
#                     'avg_acceptance_count': np.mean(acceptance_counts_np) if len(acceptance_counts_np) > 0 else 0.0,
#                     'avg_success_count': np.mean(success_counts_np) if len(success_counts_np) > 0 else 0.0,
#                     'acceptance_rate': np.mean(acceptance_counts_np) / gallery_size if gallery_size > 0 and len(acceptance_counts_np) > 0 else 0.0,
#                     'alpha_rank_threshold_percent': ALPHA_RANK_THRESHOLD_PERCENT,
#                     'beta_threshold': BETA_THRESHOLD,
#                 })
                
#                 all_acceptance_counts.extend(category_acceptance_counts)
#                 all_success_counts.extend(category_success_counts)
            
#             category_results[str(cat_id)] = cat_result
    
#     # å…¨ä½“çµæœã®è¨ˆç®—
#     overall_results = {}
#     if all_ranks:
#         all_ranks_np = np.array(all_ranks)
#         avg_gallery_size = np.mean(all_gallery_sizes) if all_gallery_sizes else 0
        
#         overall_top_1_percent = max(1, int(avg_gallery_size * 0.01))
#         overall_top_5_percent = max(1, int(avg_gallery_size * 0.05))
#         overall_top_10_percent = max(1, int(avg_gallery_size * 0.10))
#         overall_top_20_percent = max(1, int(avg_gallery_size * 0.20))
        
#         overall_results = {
#             'total_queries': len(all_ranks_np), 
#             'successful_predictions': len(all_ranks_np),
#             'average_gallery_size': avg_gallery_size,
#             'mrr': np.mean(1.0 / all_ranks_np), 
#             'r_at_1': np.mean(all_ranks_np <= overall_top_1_percent),
#             'r_at_5': np.mean(all_ranks_np <= overall_top_5_percent),
#             'r_at_10': np.mean(all_ranks_np <= overall_top_10_percent),
#             'r_at_20': np.mean(all_ranks_np <= overall_top_20_percent),
#             'mnr': np.mean(all_ranks_np), 
#             'mdr': np.median(all_ranks_np),
#             'rsum': np.sum(all_ranks_np),
#             'overall_top_1_percent_threshold': overall_top_1_percent,
#             'overall_top_5_percent_threshold': overall_top_5_percent,
#             'overall_top_10_percent_threshold': overall_top_10_percent,
#             'overall_top_20_percent_threshold': overall_top_20_percent,
#         }
#         # â˜…â˜…â˜… ä¿®æ­£ç®‡æ‰€: å…¨ä½“ã®ä¸€èˆ¬åŒ–åˆ†æ•£ã‚’è¨ˆç®—ã—ã¦å…¨ä½“çµæœã«è¿½åŠ  (ä¸­å¿ƒåŒ–) â˜…â˜…â˜…
#         all_category_pred_vectors = []
#         all_category_labels = []
#         for cat_id in predictions_by_category:
#             if predictions_by_category[cat_id]: # ã‚«ãƒ†ã‚´ãƒªã«äºˆæ¸¬ãƒ™ã‚¯ãƒˆãƒ«ãŒã‚ã‚‹ã‹ç¢ºèª
#                 all_category_pred_vectors.extend(predictions_by_category[cat_id])
#                 all_category_labels.extend([cat_id] * len(predictions_by_category[cat_id])) 

        
#         if len(all_category_pred_vectors) > 1:
#             all_category_pred_vectors_np = np.array(all_category_pred_vectors)
#             all_category_labels_np = np.array(all_category_labels)

#         # # =======================================================================
#         # # â˜…â˜…â˜… ã“ã“ã‹ã‚‰å¯è¦–åŒ–ã‚³ãƒ¼ãƒ‰ã‚’è¿½åŠ  â˜…â˜…â˜…
#         # # =======================================================================
#         # try:
#         #     from sklearn.decomposition import PCA
#         #     import matplotlib.pyplot as plt
#         #     import seaborn as sns
            
#         #     print("\n[INFO] ğŸ¨ Starting PCA and t-SNE visualization...")

#         #     # 1. PCA (2æ¬¡å…ƒ)
#         #     pca_model = PCA(n_components=2)
#         #     pca_embeddings = pca_model.fit_transform(all_category_pred_vectors_np)
            
#         #     plt.figure(figsize=(10, 8))
#         #     # ã‚«ãƒ†ã‚´ãƒªã”ã¨ã«è‰²åˆ†ã‘ã—ã¦ãƒ—ãƒ­ãƒƒãƒˆ
#         #     sns.scatterplot(
#         #         x=pca_embeddings[:, 0], 
#         #         y=pca_embeddings[:, 1], 
#         #         hue=all_category_labels_np,
#         #         palette=sns.color_palette("hsv", n_colors=len(np.unique(all_category_labels_np))),
#         #         legend="full",
#         #         s=5
#         #     )
#         #     plt.title('PCA of Predicted Embeddings (2D Projection)')
#         #     plt.xlabel(f'Principal Component 1 (Variance Explained: {pca_model.explained_variance_ratio_[0]*100:.2f}%)')
#         #     plt.ylabel(f'Principal Component 2 (Variance Explained: {pca_model.explained_variance_ratio_[1]*100:.2f}%)')
#         #     plt.grid(True)
#         #     # plt.show() # JUPYTER NOTEBOOKãªã©ã§å®Ÿè¡Œã™ã‚‹å ´åˆ
#         #     pca_plot_path = os.path.join('experiments', 'IQON3000', 'B128_L2_H2_LR1e-04_Cycle0.0_Seed42', 'pca_embedding_space_custom.png')
#         #     plt.savefig(pca_plot_path)
#         #     print(f"[INFO] âœ… Custom PCA visualization saved to: {pca_plot_path}")
#         #     plt.close()

#             # 2. PCAã®ç´¯ç©å¯„ä¸ç‡ã‚’ç¢ºèª
#         #     pca_full = PCA(n_components=None) # å…¨ã¦ã®ä¸»æˆåˆ†ã‚’è¨ˆç®—
#         #     pca_full.fit(all_category_pred_vectors_np)
#         #     explained_variance_ratio_cumsum = np.cumsum(pca_full.explained_variance_ratio_)
            
#         #     # 90%ã®åˆ†æ•£ã‚’èª¬æ˜ã™ã‚‹ã®ã«å¿…è¦ãªæ¬¡å…ƒæ•°ã‚’å–å¾—
#         #     num_dims_for_90_percent_variance = np.argmax(explained_variance_ratio_cumsum >= 0.90) + 1
#         #     print(f"[INFO] ğŸ“Š Dimensions to explain 90% of variance: {num_dims_for_90_percent_variance} out of {num_dimensions}")
            
#         #     # 3. t-SNE (2æ¬¡å…ƒ)
#         #     # t-SNEã¯è¨ˆç®—ã«æ™‚é–“ãŒã‹ã‹ã‚‹ãŸã‚ã€ã‚µãƒ–ã‚µãƒ³ãƒ—ãƒ«ã§å®Ÿè¡Œã™ã‚‹ã“ã¨ã‚‚æ¤œè¨
#         #     if len(all_category_pred_vectors) > 5000:
#         #         print("[INFO] âš ï¸ Large dataset for t-SNE, subsampling to 5000 points...")
#         #         indices = np.random.choice(len(all_category_pred_vectors), 5000, replace=False)
#         #         subset_embeddings = all_category_pred_vectors_np[indices]
#         #         subset_labels = all_category_labels_np[indices]
#         #     else:
#         #         subset_embeddings = all_category_pred_vectors_np
#         #         subset_labels = all_category_labels_np
            
#         #     from sklearn.manifold import TSNE
#         #     tsne_model = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=300)
#         #     tsne_embeddings = tsne_model.fit_transform(subset_embeddings)
            
#         #     plt.figure(figsize=(10, 8))
#         #     sns.scatterplot(
#         #         x=tsne_embeddings[:, 0], 
#         #         y=tsne_embeddings[:, 1], 
#         #         hue=subset_labels,
#         #         palette=sns.color_palette("hsv", n_colors=len(np.unique(subset_labels))),
#         #         legend="full",
#         #         s=5
#         #     )
#         #     plt.title('t-SNE of Predicted Embeddings (2D Projection)')
#         #     plt.xlabel('t-SNE Dimension 1')
#         #     plt.ylabel('t-SNE Dimension 2')
#         #     plt.grid(True)
#         #     # plt.show() # ã“ã“ãƒ•ã‚©ãƒ«ãƒ€æŒ‡å®šã—ã¦ã—ã¾ã£ã¦ã‚‹ã€‚
#         #     tsne_plot_path = os.path.join('experiments', 'IQON3000', 'B128_L2_H2_LR1e-04_Cycle0.0_Seed42', 'tsne_embedding_space_custom.png')
#         #     plt.savefig(tsne_plot_path)
#         #     print(f"[INFO] âœ… Custom t-SNE visualization saved to: {tsne_plot_path}")
#         #     plt.close()

#         # except ImportError:
#         #     print("[INFO] âš ï¸ Matplotlib or scikit-learn not found. Skipping visualization.")
#         # =======================================================================
#         # â˜…â˜…â˜… ã“ã“ã¾ã§å¯è¦–åŒ–ã‚³ãƒ¼ãƒ‰ã‚’è¿½åŠ  â˜…â˜…â˜…
#         # =======================================================================
            
#             # â˜…â˜…â˜… ä¿®æ­£: å…¨ä½“ã®å¹³å‡ã‚’å¼•ã„ã¦ã‹ã‚‰å…±åˆ†æ•£è¡Œåˆ—ã‚’è¨ˆç®— â˜…â˜…â˜…
#             overall_mean = np.mean(all_category_pred_vectors_np, axis=0)
#             overall_centered_vectors = all_category_pred_vectors_np - overall_mean
            
#             num_overall_dimensions = overall_centered_vectors.shape[1]

#             if len(overall_centered_vectors) > num_overall_dimensions:
#                 try:
#                     overall_cov_matrix = np.cov(overall_centered_vectors, rowvar=False, bias=False)
#                     overall_generalized_variance = np.linalg.det(overall_cov_matrix)
#                     overall_sign, overall_log_generalized_variance = np.linalg.slogdet(overall_cov_matrix)

#                     overall_results['overall_generalized_variance'] = float(overall_generalized_variance)
#                     overall_results['overall_log_generalized_variance'] = float(overall_log_generalized_variance)
#                 except np.linalg.LinAlgError as e:
#                     print(f"âš ï¸ Warning: Could not compute overall covariance determinant: {e}")
#                     overall_results['overall_generalized_variance'] = 0.0
#                     overall_results['overall_log_generalized_variance'] = -np.inf
#             else:
#                 overall_results['overall_generalized_variance'] = 0.0
#                 overall_results['overall_log_generalized_variance'] = -np.inf
#         else:
#             overall_results['overall_generalized_variance'] = 0.0
#             overall_results['overall_log_generalized_variance'] = -np.inf
#         # â˜…â˜…â˜… ä¿®æ­£ç®‡æ‰€ã“ã“ã¾ã§ â˜…â˜…â˜…

#         # é‡ã¿ä»˜ããƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å…¨ä½“çµæœã«è¿½åŠ 
#         if use_weighted_topk and any(len(scores) > 0 for scores in all_weighted_scores.values()):
#             # å…¨ä½“ã®é‡ã¿ä»˜ãTopK%æ­£è§£ç‡
#             for k in [1, 5, 10, 20]:
#                 if len(all_weighted_scores[k]) > 0:
#                     overall_results[f'weighted_r_at_{k}'] = np.mean(all_weighted_scores[k])
#                 else:
#                     overall_results[f'weighted_r_at_{k}'] = 0.0
            
#             # ãƒ‡ãƒãƒƒã‚°: æœŸå¾…å€¤ã¨ã®æ¯”è¼ƒ
#             print(f"\n[FINAL DEBUG] Î±_rank={ALPHA_RANK_THRESHOLD_PERCENT}%, Î²={BETA_THRESHOLD}:")
#             print(f"   Total processed queries: {total_processed}")
            
#             for k in [1, 5, 10, 20]:
#                 std_val = overall_results[f'r_at_{k}']
#                 weighted_val = overall_results.get(f'weighted_r_at_{k}', 0.0)
#                 print(f"   Top-{k}%: Standard={std_val:.4f}, Weighted={weighted_val:.4f}")
            
#             # ãã®ä»–ã®å…¨ä½“çµ±è¨ˆ
#             if all_acceptance_counts and all_success_counts:
#                 all_acceptance_counts_np = np.array(all_acceptance_counts)
#                 all_success_counts_np = np.array(all_success_counts)
                
#                 overall_results.update({
#                     'avg_acceptance_count': np.mean(all_acceptance_counts_np),
#                     'avg_success_count': np.mean(all_success_counts_np),
#                     'overall_acceptance_rate': np.mean(all_acceptance_counts_np) / avg_gallery_size if avg_gallery_size > 0 else 0.0,
#                     'alpha_rank_threshold_percent': ALPHA_RANK_THRESHOLD_PERCENT,
#                     'beta_threshold': BETA_THRESHOLD,
#                 })
    
#     return {'dataset': dataset_type, 'overall': overall_results, 'categories': category_results}


def evaluate_model_comprehensive(model, test_items, gallery_by_category, dataset_type, use_weighted_topk=False, category_centers=None):
    """
    ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ã‚’åŒ…æ‹¬çš„ã«è¡Œã†é–¢æ•°ã€‚
    use_weighted_topk=Trueã®å ´åˆã€è¨±å®¹ã‚¢ã‚¤ãƒ†ãƒ ã®ç¯„å›²(ALPHA)ã¨æˆåŠŸåˆ¤å®šã®ç¯„å›²(K)ã‚’é€£å‹•ã•ã›ã‚‹
    ã€Œãƒ©ãƒ³ã‚­ãƒ³ã‚°æ•´åˆæ€§ã€ã‚’æ¸¬å®šã™ã‚‹æ–¹å¼ã§é‡ã¿ä»˜ãTopKç²¾åº¦ã‚’è¨ˆç®—ã™ã‚‹ã€‚
    """
    print("[INFO] ğŸ¯ Starting comprehensive evaluation (Dynamic ALPHA method)...")
    # config = get_dataset_config(dataset_type)
    # min_cat, max_cat = config['category_range']
    min_cat, max_cat = 0, 10000 # ä»®ã®å€¤
    EPSILON = 1e-8

    queries_by_category = defaultdict(list)
    for item in tqdm(test_items, desc="Organizing queries"):
        for target_cat, target_id in zip(item['target_categories'], item['target_item_ids']):
            target_cat_int = int(target_cat)
            if not (min_cat <= target_cat_int <= max_cat): continue
            target_id_str = str(target_id)
            if target_cat_int in gallery_by_category and target_id_str in gallery_by_category[target_cat_int]['ids']:
                query_data = {'query_features': item['query_features'], 'target_id': target_id_str}
                queries_by_category[target_cat_int].append(query_data)

    category_results = {}
    all_ranks = []
    all_gallery_sizes = []
    predictions_by_category = defaultdict(list)

    if use_weighted_topk:
        all_weighted_scores = {k: [] for k in [1, 5, 10, 20]}
    
    for cat_id, queries in tqdm(queries_by_category.items(), desc="Evaluating categories"):
        if not queries: 
            continue
        
        gallery = gallery_by_category[cat_id]
        gallery_size = len(gallery['ids'])
        all_gallery_sizes.append(gallery_size)
        
        # Standard Top-K%ç”¨ã®é–¾å€¤
        top_1_percent_threshold = max(1, int(gallery_size * 0.01))
        top_5_percent_threshold = max(1, int(gallery_size * 0.05))
        top_10_percent_threshold = max(1, int(gallery_size * 0.10))
        top_20_percent_threshold = max(1, int(gallery_size * 0.20))
        
        query_features_batch = np.array([q['query_features'] for q in queries])
        predictions_batch = model({'query_features': tf.constant(query_features_batch)}, training=False).numpy()
        pred_vectors = predictions_batch[:, cat_id - 1, :]
        predictions_by_category[cat_id].extend(pred_vectors)
        
        pred_similarities_batch = np.dot(pred_vectors, gallery['features'].T)
        sorted_indices_batch = np.argsort(pred_similarities_batch, axis=1)[:, ::-1]

        if use_weighted_topk:
            category_weighted_scores = {k: [] for k in [1, 5, 10, 20]}
        
        ranks = []
        for i, query in enumerate(queries):
            sorted_gallery_ids = gallery['ids'][sorted_indices_batch[i]]
            pred_similarities = pred_similarities_batch[i]
            
            rank_list = np.where(sorted_gallery_ids == query['target_id'])[0]
            if len(rank_list) > 0: 
                true_rank = rank_list[0] + 1
                ranks.append(true_rank)
                
                if use_weighted_topk:
                    correct_idx = np.where(gallery['ids'] == query['target_id'])[0][0]
                    correct_feature = gallery['features'][correct_idx]
                    correct_similarities = np.dot(gallery['features'], correct_feature)
                    sorted_correct_indices = np.argsort(correct_similarities)[::-1]
                    sorted_pred_sims = np.sort(pred_similarities)[::-1]

                    for k in [1, 5, 10, 20]:
                        # 1. è¨±å®¹ã‚¢ã‚¤ãƒ†ãƒ ã‚’ã€Œk%ã€ã«åˆã‚ã›ã¦å‹•çš„ã«å®šç¾©
                        acceptable_rank_threshold = max(1, int(gallery_size * k / 100))
                        acceptable_indices = sorted_correct_indices[:acceptable_rank_threshold]
                        
                        if len(acceptable_indices) == 0:
                            category_weighted_scores[k].append(0.0)
                            continue

                        # 2. æˆåŠŸã®åŸºæº–ï¼ˆäºˆæ¸¬ã®ä¸Šä½k%ï¼‰ã‚’å®šç¾©
                        similarity_threshold = sorted_pred_sims[acceptable_rank_threshold - 1]
                        
                        # 3. é‡ã¿ä»˜ãã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
                        acceptable_similarities = correct_similarities[acceptable_indices]
                        pred_to_acceptable = pred_similarities[acceptable_indices]
                        
                        topk_success_mask = pred_to_acceptable >= similarity_threshold
                        success_weights = acceptable_similarities[topk_success_mask]
                        total_weights = acceptable_similarities
                        
                        if np.sum(total_weights) > EPSILON:
                            weighted_score = np.sum(success_weights) / np.sum(total_weights)
                        else:
                            weighted_score = 0.0
                        
                        category_weighted_scores[k].append(weighted_score)

        if ranks:
            ranks_np = np.array(ranks)
            all_ranks.extend(ranks)
            
            cat_result = {
                'count': len(ranks_np), 
                'gallery_size': gallery_size,
                'mrr': np.mean(1.0 / ranks_np), 
                'r_at_1': np.mean(ranks_np <= top_1_percent_threshold),
                'r_at_5': np.mean(ranks_np <= top_5_percent_threshold),
                'r_at_10': np.mean(ranks_np <= top_10_percent_threshold),
                'r_at_20': np.mean(ranks_np <= top_20_percent_threshold),
                'mnr': np.mean(ranks_np), 
                'mdr': np.median(ranks_np)
            }
            
            if use_weighted_topk:
                for k in [1, 5, 10, 20]:
                    if len(category_weighted_scores[k]) > 0:
                        cat_result[f'weighted_r_at_{k}'] = np.mean(category_weighted_scores[k])
                        all_weighted_scores[k].extend(category_weighted_scores[k])
                    else:
                        cat_result[f'weighted_r_at_{k}'] = 0.0

            category_results[str(cat_id)] = cat_result
    
    overall_results = {}
    if all_ranks:
        all_ranks_np = np.array(all_ranks)
        avg_gallery_size = np.mean(all_gallery_sizes) if all_gallery_sizes else 0
        
        overall_top_1_percent = max(1, int(avg_gallery_size * 0.01))
        overall_top_5_percent = max(1, int(avg_gallery_size * 0.05))
        overall_top_10_percent = max(1, int(avg_gallery_size * 0.10))
        overall_top_20_percent = max(1, int(avg_gallery_size * 0.20))
        
        overall_results = {
            'total_queries': len(all_ranks_np), 
            'average_gallery_size': avg_gallery_size,
            'mrr': np.mean(1.0 / all_ranks_np), 
            'r_at_1': np.mean(all_ranks_np <= overall_top_1_percent),
            'r_at_5': np.mean(all_ranks_np <= overall_top_5_percent),
            'r_at_10': np.mean(all_ranks_np <= overall_top_10_percent),
            'r_at_20': np.mean(all_ranks_np <= overall_top_20_percent),
            'mnr': np.mean(all_ranks_np), 
            'mdr': np.median(all_ranks_np)
        }
        
        if use_weighted_topk:
            for k in [1, 5, 10, 20]:
                if len(all_weighted_scores[k]) > 0:
                    overall_results[f'weighted_r_at_{k}'] = np.mean(all_weighted_scores[k])
                else:
                    overall_results[f'weighted_r_at_{k}'] = 0.0
    
    return {'dataset': dataset_type, 'overall': overall_results, 'categories': category_results}

    
def evaluate_model(model, test_data, output_dir: str, data_dir: str, train_generator=None, use_weighted_topk=False, category_centers=None):
    """
    Main evaluation pipeline with optional weighted TopK metrics.
    """
    print(f"\n[INFO] ğŸš€ Starting model evaluation pipeline...")
    if use_weighted_topk:
        print(f"[INFO] âœ¨ Weighted TopK metrics enabled")
    
    try:
        os.makedirs(output_dir, exist_ok=True)
        test_items, gallery, dataset_type = collect_test_data(test_data)
        config = get_dataset_config(dataset_type)

        # â˜… use_weighted_topk ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’è©•ä¾¡é–¢æ•°ã«æ¸¡ã™
        results = evaluate_model_comprehensive(model, test_items, gallery, dataset_type, use_weighted_topk=use_weighted_topk, category_centers=category_centers)
        
        if results:
            # â˜… JSONã®ä¿å­˜ã§å¾ªç’°å‚ç…§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã™ã‚‹ãŸã‚ã€ã¾ãšè¡¨ç¤ºã®ã¿å®Ÿè¡Œ
            try:
                print("\n================================================================================")
                print("ğŸ¯ EVALUATION RESULTS")
                print("================================================================================")
                
                overall_results = results['overall']
                category_results = results['categories']
                
                # Overall Performance ã®è¡¨ç¤º
                print("\nğŸ“Š Overall Performance:")
                for key, value in overall_results.items():
                    if isinstance(value, float):
                        print(f"   {key}: {value:.4f}")
                    else:
                        print(f"   {key}: {value}")
                
                # Standard TopK% Accuracy ã®è¡¨ç¤º
                print("\nğŸ“ˆ Standard TopK% Accuracy:")
                for k in model.k_values: # Assuming model.k_values is available
                    print(f"   Top-{k}%: {overall_results.get(f'r_at_{k}', 0.0):.4f} ({overall_results.get(f'r_at_{k}', 0.0)*100:.2f}%)")
                
                # Weighted TopK% Accuracy ã®è¡¨ç¤º
                if 'weighted_r_at_1' in overall_results:
                    print("\nâœ¨ Weighted TopK% Accuracy:")
                    for k in model.k_values:
                        print(f"   Weighted Top-{k}%: {overall_results.get(f'weighted_r_at_{k}', 0.0):.4f} ({overall_results.get(f'weighted_r_at_{k}', 0.0)*100:.2f}%)")
                
                # ã‚«ãƒ†ã‚´ãƒªã”ã¨ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¡¨ç¤º
                print("\nğŸ“‚ Category-wise Performance:")
                print("--------------------------------------------------------------------------------")
                for cat_id_str, cat_metrics in category_results.items():
                    print(f"\nğŸ·ï¸  Category {cat_id_str} (Category {cat_id_str}):")
                    print(f"    Queries: {cat_metrics['count']}, Gallery: {cat_metrics['gallery_size']}")
                    print(f"    MRR: {cat_metrics['mrr']:.4f}")
                    
                    # Standard
                    standard_metrics = " | ".join([f"Top-{k}%: {cat_metrics[f'r_at_{k}']:.3f}" for k in model.k_values])
                    print(f"    Standard: {standard_metrics}")
                    
                    # Weighted
                    if 'weighted_r_at_1' in cat_metrics:
                        weighted_metrics = " | ".join([f"W-Top-{k}%: {cat_metrics[f'weighted_r_at_{k}']:.3f}" for k in model.k_values])
                        print(f"    Weighted: {weighted_metrics}")
                        
                    # ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰åˆ†æ•£ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¿½åŠ 
                    if 'centroid_distance_variance' in cat_metrics:
                        print(f"    Centroid Variance: {cat_metrics['centroid_distance_variance']:.6f} | Avg Distance: {cat_metrics['avg_centroid_distance']:.6f}")
                        
                    # ãã®ä»–ã®è©³ç´°ãƒ¡ãƒˆãƒªã‚¯ã‚¹
                    print(f"    Details: Acc-Count: {cat_metrics.get('avg_acceptance_count', 0.0):.1f} | Acc-Rate: {cat_metrics.get('acceptance_rate', 0.0):.3f}")
                    
                print("\n================================================================================")
                
            except Exception as display_error:
                print(f"[WARN] Display failed: {display_error}")
            
            # â˜… å®‰å…¨ãªå½¢ã§CSVã®ã¿ä¿å­˜
            try:
                print("[INFO] ğŸ’¾ Saving CSV results...")
                save_csv_results_only(results, output_dir)
            except Exception as save_error:
                print(f"[WARN] CSV save failed: {save_error}")
            
            # â˜… å¯è¦–åŒ–ã¯å¾Œã§å®Ÿè¡Œ
            try:
                generate_all_visualizations(model, results, test_items, gallery, config, output_dir, data_dir)
            except Exception as viz_error:
                print(f"[WARN] Visualization failed: {viz_error}")
        
        clear_memory()
        return results, test_items, gallery, dataset_type

    except Exception as e:
        print(f"[ERROR] âŒ Evaluation failed: {e}")
        import traceback
        traceback.print_exc()
        return None


def save_csv_results_only(results, output_dir):
    """
    å¾ªç’°å‚ç…§ã‚’é¿ã‘ã¦CSVå½¢å¼ã®ã¿ã§çµæœã‚’ä¿å­˜
    """
    import pandas as pd
    
    csv_data = []
    
    # Overall results
    if 'overall' in results:
        overall = results['overall']
        row = {'category': 'overall'}
        # æ•°å€¤ã®ã¿ã‚’å®‰å…¨ã«æŠ½å‡º
        for key, value in overall.items():
            if isinstance(value, (int, float, np.integer, np.floating)):
                row[key] = float(value)
            elif isinstance(value, str):
                row[key] = value
        csv_data.append(row)
    
    # Category results
    if 'categories' in results:
        for cat_id, cat_results in results['categories'].items():
            row = {'category': cat_id}
            # æ•°å€¤ã®ã¿ã‚’å®‰å…¨ã«æŠ½å‡º
            for key, value in cat_results.items():
                if isinstance(value, (int, float, np.integer, np.floating)):
                    row[key] = float(value)
                elif isinstance(value, str):
                    row[key] = value
            csv_data.append(row)
    
    # Save CSV
    if csv_data:
        df = pd.DataFrame(csv_data)
        csv_path = os.path.join(output_dir, 'results_summary.csv')
        df.to_csv(csv_path, index=False)
        print(f"[INFO] âœ… CSV results saved to: {csv_path}")
        
        # é‡ã¿ä»˜ããƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒã‚ã‚‹å ´åˆã¯å°‚ç”¨CSVã‚‚ä½œæˆ
        if any('weighted_' in col for col in df.columns):
            weighted_columns = ['category'] + [col for col in df.columns if 'weighted' in col or 'acceptance' in col or 'threshold' in col]
            if len(weighted_columns) > 1:
                weighted_df = df[weighted_columns]
                weighted_csv_path = os.path.join(output_dir, 'weighted_metrics_summary.csv')
                weighted_df.to_csv(weighted_csv_path, index=False)
                print(f"[INFO] âœ… Weighted metrics CSV saved to: {weighted_csv_path}")

