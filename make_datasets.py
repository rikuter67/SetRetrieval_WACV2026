#!/usr/bin/env python3
"""
ä¿®æ­£ç‰ˆ make_datasets.py
========================
é‡è¤‡é™¤å»ã®çµ±ä¸€ã€æ­£è¦åŒ–å‡¦ç†ã®çµ±ä¸€ã€ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã®å¼·åŒ–

ä¸»ãªä¿®æ­£ç‚¹:
1. é‡è¤‡é™¤å»: åŒã˜furniture_id/item_idã¯è§’åº¦ã«é–¢ä¿‚ãªã1ã¤ã®ã¿ä¿æŒ
2. æ­£è¦åŒ–çµ±ä¸€: L2æ­£è¦åŒ–ã®ã¿ï¼ˆZ-scoreæ­£è¦åŒ–ã¯å‰Šé™¤ï¼‰
3. ã‚«ãƒ†ã‚´ãƒªIDä½“ç³»ã®å®Œå…¨çµ±ä¸€
4. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã®å¼·åŒ–

Usage:
------
# For IQON3000 dataset (7 categories)
python make_datasets.py --dataset iqon3000 --input-dir data/IQON3000 --output-dir datasets/IQON3000

# For DeepFurniture dataset (11 categories)
python make_datasets.py --dataset deepfurniture --image-dir data/DeepFurniture/furnitures --annotations-json data/DeepFurniture/metadata/annotations.json --furnitures-jsonl data/DeepFurniture/metadata/furnitures.jsonl --output-dir datasets/DeepFurniture

"""

import os
import json
import pickle
import gzip
import argparse
import random
from pathlib import Path
from typing import Dict, List, Any, Tuple, Set
import numpy as np
import torch
from PIL import Image, UnidentifiedImageError
from torch.utils.data import Dataset, DataLoader
from transformers import CLIPProcessor, CLIPModel
from tqdm import tqdm
from sklearn.model_selection import train_test_split
import tensorflow as tf

os.environ["CUDA_VISIBLE_DEVICES"] = os.environ.get("CUDA_VISIBLE_DEVICES", "0")
tf.get_logger().setLevel("ERROR")

# =============================================================================
# çµ±ä¸€ã•ã‚ŒãŸã‚«ãƒ†ã‚´ãƒªè¨­å®š
# =============================================================================

IQON3000_CATEGORIES = {
    1: "outerwear",      # ã‚¢ã‚¦ã‚¿ãƒ¼ï¼ˆå’Œæœãƒ»ãƒ«ãƒ¼ãƒ ã‚¦ã‚§ã‚¢å«ã‚€ï¼‰
    2: "tops",           # ãƒˆãƒƒãƒ—ã‚¹
    3: "dresses",        # ãƒ¯ãƒ³ãƒ”ãƒ¼ã‚¹ãƒ»ãƒ‰ãƒ¬ã‚¹
    4: "pants",          # ãƒ‘ãƒ³ãƒ„
    5: "skirts",         # ã‚¹ã‚«ãƒ¼ãƒˆ
    6: "shoes",          # ã‚·ãƒ¥ãƒ¼ã‚º
    7: "bags",           # ãƒãƒƒã‚°
    8: "hats",           # å¸½å­
    9: "watches",        # æ™‚è¨ˆ
    10: "accessories",   # ã‚¸ãƒ¥ã‚¨ãƒªãƒ¼ãƒ»ã‚¢ã‚¯ã‚»ã‚µãƒªãƒ¼
    11: "fashion_goods", # ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³é›‘è²¨ï¼ˆã‚¤ãƒ³ãƒ†ãƒªã‚¢å«ã‚€ï¼‰
    12: "wallets",       # è²¡å¸ƒãƒ»å°ç‰©
    13: "legwear",       # ãƒ¬ãƒƒã‚°ã‚¦ã‚§ã‚¢
    14: "underwear",     # ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¦ã‚§ã‚¢ãƒ»æ°´ç€
    15: "beauty",        # ãƒ“ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ¼
    16: "glasses",       # çœ¼é¡
    17: "others"         # ãã®ä»–
}

# IQON3000å®Ÿãƒ‡ãƒ¼ã‚¿åŸºæº–ã‚«ãƒ†ã‚´ãƒªãƒãƒƒãƒ”ãƒ³ã‚°
IQON3000_CATEGORY_MAPPING = {
    # ã‚«ãƒ†ã‚´ãƒª1: ã‚¢ã‚¦ã‚¿ãƒ¼
    "ã‚¸ãƒ£ã‚±ãƒƒãƒˆ": 1, "ã‚³ãƒ¼ãƒˆ": 1, "ã‚¢ã‚¦ã‚¿ãƒ¼": 1, "ã‚«ãƒ¼ãƒ‡ã‚£ã‚¬ãƒ³": 1, 
    "ãƒ–ãƒ«ã‚¾ãƒ³": 1, "ãƒ€ã‚¦ãƒ³": 1, "ãƒ‘ãƒ¼ã‚«ãƒ¼": 1, "æµ´è¡£": 1, "ç€ç‰©": 1, "ãƒ«ãƒ¼ãƒ ã‚¦ã‚§ã‚¢": 1,
    # ã‚«ãƒ†ã‚´ãƒª2: ãƒˆãƒƒãƒ—ã‚¹
    "Tã‚·ãƒ£ãƒ„": 2, "ã‚«ãƒƒãƒˆã‚½ãƒ¼": 2, "ã‚·ãƒ£ãƒ„": 2, "ãƒ–ãƒ©ã‚¦ã‚¹": 2, 
    "ãƒ‹ãƒƒãƒˆ": 2, "ã‚»ãƒ¼ã‚¿ãƒ¼": 2, "ãƒ™ã‚¹ãƒˆ": 2, "ã‚¿ãƒ³ã‚¯ãƒˆãƒƒãƒ—": 2, 
    "ã‚­ãƒ£ãƒŸã‚½ãƒ¼ãƒ«": 2, "ãƒãƒ¥ãƒ‹ãƒƒã‚¯": 2, "ãƒˆãƒƒãƒ—ã‚¹": 2, "ã‚¤ãƒ³ãƒŠãƒ¼": 2,
    # ã‚«ãƒ†ã‚´ãƒª3: ãƒ¯ãƒ³ãƒ”ãƒ¼ã‚¹ãƒ»ãƒ‰ãƒ¬ã‚¹
    "ãƒ¯ãƒ³ãƒ”ãƒ¼ã‚¹": 3, "ãƒ‰ãƒ¬ã‚¹": 3,
    # ã‚«ãƒ†ã‚´ãƒª4: ãƒ‘ãƒ³ãƒ„
    "ãƒ‘ãƒ³ãƒ„": 4, "ã‚·ãƒ§ãƒ¼ãƒˆãƒ‘ãƒ³ãƒ„": 4, "ãƒ­ãƒ³ã‚°ãƒ‘ãƒ³ãƒ„": 4, 
    "ã‚¸ãƒ¼ãƒ³ã‚º": 4, "ãƒ‡ãƒ‹ãƒ ": 4, "ãƒ¬ã‚®ãƒ³ã‚¹": 4, "ã‚¹ãƒ©ãƒƒã‚¯ã‚¹": 4,
    # ã‚«ãƒ†ã‚´ãƒª5: ã‚¹ã‚«ãƒ¼ãƒˆ
    "ã‚¹ã‚«ãƒ¼ãƒˆ": 5, "ãƒ­ãƒ³ã‚°ã‚¹ã‚«ãƒ¼ãƒˆ": 5,
    # ã‚«ãƒ†ã‚´ãƒª6: ã‚·ãƒ¥ãƒ¼ã‚º
    "ã‚·ãƒ¥ãƒ¼ã‚º": 6, "é´": 6, "ã‚¹ãƒ‹ãƒ¼ã‚«ãƒ¼": 6, "ã‚µãƒ³ãƒ€ãƒ«": 6, 
    "ãƒ–ãƒ¼ãƒ„": 6, "ãƒ‘ãƒ³ãƒ—ã‚¹": 6, "ãƒ«ãƒ¼ãƒ ã‚·ãƒ¥ãƒ¼ã‚º": 6, "ãƒ­ãƒ¼ãƒ•ã‚¡ãƒ¼": 6,
    # ã‚«ãƒ†ã‚´ãƒª7: ãƒãƒƒã‚°
    "ãƒãƒƒã‚°": 7, "ãƒˆãƒ¼ãƒˆãƒãƒƒã‚°": 7, "ã‚·ãƒ§ãƒ«ãƒ€ãƒ¼ãƒãƒƒã‚°": 7, 
    "ãƒãƒ³ãƒ‰ãƒãƒƒã‚°": 7, "ã‚¯ãƒ©ãƒƒãƒãƒãƒƒã‚°": 7, "ãƒœã‚¹ãƒˆãƒ³ãƒãƒƒã‚°": 7, 
    "ãƒªãƒ¥ãƒƒã‚¯": 7, "ãƒãƒ¼ãƒ": 7,
    # ã‚«ãƒ†ã‚´ãƒª8: å¸½å­
    "å¸½å­": 8, "ãƒãƒƒãƒˆ": 8, "ã‚­ãƒ£ãƒƒãƒ—": 8, "ãƒ‹ãƒƒãƒˆå¸½": 8, 
    "ãƒ™ãƒ¬ãƒ¼å¸½": 8, "ã‚­ãƒ£ã‚¹ã‚±ãƒƒãƒˆ": 8,
    # ã‚«ãƒ†ã‚´ãƒª9: æ™‚è¨ˆ
    "æ™‚è¨ˆ": 9,
    # ã‚«ãƒ†ã‚´ãƒª10: ã‚¸ãƒ¥ã‚¨ãƒªãƒ¼ãƒ»ã‚¢ã‚¯ã‚»ã‚µãƒªãƒ¼
    "ã‚¢ã‚¯ã‚»ã‚µãƒªãƒ¼": 10, "ã‚¸ãƒ¥ã‚¨ãƒªãƒ¼": 10, "ãƒãƒƒã‚¯ãƒ¬ã‚¹": 10, 
    "ãƒ–ãƒ¬ã‚¹ãƒ¬ãƒƒãƒˆ": 10, "ã‚¤ãƒ¤ãƒªãƒ³ã‚°": 10, "ãƒªãƒ³ã‚°": 10, 
    "ãƒ”ã‚¢ã‚¹": 10, "ãƒ–ãƒ­ãƒ¼ãƒ": 10, "ãƒ˜ã‚¢ã‚¢ã‚¯ã‚»ã‚µãƒªãƒ¼": 10,
    # ã‚«ãƒ†ã‚´ãƒª11: ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³é›‘è²¨
    "ãƒ™ãƒ«ãƒˆ": 11, "ã‚¹ã‚«ãƒ¼ãƒ•": 11, "ã‚¹ãƒˆãƒ¼ãƒ«": 11, "ãƒãƒ•ãƒ©ãƒ¼": 11, 
    "æ‰‹è¢‹": 11, "ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³é›‘è²¨": 11,
    "ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³å°ç‰©": 11, "ã‚¤ãƒ³ãƒ†ãƒªã‚¢": 11, "ã‚¹ãƒ†ãƒ¼ã‚·ãƒ§ãƒŠãƒªãƒ¼": 11,
    # ã‚«ãƒ†ã‚´ãƒª12: è²¡å¸ƒãƒ»å°ç‰©
    "è²¡å¸ƒ": 12, "ã‚­ãƒ¼ã‚±ãƒ¼ã‚¹ãƒ»ã‚­ãƒ¼ãƒ›ãƒ«ãƒ€ãƒ¼": 12, "å°ç‰©": 12,
    # ã‚«ãƒ†ã‚´ãƒª13: ãƒ¬ãƒƒã‚°ã‚¦ã‚§ã‚¢
    "ã‚½ãƒƒã‚¯ã‚¹ãƒ»é´ä¸‹": 13, "ã‚¿ã‚¤ãƒ„ãƒ»ã‚¹ãƒˆãƒƒã‚­ãƒ³ã‚°": 13, "ãƒ¬ãƒƒã‚°ã‚¦ã‚§ã‚¢": 13,
    # ã‚«ãƒ†ã‚´ãƒª14: ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¦ã‚§ã‚¢ãƒ»æ°´ç€
    "ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¦ã‚§ã‚¢": 14, "æ°´ç€": 14,
    # ã‚«ãƒ†ã‚´ãƒª15: ãƒ“ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ¼
    "ã‚³ã‚¹ãƒ¡": 15, "ãƒã‚¤ãƒ«": 15, "ãƒ•ãƒ¬ã‚°ãƒ©ãƒ³ã‚¹": 15, "ãƒœãƒ‡ã‚£ã‚±ã‚¢": 15,
    # ã‚«ãƒ†ã‚´ãƒª16: ã‚µãƒ³ã‚°ãƒ©ã‚¹ãƒ»ãƒ¡ã‚¬ãƒ
    "ã‚µãƒ³ã‚°ãƒ©ã‚¹": 16, "ãƒ¡ã‚¬ãƒ": 16,
    # ã‚«ãƒ†ã‚´ãƒª17: ãã®ä»–
    "å‚˜ãƒ»æ—¥å‚˜": 17, "å‚˜": 17, "ãã®ä»–": 17,
}

DEFAULT_IQON3000_CATEGORY = 17

DEEPFURNITURE_CATEGORIES = {
    1: "chair", 2: "table", 3: "sofa", 4: "bed", 5: "cabinet", 
    6: "lamp", 7: "bookshelf", 8: "desk", 9: "dresser", 
    10: "nightstand", 11: "other_furniture"
}

# =============================================================================
# çµ±ä¸€ã•ã‚ŒãŸæ­£è¦åŒ–é–¢æ•°
# =============================================================================

def normalize_features_unified(features):
    """
    çµ±ä¸€ã•ã‚ŒãŸç‰¹å¾´é‡æ­£è¦åŒ–
    L2æ­£è¦åŒ–ã®ã¿ã‚’é©ç”¨ï¼ˆZ-scoreæ­£è¦åŒ–ã¯å‰Šé™¤ï¼‰
    """
    if isinstance(features, torch.Tensor):
        # PyTorchãƒ†ãƒ³ã‚½ãƒ«ã®å ´åˆ
        norm = features.norm(dim=-1, keepdim=True)
        zero_norm_mask = (norm == 0)
        norm[zero_norm_mask] = 1e-9
        return features / norm
    else:
        # NumPyé…åˆ—ã®å ´åˆ
        norm = np.linalg.norm(features, axis=-1, keepdims=True)
        norm[norm == 0] = 1e-9
        return features / norm


# =============================================================================
# IQON3000 Dataset Class (ä¿®æ­£ç‰ˆ)
# =============================================================================

class IQON3000Dataset(Dataset):
    def __init__(self, iqon_dir, processor):
        self.iqon_dir = iqon_dir
        self.processor = processor
        self.item_info = {}  # item_id -> (user_id, coordinate_id, category, cat_id, filename)
        self.items = []
        self.category_corrections = 0  # ä¿®æ­£ã•ã‚ŒãŸã‚«ãƒ†ã‚´ãƒªã®æ•°
        self.validation_stats = {
            'total_processed': 0,
            'corrections_made': 0,
            'conflicts_detected': 0,
            'name_based_inferences': 0
        }
        self._load_data()
        print(f"IQON3000: Loaded {len(self.items)} unique items from {len(self.item_info)} entries")
        self._print_validation_stats()
    
    def _validate_and_correct_category(self, item_detail):
        """
        å•†å“åã¨ã‚«ãƒ†ã‚´ãƒªãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®æ•´åˆæ€§ã‚’ãƒã‚§ãƒƒã‚¯ã—ã€
        å¿…è¦ã«å¿œã˜ã¦å•†å“åã‹ã‚‰æ­£ã—ã„ã‚«ãƒ†ã‚´ãƒªã‚’æ¨å®šã™ã‚‹
        """
        self.validation_stats['total_processed'] += 1
        
        # æ—¢å­˜ã®ã‚«ãƒ†ã‚´ãƒªãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’å–å¾—
        cat_field = item_detail.get('category x color', item_detail.get('categoryName', ''))
        if not cat_field and 'category' in item_detail and isinstance(item_detail['category'], dict):
            cat_field = item_detail['category'].get('name', '')
        
        declared_category = cat_field.split(' Ã— ')[0].strip() if ' Ã— ' in cat_field else cat_field.strip()
        
        # å•†å“åã‚’å–å¾—
        item_name = item_detail.get('itemName', item_detail.get('name', '')).lower()
        
        # å•†å“åã‹ã‚‰å®Ÿéš›ã®ã‚«ãƒ†ã‚´ãƒªã‚’æ¨å®š
        inferred_category = self._infer_category_from_name(item_name)
        
        if inferred_category:
            self.validation_stats['name_based_inferences'] += 1
        
        # æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
        if inferred_category and declared_category:
            # ä¸¡æ–¹å­˜åœ¨ã™ã‚‹å ´åˆã¯æ•´åˆæ€§ã‚’ãƒã‚§ãƒƒã‚¯
            if self._categories_conflict(declared_category, inferred_category):
                self.validation_stats['conflicts_detected'] += 1
                self.validation_stats['corrections_made'] += 1
                
                # if self.category_corrections < 10:  # æœ€åˆã®10ä»¶ã®ã¿ãƒ­ã‚°å‡ºåŠ›
                #     print(f"âš ï¸ ã‚«ãƒ†ã‚´ãƒªä¸æ•´åˆæ¤œå‡º #{self.validation_stats['conflicts_detected']}:")
                #     print(f"   å•†å“å: {item_name[:50]}...")
                #     print(f"   å®£è¨€ã‚«ãƒ†ã‚´ãƒª: {declared_category}")
                #     print(f"   æ¨å®šã‚«ãƒ†ã‚´ãƒª: {inferred_category}")
                #     print(f"   â†’ æ¨å®šã‚«ãƒ†ã‚´ãƒªã‚’æ¡ç”¨")
                
                return inferred_category
        
        # å®£è¨€ã‚«ãƒ†ã‚´ãƒªãŒç©ºã§æ¨å®šã‚«ãƒ†ã‚´ãƒªãŒå­˜åœ¨ã™ã‚‹å ´åˆ
        if not declared_category and inferred_category:
            self.validation_stats['corrections_made'] += 1
            return inferred_category
        
        # æ¨å®šã‚«ãƒ†ã‚´ãƒªãŒå­˜åœ¨ã—ã€å®£è¨€ã‚«ãƒ†ã‚´ãƒªã¨çŸ›ç›¾ã—ãªã„å ´åˆã¯æ¨å®šã‚’å„ªå…ˆ
        return inferred_category or declared_category or ''

    def _infer_category_from_name(self, item_name):
        """å•†å“åã‹ã‚‰ã‚«ãƒ†ã‚´ãƒªã‚’æ¨å®š"""
        item_name = item_name.lower()
        
        # ã‚ˆã‚Šå…·ä½“çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‹ã‚‰å„ªå…ˆçš„ã«ãƒã‚§ãƒƒã‚¯
        # é•·ã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‹ã‚‰çŸ­ã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®é †ã§ãƒã‚§ãƒƒã‚¯
        category_keywords = {
            "ã‚µãƒ³ã‚°ãƒ©ã‚¹": "ã‚µãƒ³ã‚°ãƒ©ã‚¹",
            "sunglasses": "ã‚µãƒ³ã‚°ãƒ©ã‚¹",
            "ãƒ¡ã‚¬ãƒ": "ãƒ¡ã‚¬ãƒ",
            "çœ¼é¡": "ãƒ¡ã‚¬ãƒ",
            "glasses": "ãƒ¡ã‚¬ãƒ",
            "ã‚¢ã‚¤ã‚¦ã‚§ã‚¢": "ãƒ¡ã‚¬ãƒ",  
            "eyewear": "ã‚µãƒ³ã‚°ãƒ©ã‚¹",
            "ã‚¹ãƒ‹ãƒ¼ã‚«ãƒ¼": "ã‚·ãƒ¥ãƒ¼ã‚º",
            "ãƒŠã‚¤ã‚­": "ã‚·ãƒ¥ãƒ¼ã‚º",
            "nike": "ã‚·ãƒ¥ãƒ¼ã‚º",
            "ã‚¢ãƒ‡ã‚£ãƒ€ã‚¹": "ã‚·ãƒ¥ãƒ¼ã‚º", 
            "adidas": "ã‚·ãƒ¥ãƒ¼ã‚º",
            "ã‚³ãƒ³ãƒãƒ¼ã‚¹": "ã‚·ãƒ¥ãƒ¼ã‚º",
            "converse": "ã‚·ãƒ¥ãƒ¼ã‚º",
            "ã‚¿ãƒ³ã‚¸ãƒ¥ãƒ³": "ã‚·ãƒ¥ãƒ¼ã‚º",
            "tanjun": "ã‚·ãƒ¥ãƒ¼ã‚º",
            "ã‚¨ã‚¢ãƒãƒƒã‚¯ã‚¹": "ã‚·ãƒ¥ãƒ¼ã‚º",
            "air max": "ã‚·ãƒ¥ãƒ¼ã‚º",
            "airmax": "ã‚·ãƒ¥ãƒ¼ã‚º",
            "ã‚¹ã‚¿ãƒ³ã‚¹ãƒŸã‚¹": "ã‚·ãƒ¥ãƒ¼ã‚º",
            "stan smith": "ã‚·ãƒ¥ãƒ¼ã‚º",
            "ã‚³ãƒ¼ãƒˆ ãƒãƒ¼ãƒ­ã‚¦": "ã‚·ãƒ¥ãƒ¼ã‚º",
            "court barrow": "ã‚·ãƒ¥ãƒ¼ã‚º",
            "ãƒ–ãƒ¼ãƒ„": "ã‚·ãƒ¥ãƒ¼ã‚º",
            "boots": "ã‚·ãƒ¥ãƒ¼ã‚º",
            "ãƒ‘ãƒ³ãƒ—ã‚¹": "ã‚·ãƒ¥ãƒ¼ã‚º",
            "pumps": "ã‚·ãƒ¥ãƒ¼ã‚º",
            "ã‚µãƒ³ãƒ€ãƒ«": "ã‚·ãƒ¥ãƒ¼ã‚º",
            "sandal": "ã‚·ãƒ¥ãƒ¼ã‚º",
            "ãƒ­ãƒ¼ãƒ•ã‚¡ãƒ¼": "ã‚·ãƒ¥ãƒ¼ã‚º",
            "loafer": "ã‚·ãƒ¥ãƒ¼ã‚º",
            
            # ãƒãƒƒã‚°ç³»ï¼ˆæ¬¡ã«å„ªå…ˆï¼‰
            "ãƒãƒƒã‚°ãƒ‘ãƒƒã‚¯": "ãƒãƒƒã‚°",
            "ãƒãƒƒã‚¯ãƒ‘ãƒƒã‚¯": "ãƒãƒƒã‚°",
            "backpack": "ãƒãƒƒã‚°",
            "eastpak": "ãƒãƒƒã‚°",
            "ãƒªãƒ¥ãƒƒã‚¯ã‚µãƒƒã‚¯": "ãƒãƒƒã‚°",
            "ãƒªãƒ¥ãƒƒã‚¯": "ãƒãƒƒã‚°",
            "rucksack": "ãƒãƒƒã‚°",
            "ãƒˆãƒ¼ãƒˆãƒãƒƒã‚°": "ãƒãƒƒã‚°",
            "tote bag": "ãƒãƒƒã‚°",
            "ã‚·ãƒ§ãƒ«ãƒ€ãƒ¼ãƒãƒƒã‚°": "ãƒãƒƒã‚°",
            "shoulder bag": "ãƒãƒƒã‚°",
            "ãƒãƒ³ãƒ‰ãƒãƒƒã‚°": "ãƒãƒƒã‚°",
            "hand bag": "ãƒãƒƒã‚°",
            "ã‚¯ãƒ©ãƒƒãƒãƒãƒƒã‚°": "ãƒãƒƒã‚°",
            "clutch bag": "ãƒãƒƒã‚°",
            
            # å¸½å­ç³»ï¼ˆã‚­ãƒ£ãƒƒãƒ—ã¨ã®æ··åŒã‚’é¿ã‘ã‚‹ãŸã‚æ—©ã‚ã«ãƒã‚§ãƒƒã‚¯ï¼‰
            "ãƒ‹ãƒƒãƒˆå¸½": "å¸½å­",
            "knit cap": "å¸½å­",
            "ãƒ™ãƒ¬ãƒ¼å¸½": "å¸½å­",
            "beret": "å¸½å­",
            "ã‚­ãƒ£ã‚¹ã‚±ãƒƒãƒˆ": "å¸½å­",
            "casquette": "å¸½å­",
            "ãƒãƒƒãƒˆ": "å¸½å­",
            "hat": "å¸½å­",
            "ã‚­ãƒ£ãƒƒãƒ—": "å¸½å­",
            "cap": "å¸½å­",
            
            # ã‚¢ã‚¯ã‚»ã‚µãƒªãƒ¼ç³»
            "ãƒãƒƒã‚¯ãƒ¬ã‚¹": "ã‚¢ã‚¯ã‚»ã‚µãƒªãƒ¼",
            "necklace": "ã‚¢ã‚¯ã‚»ã‚µãƒªãƒ¼",
            "ãƒ–ãƒ¬ã‚¹ãƒ¬ãƒƒãƒˆ": "ã‚¢ã‚¯ã‚»ã‚µãƒªãƒ¼",
            "bracelet": "ã‚¢ã‚¯ã‚»ã‚µãƒªãƒ¼",
            "ã‚¤ãƒ¤ãƒªãƒ³ã‚°": "ã‚¢ã‚¯ã‚»ã‚µãƒªãƒ¼",
            "earring": "ã‚¢ã‚¯ã‚»ã‚µãƒªãƒ¼",
            "ãƒ”ã‚¢ã‚¹": "ã‚¢ã‚¯ã‚»ã‚µãƒªãƒ¼",
            "pierce": "ã‚¢ã‚¯ã‚»ã‚µãƒªãƒ¼",
            "æŒ‡è¼ª": "ã‚¢ã‚¯ã‚»ã‚µãƒªãƒ¼",
            "ãƒªãƒ³ã‚°": "ã‚¢ã‚¯ã‚»ã‚µãƒªãƒ¼",
            "ring": "ã‚¢ã‚¯ã‚»ã‚µãƒªãƒ¼",
            "ã‚¤ãƒ¤ãƒ¼ãƒãƒ•": "ã‚¢ã‚¯ã‚»ã‚µãƒªãƒ¼",
            "earmuff": "ã‚¢ã‚¯ã‚»ã‚µãƒªãƒ¼",
            "æ‰‹è¢‹": "ã‚¢ã‚¯ã‚»ã‚µãƒªãƒ¼",
            "glove": "ã‚¢ã‚¯ã‚»ã‚µãƒªãƒ¼",
            "ã‚°ãƒ­ãƒ¼ãƒ–": "ã‚¢ã‚¯ã‚»ã‚µãƒªãƒ¼",
            
            # ä¸€èˆ¬çš„ãªã‚«ãƒ†ã‚´ãƒªï¼ˆæœ€å¾Œã«ãƒã‚§ãƒƒã‚¯ï¼‰
            "ãƒ—ãƒªãƒ³ãƒˆtã‚·ãƒ£ãƒ„": "Tã‚·ãƒ£ãƒ„",
            "tã‚·ãƒ£ãƒ„": "Tã‚·ãƒ£ãƒ„",
            "t-shirt": "Tã‚·ãƒ£ãƒ„",
            "ã‚«ãƒƒãƒˆã‚½ãƒ¼": "Tã‚·ãƒ£ãƒ„",
            "cut sew": "Tã‚·ãƒ£ãƒ„",
            "ãƒ‘ãƒ¼ã‚«ãƒ¼": "ãƒ‘ãƒ¼ã‚«ãƒ¼",
            "hoodie": "ãƒ‘ãƒ¼ã‚«ãƒ¼",
            "ãƒ—ãƒ«ã‚ªãƒ¼ãƒãƒ¼": "ãƒ‘ãƒ¼ã‚«ãƒ¼",
            "pullover": "ãƒ‘ãƒ¼ã‚«ãƒ¼",
            "ã‚¸ãƒ£ã‚±ãƒƒãƒˆ": "ã‚¸ãƒ£ã‚±ãƒƒãƒˆ",
            "jacket": "ã‚¸ãƒ£ã‚±ãƒƒãƒˆ",
            "ãƒ–ãƒ«ã‚¾ãƒ³": "ã‚¸ãƒ£ã‚±ãƒƒãƒˆ",
            "blouson": "ã‚¸ãƒ£ã‚±ãƒƒãƒˆ",
            "ã‚³ãƒ¼ãƒˆ": "ã‚³ãƒ¼ãƒˆ",
            "coat": "ã‚³ãƒ¼ãƒˆ",
            "ãƒ‘ãƒ³ãƒ„": "ãƒ‘ãƒ³ãƒ„",
            "pants": "ãƒ‘ãƒ³ãƒ„",
            "ãƒ­ãƒ³ã‚°ãƒ‘ãƒ³ãƒ„": "ãƒ‘ãƒ³ãƒ„",
            "long pants": "ãƒ‘ãƒ³ãƒ„",
            "ã‚·ãƒ§ãƒ¼ãƒˆãƒ‘ãƒ³ãƒ„": "ã‚·ãƒ§ãƒ¼ãƒˆãƒ‘ãƒ³ãƒ„",
            "short pants": "ã‚·ãƒ§ãƒ¼ãƒˆãƒ‘ãƒ³ãƒ„",
            "ãƒ‡ãƒ‹ãƒ ": "ãƒ‘ãƒ³ãƒ„",
            "denim": "ãƒ‘ãƒ³ãƒ„",
            "ã‚¸ãƒ¼ãƒ³ã‚º": "ãƒ‘ãƒ³ãƒ„",
            "jeans": "ãƒ‘ãƒ³ãƒ„",
            "ã‚¹ã‚«ãƒ¼ãƒˆ": "ã‚¹ã‚«ãƒ¼ãƒˆ",
            "skirt": "ã‚¹ã‚«ãƒ¼ãƒˆ",
            "ãƒ¯ãƒ³ãƒ”ãƒ¼ã‚¹": "ãƒ¯ãƒ³ãƒ”ãƒ¼ã‚¹",
            "dress": "ãƒ¯ãƒ³ãƒ”ãƒ¼ã‚¹",
            "ãƒ‰ãƒ¬ã‚¹": "ãƒ¯ãƒ³ãƒ”ãƒ¼ã‚¹",
            "ãƒ‹ãƒƒãƒˆ": "ãƒ‹ãƒƒãƒˆ",
            "knit": "ãƒ‹ãƒƒãƒˆ",
            "ã‚»ãƒ¼ã‚¿ãƒ¼": "ãƒ‹ãƒƒãƒˆ",
            "sweater": "ãƒ‹ãƒƒãƒˆ",
        }
        
        # ã‚ˆã‚Šé•·ã„ï¼ˆå…·ä½“çš„ãªï¼‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‹ã‚‰é †ã«ãƒã‚§ãƒƒã‚¯
        sorted_keywords = sorted(category_keywords.items(), 
                               key=lambda x: len(x[0]), 
                               reverse=True)
        
        for keyword, category in sorted_keywords:
            if keyword in item_name:
                return category
        
        return None

    def _categories_conflict(self, declared_cat, inferred_cat):
        """ã‚«ãƒ†ã‚´ãƒªãŒçŸ›ç›¾ã—ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        if not declared_cat or not inferred_cat:
            return False
        
        # åŒã˜ã‚«ãƒ†ã‚´ãƒªãƒ•ã‚¡ãƒŸãƒªãƒ¼ã®å ´åˆã¯çŸ›ç›¾ãªã—
        shoe_family = ["ã‚·ãƒ¥ãƒ¼ã‚º", "é´", "ã‚¹ãƒ‹ãƒ¼ã‚«ãƒ¼", "ãƒ–ãƒ¼ãƒ„", "ã‚µãƒ³ãƒ€ãƒ«", "ãƒ‘ãƒ³ãƒ—ã‚¹", "ãƒ­ãƒ¼ãƒ•ã‚¡ãƒ¼"]
        bag_family = ["ãƒãƒƒã‚°", "ãƒªãƒ¥ãƒƒã‚¯", "ãƒˆãƒ¼ãƒˆãƒãƒƒã‚°", "ã‚·ãƒ§ãƒ«ãƒ€ãƒ¼ãƒãƒƒã‚°", "ãƒãƒ³ãƒ‰ãƒãƒƒã‚°", "ã‚¯ãƒ©ãƒƒãƒãƒãƒƒã‚°"]
        hat_family = ["å¸½å­", "ã‚­ãƒ£ãƒƒãƒ—", "ãƒãƒƒãƒˆ", "ãƒ‹ãƒƒãƒˆå¸½", "ãƒ™ãƒ¬ãƒ¼å¸½"]
        accessory_family = ["ã‚¢ã‚¯ã‚»ã‚µãƒªãƒ¼", "ãƒ”ã‚¢ã‚¹", "ãƒãƒƒã‚¯ãƒ¬ã‚¹", "ãƒ–ãƒ¬ã‚¹ãƒ¬ãƒƒãƒˆ", "ã‚¤ãƒ¤ãƒªãƒ³ã‚°", "æ‰‹è¢‹"]
        top_family = ["Tã‚·ãƒ£ãƒ„", "ã‚«ãƒƒãƒˆã‚½ãƒ¼", "ã‚·ãƒ£ãƒ„", "ãƒ–ãƒ©ã‚¦ã‚¹", "ãƒ‹ãƒƒãƒˆ", "ã‚»ãƒ¼ã‚¿ãƒ¼", "ãƒ‘ãƒ¼ã‚«ãƒ¼"]
        eyewear_family = ["ã‚µãƒ³ã‚°ãƒ©ã‚¹", "ãƒ¡ã‚¬ãƒ", "çœ¼é¡"]
        
        families = [shoe_family, bag_family, hat_family, accessory_family, top_family, eyewear_family]
        
        # åŒã˜ãƒ•ã‚¡ãƒŸãƒªãƒ¼å†…ãªã‚‰çŸ›ç›¾ãªã—
        for family in families:
            if declared_cat in family and inferred_cat in family:
                return False
        
        # æ˜ã‚‰ã‹ã«ç•°ãªã‚‹ã‚«ãƒ†ã‚´ãƒªãƒ•ã‚¡ãƒŸãƒªãƒ¼é–“ã¯çŸ›ç›¾
        declared_family = None
        inferred_family = None
        
        for i, family in enumerate(families):
            if declared_cat in family:
                declared_family = i
            if inferred_cat in family:
                inferred_family = i
        
        # ã©ã¡ã‚‰ã‚‚ä¸»è¦ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã«å±ã—ã€ç•°ãªã‚‹ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã®å ´åˆã¯çŸ›ç›¾
        if (declared_family is not None and inferred_family is not None and 
            declared_family != inferred_family):
            return True
        
        return False

    def _print_validation_stats(self):
        """ã‚«ãƒ†ã‚´ãƒªæ¤œè¨¼çµ±è¨ˆã‚’å‡ºåŠ›"""
        stats = self.validation_stats
        print(f"\nğŸ”§ ã‚«ãƒ†ã‚´ãƒªæ¤œè¨¼çµ±è¨ˆ:")
        print(f"  å‡¦ç†ã•ã‚ŒãŸã‚¢ã‚¤ãƒ†ãƒ : {stats['total_processed']}")
        print(f"  å•†å“åã‹ã‚‰ã®æ¨å®š: {stats['name_based_inferences']}")
        print(f"  çŸ›ç›¾æ¤œå‡º: {stats['conflicts_detected']}")
        print(f"  ä¿®æ­£é©ç”¨: {stats['corrections_made']}")
        if stats['total_processed'] > 0:
            print(f"  ä¿®æ­£ç‡: {stats['corrections_made']/stats['total_processed']*100:.1f}%")
            print(f"  æ¨å®šç‡: {stats['name_based_inferences']/stats['total_processed']*100:.1f}%")

    def _map_to_main_category_strict(self, japanese_category_name):
        # æ—¢å­˜ã®å®Ÿè£…ã‚’ãã®ã¾ã¾ç¶­æŒ
        if not japanese_category_name or japanese_category_name.strip() == '':
            return DEFAULT_IQON3000_CATEGORY, True
        if japanese_category_name in IQON3000_CATEGORY_MAPPING:
            return IQON3000_CATEGORY_MAPPING[japanese_category_name], False
        for key in sorted(IQON3000_CATEGORY_MAPPING.keys(), key=len, reverse=True):
            if key in japanese_category_name:
                return IQON3000_CATEGORY_MAPPING[key], False
        if japanese_category_name.strip().isdigit():
            return DEFAULT_IQON3000_CATEGORY, True
        return DEFAULT_IQON3000_CATEGORY, True
    
    def _load_data(self):
        print(f"Starting IQON3000 data loading with CATEGORY VALIDATION from: {self.iqon_dir}")
        if not os.path.isdir(self.iqon_dir):
            print(f"Error: IQON3000 directory not found: {self.iqon_dir}")
            return

        # çµ±è¨ˆæƒ…å ±
        source_category_counts = {}
        unmapped_to_default_count = 0
        json_decode_errors = 0
        items_missing_id_in_json = 0
        items_missing_image = 0
        duplicate_items_skipped = 0
        
        user_id_dirs = [d for d in os.listdir(self.iqon_dir) if os.path.isdir(os.path.join(self.iqon_dir, d))]
        print(f"Found {len(user_id_dirs)} user directories.")

        # é‡è¤‡è¿½è·¡ç”¨ã‚»ãƒƒãƒˆ
        seen_item_ids = set()

        for user_id_str in tqdm(user_id_dirs, desc="Processing IQON3000 Users with validation"):
            current_user_path = os.path.join(self.iqon_dir, user_id_str)
            try:
                coordinate_id_dirs = [d for d in os.listdir(current_user_path) if os.path.isdir(os.path.join(current_user_path, d))]
            except OSError:
                continue

            for coordinate_id_str in coordinate_id_dirs:
                current_coordinate_path = os.path.join(current_user_path, coordinate_id_str)
                coordinate_json_path = os.path.join(current_coordinate_path, f"{coordinate_id_str}.json")

                if not os.path.exists(coordinate_json_path):
                    continue
                
                try:
                    with open(coordinate_json_path, 'r', encoding='utf-8') as f:
                        outfit_data = json.load(f)
                except json.JSONDecodeError:
                    json_decode_errors += 1
                    continue
                
                items_list = outfit_data.get('items', [])
                for item_detail in items_list:
                    true_item_id = item_detail.get('itemId')
                    if not true_item_id:
                        items_missing_id_in_json += 1
                        continue
                    true_item_id_str = str(true_item_id)

                    # é‡è¤‡ãƒã‚§ãƒƒã‚¯: åŒã˜item_idã¯1å›ã®ã¿å‡¦ç†
                    if true_item_id_str in seen_item_ids:
                        duplicate_items_skipped += 1
                        continue
                    seen_item_ids.add(true_item_id_str)

                    # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«åã‚’æ±ºå®š
                    image_filename = f"{true_item_id_str}_m.jpg"
                    full_image_path = os.path.join(current_coordinate_path, image_filename)
                    
                    if not os.path.exists(full_image_path):
                        img_url = item_detail.get('imgUrl', '')
                        if img_url:
                            potential_filename = os.path.basename(img_url)
                            if potential_filename.endswith(("_m.jpg", ".jpg", ".png", ".jpeg")):
                                alt_path = os.path.join(current_coordinate_path, potential_filename)
                                if os.path.exists(alt_path):
                                    image_filename = potential_filename
                                    full_image_path = alt_path
                                else:
                                    items_missing_image += 1; continue
                            else:
                                items_missing_image += 1; continue
                        else:
                            items_missing_image += 1; continue

                    # ğŸ†• ã‚«ãƒ†ã‚´ãƒªæ¤œè¨¼ãƒ»ä¿®æ­£ã‚’è¿½åŠ 
                    corrected_category = self._validate_and_correct_category(item_detail)
                    
                    if corrected_category:
                        source_category_counts[corrected_category] = source_category_counts.get(corrected_category, 0) + 1
                    
                    main_cat_id, was_fallback = self._map_to_main_category_strict(corrected_category)
                    if was_fallback:
                        unmapped_to_default_count += 1
                    
                    # ã‚¢ã‚¤ãƒ†ãƒ æƒ…å ±ã‚’ä¿å­˜ï¼ˆé‡è¤‡ãªã—ï¼‰
                    self.item_info[true_item_id_str] = (user_id_str, coordinate_id_str, corrected_category, main_cat_id, image_filename)
                    self.items.append(true_item_id_str)
        
        print(f"\nIQON3000 Data Loading Summary (with Category Validation):")
        print(f"  Total items processed: {self.validation_stats['total_processed']}")
        print(f"  Items skipped (duplicates): {duplicate_items_skipped}")
        print(f"  Items skipped (missing 'itemId'): {items_missing_id_in_json}")
        print(f"  Items skipped (missing image): {items_missing_image}")
        print(f"  JSON decode errors: {json_decode_errors}")
        print(f"  Unique item entries: {len(self.item_info)}")
        print(f"  Valid items: {len(self.items)}")
        print(f"  Items mapped to default category: {unmapped_to_default_count}")


    # æ—¢å­˜ã® __len__ ã¨ __getitem__ ãƒ¡ã‚½ãƒƒãƒ‰ã¯ãã®ã¾ã¾ç¶­æŒ
    def __len__(self):
        return len(self.items)

    def __getitem__(self, idx):
        true_item_id_str = self.items[idx]
        user_id_str, coordinate_id_str, _, main_cat_id, image_filename = self.item_info[true_item_id_str]
        img_path = os.path.join(self.iqon_dir, user_id_str, coordinate_id_str, image_filename)
        
        try:
            image = Image.open(img_path).convert("RGB")
            inputs = self.processor(images=image, return_tensors="pt")
            return {key: val.squeeze(0) for key, val in inputs.items()}, true_item_id_str, main_cat_id
        except (UnidentifiedImageError, FileNotFoundError, OSError) as e:
            dummy_size = (224, 224)
            dummy_image = Image.new("RGB", dummy_size, color="gray")
            dummy_input = self.processor(images=dummy_image, return_tensors="pt")
            return {key: val.squeeze(0) for key, val in dummy_input.items()}, true_item_id_str, DEFAULT_IQON3000_CATEGORY

# =============================================================================
# ä¿®æ­£ã•ã‚ŒãŸCollateé–¢æ•°
# =============================================================================

def collate_fn(batch):
    """ä¿®æ­£ç‰ˆcollateé–¢æ•° - ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å¼·åŒ–"""
    inputs_dict = {}
    item_ids_list, main_cat_ids_list = [], []
    
    for b_input, item_id, main_cat_id in batch:
        if b_input is None: 
            continue
        for key, val in b_input.items(): 
            inputs_dict.setdefault(key, []).append(val)
        item_ids_list.append(item_id)
        main_cat_ids_list.append(main_cat_id)
    
    if not inputs_dict: 
        return {'pixel_values': torch.empty(0, 3, 224, 224)}, [], []
    
    try:
        final_inputs = {}
        for key, val_list in inputs_dict.items():
            if val_list:
                final_inputs[key] = torch.stack(val_list)
            elif key == 'pixel_values': 
                final_inputs[key] = torch.empty(0, 3, 224, 224)
        return final_inputs, item_ids_list, main_cat_ids_list
    except RuntimeError as e: 
        print(f"collate_fn error: {e}")
        raise e

# =============================================================================
# ä¿®æ­£ã•ã‚ŒãŸãƒ‘ãƒ‡ã‚£ãƒ³ã‚°é–¢æ•°
# =============================================================================

def pad_or_truncate_df(data_list, max_len, pad_value, dtype_override=None):
    """ä¿®æ­£ç‰ˆãƒ‘ãƒ‡ã‚£ãƒ³ã‚°é–¢æ•°"""
    if not data_list: 
        if isinstance(pad_value, np.ndarray): 
            return np.array([pad_value.copy() for _ in range(max_len)], dtype=dtype_override or pad_value.dtype)
        return np.array([pad_value] * max_len, dtype=dtype_override or type(pad_value))
    
    processed_list = list(data_list[:max_len]) 
    num_padding = max_len - len(processed_list)
    
    if num_padding > 0:
        if processed_list and isinstance(processed_list[0], np.ndarray): 
            processed_list.extend([pad_value.copy() for _ in range(num_padding)])
        elif processed_list: 
            processed_list.extend([pad_value] * num_padding)
        else:
            if isinstance(pad_value, np.ndarray): 
                processed_list = [pad_value.copy() for _ in range(max_len)]
            else: 
                processed_list = [pad_value] * max_len
    
    final_dtype = dtype_override
    if not final_dtype:
        if processed_list and isinstance(processed_list[0], np.ndarray): 
            final_dtype = processed_list[0].dtype
        elif processed_list: 
            final_dtype = type(processed_list[0])
        elif isinstance(pad_value, np.ndarray): 
            final_dtype = pad_value.dtype
        else: 
            final_dtype = type(pad_value)
    
    return np.array(processed_list, dtype=final_dtype)

# =============================================================================
# IQON3000å‡¦ç† (ä¿®æ­£ç‰ˆ)
# =============================================================================

def split_data_by_users(set_data_for_conversion, test_size=0.15, val_size=0.15, random_state=42):
    """
    ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ™ãƒ¼ã‚¹ã§ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ†å‰²
    åŒã˜ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ‡ãƒ¼ã‚¿ãŒè¤‡æ•°ã®åˆ†å‰²ã«æ··åœ¨ã—ãªã„ã‚ˆã†ã«ã™ã‚‹
    """
    print("ğŸ”„ ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ™ãƒ¼ã‚¹ã§ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ†å‰²ã‚’å®Ÿè¡Œ...")
    
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã”ã¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    user_to_data = {}
    for data_item in set_data_for_conversion:
        if isinstance(data_item, tuple) and len(data_item) == 2:
            (user_id, coord_id), items_data = data_item
            if user_id not in user_to_data:
                user_to_data[user_id] = []
            user_to_data[user_id].append(data_item)
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥å½¢å¼ã®å ´åˆã¯è­¦å‘Š
            print(f"âš ï¸ è­¦å‘Š: äºˆæœŸã—ãªã„ãƒ‡ãƒ¼ã‚¿å½¢å¼ã§ã™")
            continue
    
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼çµ±è¨ˆ
    user_data_counts = {user_id: len(data_list) for user_id, data_list in user_to_data.items()}
    total_users = len(user_to_data)
    total_scenes = sum(user_data_counts.values())
    
    print(f"ğŸ“Š ãƒ¦ãƒ¼ã‚¶ãƒ¼çµ±è¨ˆ:")
    print(f"  ç·ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°: {total_users}")
    print(f"  ç·ã‚·ãƒ¼ãƒ³æ•°: {total_scenes}")
    print(f"  ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚ãŸã‚Šå¹³å‡ã‚·ãƒ¼ãƒ³æ•°: {total_scenes/total_users:.1f}")
    
    # ãƒ‡ãƒ¼ã‚¿é‡ã®å¤šã„ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰é †ã«ã‚½ãƒ¼ãƒˆ
    sorted_users = sorted(user_to_data.keys(), 
                         key=lambda u: len(user_to_data[u]), 
                         reverse=True)
    
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’åˆ†å‰²
    np.random.seed(random_state)
    shuffled_users = np.random.permutation(sorted_users)
    
    # ç´¯ç©ãƒ‡ãƒ¼ã‚¿é‡ã§åˆ†å‰²ç‚¹ã‚’æ±ºå®š
    cumulative_scenes = 0
    train_users, val_users, test_users = [], [], []
    
    for user_id in shuffled_users:
        user_scene_count = len(user_to_data[user_id])
        
        # ç¾åœ¨ã®ç´¯ç©ç‡ã‚’è¨ˆç®—
        current_ratio = cumulative_scenes / total_scenes
        
        if current_ratio < (1 - test_size - val_size):
            train_users.append(user_id)
        elif current_ratio < (1 - test_size):
            val_users.append(user_id)
        else:
            test_users.append(user_id)
        
        cumulative_scenes += user_scene_count
    
    # å„åˆ†å‰²ã«ãƒ‡ãƒ¼ã‚¿ã‚’å‰²ã‚Šå½“ã¦
    train_data = []
    val_data = []
    test_data = []
    
    for user_id in train_users:
        train_data.extend(user_to_data[user_id])
    
    for user_id in val_users:
        val_data.extend(user_to_data[user_id])
    
    for user_id in test_users:
        test_data.extend(user_to_data[user_id])
    
    # çµ±è¨ˆã‚’è¡¨ç¤º
    print(f"\nğŸ“ˆ ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ™ãƒ¼ã‚¹åˆ†å‰²çµæœ:")
    print(f"  Train: {len(train_users)} users, {len(train_data)} scenes")
    print(f"  Val:   {len(val_users)} users, {len(val_data)} scenes")
    print(f"  Test:  {len(test_users)} users, {len(test_data)} scenes")
    
    # é‡è¤‡ãƒã‚§ãƒƒã‚¯
    train_user_set = set(train_users)
    val_user_set = set(val_users)
    test_user_set = set(test_users)
    
    train_val_overlap = train_user_set & val_user_set
    train_test_overlap = train_user_set & test_user_set
    val_test_overlap = val_user_set & test_user_set
    
    print(f"\nâœ… é‡è¤‡ãƒã‚§ãƒƒã‚¯:")
    print(f"  Train-Valé‡è¤‡: {len(train_val_overlap)} users")
    print(f"  Train-Testé‡è¤‡: {len(train_test_overlap)} users")
    print(f"  Val-Testé‡è¤‡: {len(val_test_overlap)} users")
    
    if len(train_val_overlap) == 0 and len(train_test_overlap) == 0 and len(val_test_overlap) == 0:
        print("  ğŸ‰ ãƒ¦ãƒ¼ã‚¶ãƒ¼é‡è¤‡ãªã—ï¼å®Œå…¨åˆ†é›¢æˆåŠŸ")
    else:
        print("  âš ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼é‡è¤‡ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ")
    
    return train_data, val_data, test_data

def process_iqon3000(input_dir, output_dir, batch_size=32):
    """ä¿®æ­£ç‰ˆ: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ™ãƒ¼ã‚¹åˆ†å‰² + æ­£è¦åŒ–çµ±ä¸€"""
    print(f"Processing IQON3000 dataset with USER-BASED splitting")
    print("Key improvements: User-based split + proper normalization")
    
    os.makedirs(output_dir, exist_ok=True)
    
    # ãƒ‡ãƒã‚¤ã‚¹ã¨ãƒ¢ãƒ‡ãƒ«ã®è¨­å®š
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")
    
    model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
    processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
    model = model.to(device)
    model.eval()
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®ä½œæˆ
    dataset = IQON3000Dataset(input_dir, processor)
    if len(dataset) == 0:
        print("Dataset is empty. Skipping feature extraction.")
        return
    
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, 
                            collate_fn=collate_fn, num_workers=4, 
                            pin_memory=device.type == 'cuda')
    
    # ç‰¹å¾´é‡æŠ½å‡ºï¼ˆæ­£è¦åŒ–ãªã—ï¼‰
    item_features = {}
    item_main_categories = {}
    item_to_coordinate = {}
    coordinate_to_items = {}
    
    print(f"Starting RAW feature extraction for {len(dataset)} unique items...")
    
    with torch.no_grad():
        for batch_inputs, batch_true_item_ids, batch_main_cat_ids in tqdm(dataloader, desc="Extracting IQON3000 features"):
            if not batch_true_item_ids: continue
            if not batch_inputs or 'pixel_values' not in batch_inputs or batch_inputs['pixel_values'].numel() == 0: 
                continue
            
            batch_inputs = {k: v.to(device) for k, v in batch_inputs.items()}
            
            try: 
                img_feats = model.get_image_features(**batch_inputs)
            except Exception as e: 
                print(f"Error during get_image_features: {e}")
                continue 
            
            # âœ… æ­£è¦åŒ–ã‚’å‰Šé™¤ - ç”Ÿã®ç‰¹å¾´é‡ã‚’ä¿å­˜
            img_feats_np = img_feats.cpu().numpy()
            
            for i, true_item_id in enumerate(batch_true_item_ids):
                if true_item_id is None: continue
                
                item_features[true_item_id] = img_feats_np[i]
                item_main_categories[true_item_id] = batch_main_cat_ids[i]
                
                # ã‚³ãƒ¼ãƒ‡ã‚£ãƒãƒ¼ãƒˆæƒ…å ±ã‚’å–å¾—
                if true_item_id in dataset.item_info:
                    _, coordinate_id_for_item, _, _, _ = dataset.item_info[true_item_id]
                    item_to_coordinate[true_item_id] = coordinate_id_for_item 
                    coordinate_to_items.setdefault(coordinate_id_for_item, []).append(true_item_id)
    
    print(f"Extracted RAW features for {len(item_features)} unique items from {len(coordinate_to_items)} coordinate sets.")
    
    # æœ‰åŠ¹ãªã‚³ãƒ¼ãƒ‡ã‚£ãƒãƒ¼ãƒˆã®é¸åˆ¥ï¼ˆãƒ‘ã‚¹å½¢å¼ã®ã‚­ãƒ¼ã‚’ä½¿ç”¨ï¼‰
    valid_coordinates = {} 
    for coord_id, true_item_ids_in_coord in coordinate_to_items.items():
        valid_items_in_this_coord = [
            tid for tid in true_item_ids_in_coord 
            if tid in item_main_categories and 1 <= item_main_categories[tid] <= 17
        ]
        if len(valid_items_in_this_coord) >= 4:
            if true_item_ids_in_coord and true_item_ids_in_coord[0] in dataset.item_info:
                user_id_for_coord, _, _, _, _ = dataset.item_info[true_item_ids_in_coord[0]]
                path_key = (user_id_for_coord, coord_id)
                valid_coordinates[path_key] = valid_items_in_this_coord
    
    print(f"Found {len(valid_coordinates)} valid coordinate sets for conversion.")

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå¤‰æ›
    set_data_for_conversion = []
    if valid_coordinates:
        for (user_id, coord_id_key), true_item_ids_in_valid_coord in valid_coordinates.items():
            items_data_for_this_coord = []
            for true_item_id in true_item_ids_in_valid_coord:
                if true_item_id in item_main_categories and true_item_id in item_features:
                     items_data_for_this_coord.append((
                         true_item_id, 
                         item_main_categories[true_item_id], 
                         item_features[true_item_id]  # ç”Ÿã®ç‰¹å¾´é‡
                     ))
            if len(items_data_for_this_coord) >= 2:
                 set_data_for_conversion.append(((user_id, coord_id_key), items_data_for_this_coord))

    if set_data_for_conversion:
        # âœ… ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ™ãƒ¼ã‚¹åˆ†å‰²
        train_sets, val_sets, test_sets = split_data_by_users(
            set_data_for_conversion, 
            test_size=0.2, 
            val_size=0.1, 
            random_state=42
        )
        
        print(f"USER-BASED Dataset split: Train {len(train_sets)}, Validation {len(val_sets)}, Test {len(test_sets)}")
        
        # âœ… å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ­£è¦åŒ–çµ±è¨ˆé‡ã‚’è¨ˆç®—
        train_normalization_stats = compute_normalization_stats(train_sets)
        
        # âœ… çµ±ä¸€ã•ã‚ŒãŸæ­£è¦åŒ–ã§å…¨åˆ†å‰²ã‚’å‡¦ç†
        def process_and_save_iqon3000_split(dataset_with_orig_ids_list, split_name, norm_stats, base_output_dir):
            if not dataset_with_orig_ids_list:
                return None
            return convert_to_deepfurniture_format_fixed(
                dataset_with_orig_ids_list, 
                os.path.join(base_output_dir, f'{split_name}.pkl'),
                norm_stats
            )

        if train_sets:
            process_and_save_iqon3000_split(train_sets, 'train', train_normalization_stats, output_dir)
        if val_sets:
            process_and_save_iqon3000_split(val_sets, 'validation', train_normalization_stats, output_dir)
        if test_sets:
            process_and_save_iqon3000_split(test_sets, 'test', train_normalization_stats, output_dir)
    else:
        print("No valid coordinate sets for conversion.")
    
    # ã‚«ãƒ†ã‚´ãƒªæƒ…å ±ã®ä¿å­˜
    category_info = {
        'main_categories_def': IQON3000_CATEGORIES, 
        'main_category_stats': {}
    }
    for item_id_stat in item_features: 
        main_cat = item_main_categories.get(item_id_stat, DEFAULT_IQON3000_CATEGORY)
        category_info['main_category_stats'][main_cat] = category_info['main_category_stats'].get(main_cat, 0) + 1
    
    with open(os.path.join(output_dir, 'category_info.pkl'), 'wb') as f: 
        pickle.dump(category_info, f)
    
    json_safe = {
        'main_categories_def': category_info['main_categories_def'], 
        'main_category_stats': {str(k):v for k,v in category_info['main_category_stats'].items()}
    }
    with open(os.path.join(output_dir, 'category_info.json'), 'w', encoding='utf-8') as f: 
        json.dump(json_safe, f, ensure_ascii=False, indent=2)

    # ã‚«ãƒ†ã‚´ãƒªä¸­å¿ƒã®è¨ˆç®—
    compute_iqon3000_category_centers_fixed(output_dir)
    
    print(f"IQON3000 processing complete with USER-BASED split. Files saved to {output_dir}")

def compute_normalization_stats(train_sets):
    """å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ­£è¦åŒ–çµ±è¨ˆé‡ã‚’è¨ˆç®—"""
    print("Computing normalization statistics from training data...")
    
    all_train_features = []
    
    for (user_id, coord_id), items_data in train_sets:
        for item_id, category, feature in items_data:
            all_train_features.append(feature)
    
    if not all_train_features:
        print("Warning: No training features found")
        return None
    
    # ç‰¹å¾´é‡ã‚’çµåˆ
    train_features_array = np.stack(all_train_features)
    
    # L2æ­£è¦åŒ–çµ±è¨ˆé‡ï¼ˆå®Ÿéš›ã«ã¯L2æ­£è¦åŒ–ã¯çµ±è¨ˆé‡ä¸è¦ã ãŒã€ä¸€è²«æ€§ã®ãŸã‚ï¼‰
    feature_norms = np.linalg.norm(train_features_array, axis=1, keepdims=True)
    feature_norms[feature_norms == 0] = 1e-9
    
    # Z-scoreæ­£è¦åŒ–ç”¨çµ±è¨ˆé‡ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    feature_mean = np.mean(train_features_array, axis=0)
    feature_std = np.std(train_features_array, axis=0)
    feature_std[feature_std == 0] = 1e-9
    
    normalization_stats = {
        'method': 'l2_norm',  # 'l2_norm' or 'z_score'
        'train_mean': feature_mean,
        'train_std': feature_std,
        'train_feature_count': len(all_train_features)
    }
    
    print(f"Computed normalization stats from {len(all_train_features)} training features")
    print(f"Feature mean magnitude: {np.mean(np.abs(feature_mean)):.6f}")
    print(f"Feature std mean: {np.mean(feature_std):.6f}")
    
    return normalization_stats

def apply_normalization_with_stats(features, normalization_stats):
    """çµ±è¨ˆé‡ã‚’ä½¿ã£ã¦æ­£è¦åŒ–ã‚’é©ç”¨"""
    if normalization_stats is None:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: L2æ­£è¦åŒ–
        norm = np.linalg.norm(features, axis=-1, keepdims=True)
        norm[norm == 0] = 1e-9
        return features / norm
    
    method = normalization_stats.get('method', 'l2_norm')
    
    if method == 'l2_norm':
        # L2æ­£è¦åŒ–ï¼ˆçµ±è¨ˆé‡ä¸è¦ï¼‰
        norm = np.linalg.norm(features, axis=-1, keepdims=True)
        norm[norm == 0] = 1e-9
        return features / norm
    
    elif method == 'z_score':
        # Z-scoreæ­£è¦åŒ–ï¼ˆå­¦ç¿’ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆé‡ä½¿ç”¨ï¼‰
        train_mean = normalization_stats['train_mean']
        train_std = normalization_stats['train_std']
        return (features - train_mean) / train_std
    
    else:
        raise ValueError(f"Unknown normalization method: {method}")

# =============================================================================
# ä¿®æ­£ç‰ˆ: DeepFurnitureå½¢å¼å¤‰æ›é–¢æ•°
# =============================================================================

def convert_to_deepfurniture_format_fixed(sets_of_items_data, output_file, normalization_stats=None, max_item_num=10):
    """æ­£è¦åŒ–çµ±è¨ˆé‡ã‚’ä½¿ç”¨ã—ã¦DeepFurnitureå½¢å¼ã«å¤‰æ›"""
    q_feats_list, t_feats_list, q_main_cats_list, t_main_cats_list, q_ids_list, t_ids_list = [], [], [], [], [], []
    scene_ids_list, set_sizes_list = [], []
    skipped_sets_count = 0
    overlap_warning_count = 0
    
    print(f"Converting with normalization stats: {normalization_stats['method'] if normalization_stats else 'fallback_l2'}")
    
    for set_idx_df, items_in_current_outfit_data in enumerate(tqdm(sets_of_items_data, desc=f"Converting ({os.path.basename(output_file)})")):
        # ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã®å‡¦ç†ï¼ˆæ—¢å­˜ã¨åŒã˜ï¼‰
        scene_id = None
        items_to_process = None
        
        if isinstance(items_in_current_outfit_data, tuple) and len(items_in_current_outfit_data) == 2:
            first_elem, second_elem = items_in_current_outfit_data
            if isinstance(first_elem, tuple) and len(first_elem) == 2:
                user_id, coord_id = first_elem
                scene_id = f"{user_id}/{coord_id}"
                items_to_process = second_elem
            else:
                items_to_process = items_in_current_outfit_data
                scene_id = f"scene_{set_idx_df}"
        else:
            items_to_process = items_in_current_outfit_data
            scene_id = f"scene_{set_idx_df}"
        
        if len(items_to_process) < 2: 
            skipped_sets_count += 1
            continue
        
        random.shuffle(items_to_process)
        split_idx = len(items_to_process) // 2
        if split_idx == 0: 
            skipped_sets_count += 1
            continue
        
        query_item_data_list = items_to_process[:split_idx]
        target_item_data_list = items_to_process[split_idx:]
        
        if not query_item_data_list or not target_item_data_list: 
            skipped_sets_count += 1
            continue
        
        # é‡è¤‡ãƒã‚§ãƒƒã‚¯
        query_ids = set([d[0] for d in query_item_data_list])
        target_ids = set([d[0] for d in target_item_data_list])
        
        if query_ids & target_ids:
            overlap_warning_count += 1
            target_item_data_list = [d for d in target_item_data_list if d[0] not in query_ids]
            if not target_item_data_list:
                skipped_sets_count += 1
                continue
        
        q_ids_raw, q_main_cats_raw, q_feats_raw = zip(*[(d[0], d[1], d[2]) for d in query_item_data_list])
        t_ids_raw, t_main_cats_raw, t_feats_raw = zip(*[(d[0], d[1], d[2]) for d in target_item_data_list])
        
        if not q_feats_raw or not t_feats_raw: 
            skipped_sets_count += 1
            continue
        
        # âœ… çµ±ä¸€ã•ã‚ŒãŸæ­£è¦åŒ–ã‚’é©ç”¨
        q_feats_normalized = [apply_normalization_with_stats(feat, normalization_stats) for feat in q_feats_raw]
        t_feats_normalized = [apply_normalization_with_stats(feat, normalization_stats) for feat in t_feats_raw]
        
        feature_dim = q_feats_normalized[0].shape[0]
        zero_feature_pad = np.zeros(feature_dim, dtype=np.float32)
        
        q_ids_list.append(pad_or_truncate_df(q_ids_raw, max_item_num, '0', object))
        q_main_cats_list.append(pad_or_truncate_df(q_main_cats_raw, max_item_num, 0, np.int32))
        q_feats_list.append(pad_or_truncate_df(q_feats_normalized, max_item_num, zero_feature_pad, np.float32))
        t_ids_list.append(pad_or_truncate_df(t_ids_raw, max_item_num, '0', object))
        t_main_cats_list.append(pad_or_truncate_df(t_main_cats_raw, max_item_num, 0, np.int32))
        t_feats_list.append(pad_or_truncate_df(t_feats_normalized, max_item_num, zero_feature_pad, np.float32))
        
        scene_ids_list.append(scene_id)
        set_sizes_list.append(len(items_to_process))
    
    if skipped_sets_count > 0: 
        print(f"Converting ({os.path.basename(output_file)}): Skipped {skipped_sets_count} sets.")
    if overlap_warning_count > 0:
        print(f"Converting ({os.path.basename(output_file)}): Resolved {overlap_warning_count} query-target overlaps.")
    
    if not q_feats_list: 
        print(f"Warning: No data to save for {output_file}.")
        return {}
    
    # æ­£è¦åŒ–çµ±è¨ˆé‡ã‚‚ä¿å­˜
    normalization_info = {
        'applied_normalization': normalization_stats,
        'split_name': os.path.basename(output_file).replace('.pkl', ''),
        'feature_count': len(q_feats_list) * max_item_num * 2
    }
    
    df_tuple = (
        np.array(q_feats_list, dtype=np.float32),      # 0: query_features
        np.array(t_feats_list, dtype=np.float32),      # 1: target_features  
        np.array(scene_ids_list, dtype=object),        # 2: scene_ids
        np.array(q_main_cats_list, dtype=np.int32),    # 3: query_categories
        np.array(t_main_cats_list, dtype=np.int32),    # 4: target_categories
        np.array(set_sizes_list, dtype=np.int32),      # 5: set_sizes
        np.array(q_ids_list, dtype=object),            # 6: query_item_ids
        np.array(t_ids_list, dtype=object),            # 7: target_item_ids
        normalization_info                             # 8: normalization_info
    )
    
    try:
        with open(output_file, 'wb') as f: 
            pickle.dump(df_tuple, f)
        print(f"âœ… Saved {len(q_feats_list)} sets to {output_file} with unified normalization.")
    except Exception as e_save: 
        print(f"Error saving {output_file}: {e_save}")
        return {}
    
    return {'query_categories': df_tuple[3], 'target_categories': df_tuple[4]}

    
# =============================================================================
# ä¿®æ­£ç‰ˆ: ã‚«ãƒ†ã‚´ãƒªä¸­å¿ƒè¨ˆç®—é–¢æ•°
# =============================================================================

def compute_iqon3000_category_centers_fixed(features_dir):
    """ä¿®æ­£ç‰ˆ: IQON3000ã‚«ãƒ†ã‚´ãƒªä¸­å¿ƒè¨ˆç®— - è¾æ›¸å½¢å¼ã€æ­£è¦åŒ–çµ±ä¸€"""
    print("\nComputing IQON3000 category centers (fixed version)...")
    
    train_path = os.path.join(features_dir, 'train.pkl')
    if not os.path.exists(train_path):
        print(f"Error: {train_path} not found")
        return
    
    with open(train_path, 'rb') as f:
        train_data = pickle.load(f)
    
    if len(train_data) >= 9:
        query_features, target_features, scene_ids, query_categories, target_categories, set_sizes, query_item_ids, target_item_ids, normalization_info = train_data
    elif len(train_data) >= 8:
        query_features, target_features, scene_ids, query_categories, target_categories, set_sizes, query_item_ids, target_item_ids = train_data
    else:
        print(f"Error: Unexpected data format in {train_path}")
        return
    
    print(f"Loaded training data: {len(query_features)} sets")
    
    # IQON3000ã®7ã‚«ãƒ†ã‚´ãƒªID (1-7)
    active_main_cat_ids = list(range(1, 18))  # 1-17
    
    # ç‰¹å¾´é‡æ¬¡å…ƒã‚’å–å¾—
    if len(query_features) > 0 and query_features[0].shape[-1] > 0:
        embedding_dim = query_features[0].shape[-1]
    else:
        print("Error: No valid features found")
        return
    
    print(f"Feature dimension: {embedding_dim}")
    
    # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«å…¨ç‰¹å¾´é‡ã‚’åé›†
    features_per_main_category = {cat_id: [] for cat_id in active_main_cat_ids}
    
    # ã‚¯ã‚¨ãƒªã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®ç‰¹å¾´é‡ã‚’å‡¦ç†
    all_features = np.concatenate([query_features, target_features], axis=0)
    all_categories = np.concatenate([query_categories, target_categories], axis=0)
    
    print("Collecting features by category...")
    valid_feature_count = 0
    invalid_feature_count = 0
    
    for set_idx in tqdm(range(len(all_features)), desc="Processing sets"):
        for item_idx in range(len(all_features[set_idx])):
            feat = all_features[set_idx][item_idx]
            cat = all_categories[set_idx][item_idx]
            
            # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆã‚¼ãƒ­ãƒ™ã‚¯ãƒˆãƒ«ï¼‰ã¨ã‚«ãƒ†ã‚´ãƒªç¯„å›²å¤–ã‚’ã‚¹ã‚­ãƒƒãƒ—
            if cat == 0 or np.all(feat == 0) or cat not in active_main_cat_ids:
                invalid_feature_count += 1
                continue
            
            # ç‰¹å¾´é‡ã®æ­£è¦åŒ–çŠ¶æ…‹ã‚’ç¢ºèªï¼ˆL2ãƒãƒ«ãƒ ãŒ1ã«è¿‘ã„ã‹ãƒã‚§ãƒƒã‚¯ï¼‰
            feat_norm = np.linalg.norm(feat)
            if feat_norm < 0.9 or feat_norm > 1.1:
                # æ­£è¦åŒ–ã•ã‚Œã¦ã„ãªã„å ´åˆã¯å†æ­£è¦åŒ–
                feat = normalize_features_unified(feat.reshape(1, -1))[0]
            
            features_per_main_category[cat].append(feat)
            valid_feature_count += 1
    
    print(f"Processed {valid_feature_count} valid features, skipped {invalid_feature_count} invalid features")
    
    # ã‚«ãƒ†ã‚´ãƒªä¸­å¿ƒã‚’è¨ˆç®—
    category_centers_dict = {}
    for main_cat_id in active_main_cat_ids:
        if features_per_main_category[main_cat_id]:
            center_vec = np.mean(np.stack(features_per_main_category[main_cat_id]), axis=0)
            # ä¸­å¿ƒãƒ™ã‚¯ãƒˆãƒ«ã‚‚æ­£è¦åŒ–
            center_vec = normalize_features_unified(center_vec.reshape(1, -1))[0]
            category_centers_dict[main_cat_id] = center_vec.tolist()
            print(f"Category {main_cat_id}: {len(features_per_main_category[main_cat_id])} features")
        else:
            print(f"Warning: No features for category ID {main_cat_id}. Initializing randomly.")
            rand_vec = np.random.randn(embedding_dim).astype(np.float32)
            rand_vec = normalize_features_unified(rand_vec.reshape(1, -1))[0]
            category_centers_dict[main_cat_id] = rand_vec.tolist()
    
    # è¾æ›¸å½¢å¼ã§ä¿å­˜
    output_path = os.path.join(features_dir, 'category_centers.pkl.gz')
    with gzip.open(output_path, 'wb') as f:
        pickle.dump(category_centers_dict, f)
    
    print(f"Saved {len(category_centers_dict)} main category centers to {output_path}")
    
    category_names = {
        1: "outerwear", 2: "tops", 3: "dresses", 4: "pants", 5: "skirts",
        6: "shoes", 7: "bags", 8: "hats", 9: "watches", 10: "accessories",
        11: "fashion_goods", 12: "wallets", 13: "legwear", 
        14: "underwear", 15: "beauty", 16: "glasses", 17: "others"
    }
    
    print("\nCategory center summary:")
    for cat_id in sorted(category_centers_dict.keys()):
        cat_name = category_names.get(cat_id, f"Category {cat_id}")
        feature_count = len(features_per_main_category[cat_id])
        print(f"  {cat_id}: {cat_name} - {feature_count} features")
    
    return category_centers_dict

# =============================================================================
# DeepFurnitureå‡¦ç† (ä¿®æ­£ç‰ˆ)
# =============================================================================

# def process_deepfurniture(image_dir, annotations_json, furnitures_jsonl, output_dir, batch_size=32):
#     """ä¿®æ­£ç‰ˆDeepFurnitureå‡¦ç† - é‡è¤‡é™¤å»ã¨æ­£è¦åŒ–çµ±ä¸€"""
#     print(f"Processing DeepFurniture dataset to 11 categories")
#     print("Key improvements: Proper duplicate removal, unified normalization")
#     print(f"  Images: {image_dir}")
#     print(f"  Annotations: {annotations_json}")
#     print(f"  Furnitures: {furnitures_jsonl}")
    
#     os.makedirs(output_dir, exist_ok=True)
    
#     # å®¶å…·ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
#     furniture_to_category = load_jsonl_mapping(Path(furnitures_jsonl), "furniture_id", "category_id")
    
#     # ãƒ‡ãƒã‚¤ã‚¹ã¨ãƒ¢ãƒ‡ãƒ«ã®è¨­å®š
#     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
#     print(f"Using device: {device}")
    
#     model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
#     processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
#     model = model.to(device)
#     model.eval()
    
#     # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®æ¤œç´¢ã¨å‡¦ç†
#     image_files = []
#     for ext in ("*.jpg", "*.png", "*.jpeg"):
#         image_files.extend(Path(image_dir).rglob(ext))
    
#     valid_images = [img for img in image_files if img.stem in furniture_to_category]
#     print(f"Found {len(valid_images)} valid images")
    
#     # é‡è¤‡é™¤å»ã®ãŸã‚ã®è¿½è·¡
#     processed_furniture_ids = set()
#     duplicate_count = 0
    
#     # ç‰¹å¾´é‡æŠ½å‡º
#     all_features = []
#     all_furniture_ids = []
    
#     with torch.no_grad():
#         for i in tqdm(range(0, len(valid_images), batch_size), desc="Extracting DeepFurniture features"):
#             batch_paths = valid_images[i:i+batch_size]
            
#             # é‡è¤‡ãƒã‚§ãƒƒã‚¯ä»˜ãã§ãƒãƒƒãƒã‚’å‡¦ç†
#             filtered_batch_paths = []
#             for path in batch_paths:
#                 furniture_id = path.stem
#                 if furniture_id not in processed_furniture_ids:
#                     processed_furniture_ids.add(furniture_id)
#                     filtered_batch_paths.append(path)
#                 else:
#                     duplicate_count += 1
            
#             if not filtered_batch_paths:
#                 continue
            
#             try:
#                 pil_images = [Image.open(p).convert("RGB") for p in filtered_batch_paths]
#                 inputs = processor(images=pil_images, return_tensors="pt", padding=True)
#                 inputs = {k: v.to(device) for k, v in inputs.items()}
                
#                 output = model.get_image_features(**inputs)
#                 # çµ±ä¸€ã•ã‚ŒãŸæ­£è¦åŒ–å‡¦ç†
#                 output_normalized = normalize_features_unified(output)
#                 batch_feats = output_normalized.cpu().numpy()
                
#                 all_features.append(batch_feats)
#                 all_furniture_ids.extend([p.stem for p in filtered_batch_paths])
                
#             except Exception as e:
#                 print(f"Error processing batch: {e}")
#                 continue
    
#     if duplicate_count > 0:
#         print(f"Removed {duplicate_count} duplicate furniture items")
    
#     if not all_features:
#         print("No features extracted, exiting")
#         return
    
#     all_features = np.vstack(all_features)
#     print(f"Extracted features: {all_features.shape}")
    
#     # ã‚·ãƒ¼ãƒ³ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã®èª­ã¿è¾¼ã¿ã¨ã‚·ãƒ¼ãƒ³æ§‹ç¯‰
#     with open(annotations_json, 'r') as f:
#         annotations = json.load(f)
    
#     scenes = {}
#     category_ids_in_scenes = set()
    
#     for scene_record in tqdm(annotations, desc="Building DeepFurniture scenes"):
#         scene_id = scene_record.get("scene", {}).get("sceneTaskID")
#         if not scene_id:
#             continue
        
#         # é‡è¤‡é™¤å»: åŒã˜furniture_idã¯1ã¤ã®ã¿ä¿æŒ
#         scene_items_unique = {}
        
#         for instance in scene_record.get("instances", []):
#             furniture_id = str(instance.get("identityID"))
#             category_id = instance.get("categoryID")
    
#             if not furniture_id or category_id is None:
#                 continue
 
#             # é‡è¤‡ãƒã‚§ãƒƒã‚¯: åŒã˜furniture_idã¯æœ€åˆã®ã‚‚ã®ã®ã¿ä¿æŒ
#             if furniture_id not in scene_items_unique:
#                 if furniture_id in all_furniture_ids:
#                     idx = all_furniture_ids.index(furniture_id)
#                     scene_items_unique[furniture_id] = (all_features[idx], category_id)

#         # æœ‰åŠ¹ãªã‚·ãƒ¼ãƒ³ã®æ§‹ç¯‰ï¼ˆ4å€‹ä»¥ä¸Šã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚¢ã‚¤ãƒ†ãƒ ï¼‰
#         if len(scene_items_unique) >= 4:
#             item_ids_final = list(scene_items_unique.keys())
#             features_final = np.array([v[0] for v in scene_items_unique.values()])
#             categories_final = np.array([v[1] for v in scene_items_unique.values()])
          
#             scenes[str(scene_id)] = {
#                 "features": features_final,
#                 "category_ids": categories_final,
#                 "item_ids": np.array(item_ids_final, dtype=object)
#             }
#             category_ids_in_scenes.update(categories_final)
    
#     print(f"Built {len(scenes)} valid DeepFurniture scenes")
#     print(f"Found category IDs in scenes: {sorted(list(category_ids_in_scenes))}")
    
#     # ã‚·ãƒ¼ãƒ³ã‚’DeepFurnitureå½¢å¼ã«å¤‰æ›
#     convert_scenes_to_deepfurniture_format_fixed(scenes, output_dir)
    
#     # ã‚«ãƒ†ã‚´ãƒªä¸­å¿ƒã®è¨ˆç®—ï¼ˆ11ã‚«ãƒ†ã‚´ãƒªç”¨ï¼‰
#     compute_deepfurniture_category_centers_fixed(output_dir)
    
#     # ã‚«ãƒ†ã‚´ãƒªãƒãƒƒãƒ”ãƒ³ã‚°ã®ä¿å­˜
#     with open(os.path.join(output_dir, 'category_mapping.json'), 'w', encoding='utf-8') as f:
#         json.dump({
#             'categories': DEEPFURNITURE_CATEGORIES
#         }, f, ensure_ascii=False, indent=2)
    
#     print(f"DeepFurniture processing complete. Files saved to {output_dir}")

def process_deepfurniture_with_inclusion_removal(image_dir, annotations_json, furnitures_jsonl, output_dir, batch_size=32, apply_inclusion_removal=True):
    """
    åŒ…å«é–¢ä¿‚é™¤å»ã‚’çµ±åˆã—ãŸDeepFurnitureå‡¦ç†
    1. ã‚·ãƒ¼ãƒ³æ§‹ç¯‰
    2. åŒ…å«é–¢ä¿‚é™¤å»ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    3. ç‰¹å¾´é‡æŠ½å‡º
    4. åˆ†å‰²ãƒ»ä¿å­˜
    """
    print(f"ğŸ”¥ DeepFurniture processing with inclusion relationship removal")
    print(f"  Images: {image_dir}")
    print(f"  Annotations: {annotations_json}")  
    print(f"  Furnitures: {furnitures_jsonl}")
    print(f"  Apply inclusion removal: {apply_inclusion_removal}")

    
    os.makedirs(output_dir, exist_ok=True)
    
    # Step 1: ã‚·ãƒ¼ãƒ³ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã®èª­ã¿è¾¼ã¿ã¨ã‚·ãƒ¼ãƒ³æ§‹ç¯‰
    print(f"\nğŸ“– Step 1: ã‚·ãƒ¼ãƒ³ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³èª­ã¿è¾¼ã¿")
    
    with open(annotations_json, 'r') as f:
        annotations = json.load(f)
\
    print(f"ç·ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³æ•°: {len(annotations)}")
    
    # å®¶å…·ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
    furniture_to_category = load_jsonl_mapping(Path(furnitures_jsonl), "furniture_id", "category_id")
    
    # åˆæœŸã‚·ãƒ¼ãƒ³æ§‹ç¯‰ï¼ˆç‰¹å¾´é‡æŠ½å‡ºå‰ï¼‰
    raw_scenes = {}
    category_ids_in_scenes = set()
    
    for scene_record in tqdm(annotations, desc="Raw scenes building"):
        scene_id = scene_record.get("scene", {}).get("sceneTaskID")
        if not scene_id:
            continue
        
        # é‡è¤‡é™¤å»: åŒã˜furniture_idã¯1ã¤ã®ã¿ä¿æŒ
        scene_items_unique = {}
        
        for instance in scene_record.get("instances", []):
            furniture_id = str(instance.get("identityID"))
            category_id = instance.get("categoryID")
            
            if not furniture_id or category_id is None:
                continue
            
            # é‡è¤‡ãƒã‚§ãƒƒã‚¯: åŒã˜furniture_idã¯æœ€åˆã®ã‚‚ã®ã®ã¿ä¿æŒ
            if furniture_id not in scene_items_unique:
                # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                if furniture_id in furniture_to_category:
                    scene_items_unique[furniture_id] = category_id
        
        # æœ‰åŠ¹ãªã‚·ãƒ¼ãƒ³ã®æ§‹ç¯‰ï¼ˆ4å€‹ä»¥ä¸Šã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚¢ã‚¤ãƒ†ãƒ ï¼‰
        if len(scene_items_unique) >= 4:
            # (furniture_id, category_id)ã®ã‚¿ãƒ—ãƒ«ãƒªã‚¹ãƒˆã¨ã—ã¦ä¿å­˜
            raw_scenes[str(scene_id)] = [(fid, cid) for fid, cid in scene_items_unique.items()]
            category_ids_in_scenes.update(scene_items_unique.values())
    
    print(f"æ§‹ç¯‰ã•ã‚ŒãŸç”Ÿã‚·ãƒ¼ãƒ³æ•°: {len(raw_scenes)}")
    print(f"ç™ºè¦‹ã•ã‚ŒãŸã‚«ãƒ†ã‚´ãƒªID: {sorted(list(category_ids_in_scenes))}")
    
    # Step 2: åŒ…å«é–¢ä¿‚é™¤å»ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    if apply_inclusion_removal:
        print(f"\nâœ‚ï¸  Step 2: åŒ…å«é–¢ä¿‚é™¤å»å®Ÿè¡Œ")
        
        remover = InclusionRelationshipRemover(debug_mode=True)
        filtered_scenes = remover.remove_inclusion_relationships(
            raw_scenes, 
            min_items=4, 
            debug_limit=15
        )
        
        print(f"åŒ…å«é–¢ä¿‚é™¤å»çµæœ:")
        print(f"  é™¤å»å‰: {len(raw_scenes)} ã‚·ãƒ¼ãƒ³")
        print(f"  é™¤å»å¾Œ: {len(filtered_scenes)} ã‚·ãƒ¼ãƒ³")
        print(f"  é™¤å»ã•ã‚ŒãŸã‚·ãƒ¼ãƒ³: {len(raw_scenes) - len(filtered_scenes)}")
        print(f"  é™¤å»ç‡: {(len(raw_scenes) - len(filtered_scenes))/len(raw_scenes)*100:.1f}%")
        
        # åŒ…å«é–¢ä¿‚é™¤å»çµæœã‚’ä¸­é–“ä¿å­˜
        with open(os.path.join(output_dir, 'inclusion_removal_report.json'), 'w') as f:
            json.dump({
                'original_scenes': len(raw_scenes),
                'filtered_scenes': len(filtered_scenes),
                'removed_scenes': len(raw_scenes) - len(filtered_scenes),
                'removal_rate': (len(raw_scenes) - len(filtered_scenes))/len(raw_scenes)*100
            }, f, indent=2)
        
        scenes_to_process = filtered_scenes
    else:
        print(f"\nâ­ï¸  Step 2: åŒ…å«é–¢ä¿‚é™¤å»ã‚’ã‚¹ã‚­ãƒƒãƒ—")
        scenes_to_process = raw_scenes
    
    # Step 3: ç‰¹å¾´é‡æŠ½å‡º
    print(f"\nğŸ¨ Step 3: CLIPç‰¹å¾´é‡æŠ½å‡º")
    
    # ãƒ‡ãƒã‚¤ã‚¹ã¨ãƒ¢ãƒ‡ãƒ«ã®è¨­å®š
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")
    
    model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
    processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
    model = model.to(device)
    model.eval()
    
    # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®æ¤œç´¢ã¨å‡¦ç†
    image_files = []
    for ext in ("*.jpg", "*.png", "*.jpeg"):
        image_files.extend(Path(image_dir).rglob(ext))
    
    valid_images = [img for img in image_files if img.stem in furniture_to_category]
    print(f"ç™ºè¦‹ã•ã‚ŒãŸæœ‰åŠ¹ç”»åƒæ•°: {len(valid_images)}")
    
    # ãƒ•ã‚£ãƒ«ã‚¿æ¸ˆã¿ã‚·ãƒ¼ãƒ³ã§å®Ÿéš›ã«ä½¿ç”¨ã•ã‚Œã‚‹å®¶å…·IDã®ã¿ã‚’å‡¦ç†
    used_furniture_ids = set()
    for scene_items in scenes_to_process.values():
        for furniture_id, _ in scene_items:
            used_furniture_ids.add(furniture_id)
    
    # ä½¿ç”¨ã•ã‚Œã‚‹ç”»åƒã®ã¿ã«çµã‚Šè¾¼ã¿
    relevant_images = [img for img in valid_images if img.stem in used_furniture_ids]
    print(f"å®Ÿéš›ã«ä½¿ç”¨ã•ã‚Œã‚‹ç”»åƒæ•°: {len(relevant_images)}")
    
    # é‡è¤‡é™¤å»ã®ãŸã‚ã®è¿½è·¡
    processed_furniture_ids = set()
    duplicate_count = 0
    
    # ç‰¹å¾´é‡æŠ½å‡º
    all_features = []
    all_furniture_ids = []
    
    with torch.no_grad():
        for i in tqdm(range(0, len(relevant_images), batch_size), desc="CLIPç‰¹å¾´é‡æŠ½å‡º"):
            batch_paths = relevant_images[i:i+batch_size]
            
            # é‡è¤‡ãƒã‚§ãƒƒã‚¯ä»˜ãã§ãƒãƒƒãƒã‚’å‡¦ç†
            filtered_batch_paths = []
            for path in batch_paths:
                furniture_id = path.stem
                if furniture_id not in processed_furniture_ids:
                    processed_furniture_ids.add(furniture_id)
                    filtered_batch_paths.append(path)
                else:
                    duplicate_count += 1
            
            if not filtered_batch_paths:
                continue
            
            try:
                pil_images = [Image.open(p).convert("RGB") for p in filtered_batch_paths]
                inputs = processor(images=pil_images, return_tensors="pt", padding=True)
                inputs = {k: v.to(device) for k, v in inputs.items()}
                
                output = model.get_image_features(**inputs)
                # çµ±ä¸€ã•ã‚ŒãŸæ­£è¦åŒ–å‡¦ç†
                output_normalized = normalize_features_unified(output)
                batch_feats = output_normalized.cpu().numpy()
                
                all_features.append(batch_feats)
                all_furniture_ids.extend([p.stem for p in filtered_batch_paths])
                
            except Exception as e:
                print(f"ãƒãƒƒãƒå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                continue
    
    if duplicate_count > 0:
        print(f"é™¤å»ã•ã‚ŒãŸé‡è¤‡å®¶å…·ã‚¢ã‚¤ãƒ†ãƒ : {duplicate_count}")
    
    if not all_features:
        print("ç‰¹å¾´é‡ãŒæŠ½å‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚å‡¦ç†ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
        return
    
    all_features = np.vstack(all_features)
    print(f"æŠ½å‡ºã•ã‚ŒãŸç‰¹å¾´é‡: {all_features.shape}")
    
    # Step 4: ã‚·ãƒ¼ãƒ³ã®å†æ§‹ç¯‰ï¼ˆç‰¹å¾´é‡ä»˜ãï¼‰
    print(f"\nğŸ—ï¸  Step 4: ç‰¹å¾´é‡ä»˜ãã‚·ãƒ¼ãƒ³å†æ§‹ç¯‰")
    
    final_scenes = {}
    
    for scene_id, scene_items in tqdm(scenes_to_process.items(), desc="ç‰¹å¾´é‡çµ±åˆ"):
        scene_features = []
        scene_categories = []
        scene_furniture_ids = []
        
        for furniture_id, category_id in scene_items:
            if furniture_id in all_furniture_ids:
                idx = all_furniture_ids.index(furniture_id)
                scene_features.append(all_features[idx])
                scene_categories.append(category_id)
                scene_furniture_ids.append(furniture_id)
        
        if len(scene_features) >= 4:
            final_scenes[scene_id] = {
                "features": np.array(scene_features),
                "category_ids": np.array(scene_categories),
                "item_ids": np.array(scene_furniture_ids, dtype=object)
            }
    
    print(f"æœ€çµ‚ã‚·ãƒ¼ãƒ³æ•°: {len(final_scenes)}")
    
    # Step 5: DeepFurnitureå½¢å¼ã¸ã®å¤‰æ›ã¨ä¿å­˜
    print(f"\nğŸ’¾ Step 5: DeepFurnitureå½¢å¼å¤‰æ›ãƒ»ä¿å­˜")
    
    convert_scenes_to_deepfurniture_format_fixed(final_scenes, output_dir)
    
    # ã‚«ãƒ†ã‚´ãƒªä¸­å¿ƒã®è¨ˆç®—
    compute_deepfurniture_category_centers_fixed(output_dir)
    
    # ã‚«ãƒ†ã‚´ãƒªãƒãƒƒãƒ”ãƒ³ã‚°ã®ä¿å­˜
    with open(os.path.join(output_dir, 'category_mapping.json'), 'w', encoding='utf-8') as f:
        json.dump({
            'categories': DEEPFURNITURE_CATEGORIES
        }, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… DeepFurnitureå‡¦ç†å®Œäº†ï¼ˆåŒ…å«é–¢ä¿‚é™¤å»çµ±åˆç‰ˆï¼‰")
    print(f"   çµæœä¿å­˜å…ˆ: {output_dir}")
    
    return final_scenes



def load_jsonl_mapping(path, key_field, value_field):
    """JSONL ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°ã®èª­ã¿è¾¼ã¿"""
    mapping = {}
    unique_values_found = set()
    
    try:
        with path.open("r", encoding="utf-8") as f:
            for line_num, line in enumerate(f):
                try:
                    rec = json.loads(line)
                    key = str(rec[key_field])
                    value = int(rec[value_field])
                    mapping[key] = value
                    unique_values_found.add(value)
                except (json.JSONDecodeError, KeyError, ValueError) as e:
                    print(f"[WARN] Skipping line {line_num+1} in '{path.name}' due to error: {e}")
        
        if mapping:
            print(f"[INFO] Loaded mapping from '{path.name}': {len(mapping)} entries.")
            print(f"[INFO] Found {len(unique_values_found)} unique category IDs: {sorted(list(unique_values_found))}")
            if unique_values_found:
                print(f"[INFO] Min/Max Category ID: {min(unique_values_found)} / {max(unique_values_found)}")
        else:
            print(f"[WARN] No valid entries loaded from '{path.name}'.")
    except FileNotFoundError:
        print(f"[ERROR] Metadata file not found: {path}")
    except Exception as e:
        print(f"[ERROR] Failed to load '{path.name}': {e}")
    
    return mapping

def convert_scenes_to_deepfurniture_format_fixed(scenes, output_dir, min_items=4, max_items=20, max_item_num=10):
    """ä¿®æ­£ç‰ˆ: ã‚·ãƒ¼ãƒ³ã‚’DeepFurnitureå½¢å¼ã«å¤‰æ› - é‡è¤‡ãƒã‚§ãƒƒã‚¯å¼·åŒ–"""
    from collections import defaultdict
    import numpy as np
    import random
    from sklearn.model_selection import train_test_split
    from tqdm import tqdm
    import pickle
    import os
    
    # ã‚·ãƒ¼ãƒ³ã”ã¨ã«ã‚¯ã‚¨ãƒªã¨ãƒã‚¸ãƒ†ã‚£ãƒ–ã‚’ä½œæˆ
    q_feats, p_feats, q_cats, p_cats, q_ids, p_ids, s_keys = [], [], [], [], [], [], []
    
    rng = random.Random(42)
    
    # çµ±è¨ˆæƒ…å ±
    scene_count = 0
    skipped_scenes = 0
    overlap_resolved_count = 0
    
    for sid, rec in tqdm(scenes.items(), desc="Converting DeepFurniture scenes"):
        feats, cats, ids = rec["features"], rec["category_ids"], rec["item_ids"]
        n = len(feats)
        
        if n < min_items or n > max_items:
            skipped_scenes += 1
            continue
        
        # ã‚·ãƒ¼ãƒ³å†…ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã—ã¦åŠåˆ†ã«åˆ†ã‘ã‚‹
        idx = list(range(n))
        rng.shuffle(idx)
        
        # é‡è¤‡ãªã—ã§åŠåˆ†ãšã¤ã«åˆ†å‰²
        half = n // 2
        q_idx = idx[:half]      # ã‚¯ã‚¨ãƒªç”¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        p_idx = idx[half:]      # ãƒã‚¸ãƒ†ã‚£ãƒ–ç”¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        
        # é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
        q_items_set = set([ids[i] for i in q_idx])
        p_items_set = set([ids[i] for i in p_idx])
        
        if q_items_set & p_items_set:
            print(f"WARNING: Scene {sid} has overlapping items between query and positive!")
            print(f"  Overlapping items: {q_items_set & p_items_set}")
            overlap_resolved_count += 1
            continue
        
        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°é–¢æ•°
        def pad_df(arr, target, pad_val=0):
            cur = len(arr)
            if cur < target:
                if arr.ndim > 1:
                    pad_shape = (target - cur, *arr.shape[1:])
                else:
                    pad_shape = (target - cur,)
                pad_arr = np.full(pad_shape, pad_val, dtype=arr.dtype)
                arr = np.concatenate([arr, pad_arr], axis=0)
            else:
                arr = arr[:target]
            return arr
        
        # ã‚¯ã‚¨ãƒª
        qf = pad_df(feats[q_idx], max_item_num)
        qi = pad_df(ids[q_idx], max_item_num, pad_val="")
        qc = pad_df(cats[q_idx], max_item_num, pad_val=0)
        
        # ãƒã‚¸ãƒ†ã‚£ãƒ–
        pf = pad_df(feats[p_idx], max_item_num)
        pi = pad_df(ids[p_idx], max_item_num, pad_val="")
        pc = pad_df(cats[p_idx], max_item_num, pad_val=0)
        
        q_feats.append(qf); q_ids.append(qi); q_cats.append(qc)
        p_feats.append(pf); p_ids.append(pi); p_cats.append(pc)
        s_keys.append(sid)
        scene_count += 1
    
    print(f"\nProcessed {scene_count} scenes (skipped {skipped_scenes} scenes, resolved {overlap_resolved_count} overlaps)")
    
    if not q_feats:
        print("No valid scenes found for conversion")
        return
    
    # NumPyé…åˆ—ã«å¤‰æ›
    Q = np.stack(q_feats)
    P = np.stack(p_feats)
    qcat = np.stack(q_cats)
    pcat = np.stack(p_cats)
    qid = np.stack(q_ids)
    pid = np.stack(p_ids)
    sids = np.array(s_keys)
    
    # ã‚«ãƒ†ã‚´ãƒªãƒªãƒãƒƒãƒ—ï¼ˆãƒ‘ãƒ‡ã‚£ãƒ³ã‚°=0ï¼‰
    unique = np.unique(np.concatenate([qcat, pcat]))
    unique = unique[unique > 0]
    cat_map = {cid: i + 1 for i, cid in enumerate(sorted(unique))}
    vect = np.vectorize(lambda c: cat_map.get(c, 0))
    qcat = vect(qcat)
    pcat = vect(pcat)
    
    print(f"Found {len(unique)} unique categories, remapped to 1-{len(unique)}")
    
    # ã‚·ãƒ¼ãƒ³ãƒ™ãƒ¼ã‚¹ã§train/val/testã«åˆ†å‰²
    indices = np.arange(len(Q))
    
    # 70/15/15ã®å‰²åˆã§åˆ†å‰²
    train_idx, temp_idx = train_test_split(indices, test_size=0.3, random_state=42)
    val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, random_state=42)
    
    # åˆ†å‰²ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
    Q_tr, P_tr = Q[train_idx], P[train_idx]
    qcat_tr, pcat_tr = qcat[train_idx], pcat[train_idx]
    qid_tr, pid_tr = qid[train_idx], pid[train_idx]
    sid_tr = sids[train_idx]
    
    Q_val, P_val = Q[val_idx], P[val_idx]
    qcat_val, pcat_val = qcat[val_idx], pcat[val_idx]
    qid_val, pid_val = qid[val_idx], pid[val_idx]
    sid_val = sids[val_idx]
    
    Q_te, P_te = Q[test_idx], P[test_idx]
    qcat_te, pcat_te = qcat[test_idx], pcat[test_idx]
    qid_te, pid_te = qid[test_idx], pid[test_idx]
    sid_te = sids[test_idx]
    
    # å„åˆ†å‰²å†…ã§ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯
    def check_query_positive_overlap(split_name, q_ids, p_ids):
        """ã‚¯ã‚¨ãƒªã¨ãƒã‚¸ãƒ†ã‚£ãƒ–é–“ã®é‡è¤‡ã‚’ãƒã‚§ãƒƒã‚¯"""
        overlap_count = 0
        for i in range(len(q_ids)):
            q_items = set([item for item in q_ids[i] if item and item != "" and item != "0"])
            p_items = set([item for item in p_ids[i] if item and item != "" and item != "0"])
            if q_items & p_items:
                overlap_count += 1
        
        if overlap_count > 0:
            print(f"âš ï¸  {split_name}: {overlap_count}/{len(q_ids)} scenes have query-positive overlap")
        else:
            print(f"âœ… {split_name}: No query-positive overlap in any scene")
        
        return overlap_count == 0
    
    print("\n=== Query-Positive Overlap Check ===")
    check_query_positive_overlap("Train", qid_tr, pid_tr)
    check_query_positive_overlap("Validation", qid_val, pid_val)
    check_query_positive_overlap("Test", qid_te, pid_te)
    
    # æ­£è¦åŒ–ã¯æ—¢ã«çµ±ä¸€ã•ã‚Œã¦ã„ã‚‹ãŸã‚Z-scoreæ­£è¦åŒ–ã¯å‰Šé™¤
    # ã™ã¹ã¦ã®ç‰¹å¾´é‡ã¯æ—¢ã«L2æ­£è¦åŒ–æ¸ˆã¿
    print(f"Features are already L2-normalized, skipping additional normalization")
    
    print(f"\nDeepFurniture split: Train {len(Q_tr)}, Validation {len(Q_val)}, Test {len(Q_te)}")
    
    # åˆ†å‰²ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
    def save_partition(path, objects):
        with open(path, "wb") as f:
            pickle.dump(objects, f)
    
    # å…ƒã®å½¢å¼ã‚’ç¶­æŒã—ã¦set_sizesã¯0ã§åˆæœŸåŒ–
    save_partition(os.path.join(output_dir, "train.pkl"), 
                  (Q_tr, P_tr, sid_tr, qcat_tr, pcat_tr, np.zeros(len(Q_tr)), qid_tr, pid_tr))
    save_partition(os.path.join(output_dir, "validation.pkl"), 
                  (Q_val, P_val, sid_val, qcat_val, pcat_val, np.zeros(len(Q_val)), qid_val, pid_val))
    save_partition(os.path.join(output_dir, "test.pkl"), 
                  (Q_te, P_te, sid_te, qcat_te, pcat_te, np.zeros(len(Q_te)), qid_te, pid_te))
    
    print("\nâœ… Dataset saved successfully!")

def compute_deepfurniture_category_centers_fixed(features_dir):
    """ä¿®æ­£ç‰ˆ: DeepFurnitureã‚«ãƒ†ã‚´ãƒªä¸­å¿ƒè¨ˆç®— - è¾æ›¸å½¢å¼ã€æ­£è¦åŒ–çµ±ä¸€"""
    train_path = os.path.join(features_dir, 'train.pkl')
    
    with open(train_path, 'rb') as f:
        Q_tr, P_tr, _, qcat_tr, pcat_tr, _, _, _ = pickle.load(f)
    
    # ã‚«ãƒ†ã‚´ãƒªä¸­å¿ƒè¨ˆç®—ï¼ˆè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ã¿ï¼‰
    flat_feat = np.concatenate([Q_tr.reshape(-1, Q_tr.shape[-1]), P_tr.reshape(-1, P_tr.shape[-1])])
    flat_cat = np.concatenate([qcat_tr.reshape(-1), pcat_tr.reshape(-1)])
    
    unique_cats = np.unique(flat_cat)
    unique_cats = unique_cats[unique_cats > 0]
    
    # è¾æ›¸å½¢å¼ã§ä¿å­˜ï¼ˆäº’æ›æ€§ã®ãŸã‚ï¼‰
    category_centers_dict = {}
    
    print("Computing DeepFurniture category centers...")
    for cid in sorted(unique_cats):
        mask = flat_cat == cid
        if mask.any():
            center_vec = flat_feat[mask].mean(axis=0)
            # ä¸­å¿ƒãƒ™ã‚¯ãƒˆãƒ«ã‚‚æ­£è¦åŒ–
            center_vec = normalize_features_unified(center_vec.reshape(1, -1))[0]
            category_centers_dict[int(cid)] = center_vec.tolist()
            print(f"Category {cid}: {mask.sum()} features")
        else:
            # ç©ºã®ã‚«ãƒ†ã‚´ãƒªã«ã¯ãƒ©ãƒ³ãƒ€ãƒ ãªæ­£è¦åŒ–ãƒ™ã‚¯ãƒˆãƒ«ã‚’å‰²ã‚Šå½“ã¦
            rand_vec = np.random.randn(flat_feat.shape[1]).astype(np.float32)
            rand_vec = normalize_features_unified(rand_vec.reshape(1, -1))[0]
            category_centers_dict[int(cid)] = rand_vec.tolist()
    
    import gzip
    with gzip.open(os.path.join(features_dir, "category_centers.pkl.gz"), "wb") as f:
        pickle.dump(category_centers_dict, f)
    
    print(f"Saved {len(category_centers_dict)} DeepFurniture category centers to category_centers.pkl.gz")


class InclusionRelationshipRemover:
    """åŒ…å«é–¢ä¿‚ã‚’é™¤å»ã™ã‚‹ã‚¯ãƒ©ã‚¹ï¼ˆç›´æ¥çµ±åˆç‰ˆï¼‰"""
    
    def __init__(self, debug_mode=True):
        self.debug_mode = debug_mode
        
    def remove_inclusion_relationships(self, scenes_dict, min_items=4, debug_limit=10):
        """
        åŒ…å«é–¢ä¿‚ã‚’é™¤å»ã™ã‚‹çµ±åˆé–¢æ•°
        
        Args:
            scenes_dict: ã‚·ãƒ¼ãƒ³ID -> ã‚¢ã‚¤ãƒ†ãƒ ãƒªã‚¹ãƒˆã®è¾æ›¸
            min_items: æœ€å°ã‚¢ã‚¤ãƒ†ãƒ æ•°
            debug_limit: ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›ã™ã‚‹ä¾‹æ•°
            
        Returns:
            filtered_scenes: åŒ…å«é–¢ä¿‚ã‚’é™¤å»ã—ãŸã‚·ãƒ¼ãƒ³è¾æ›¸
        """
        
        if self.debug_mode:
            print(f"ğŸ” åŒ…å«é–¢ä¿‚é™¤å»é–‹å§‹")
            print(f"å…¥åŠ›ã‚·ãƒ¼ãƒ³æ•°: {len(scenes_dict)}")
        
        # Step 1: ã‚·ãƒ¼ãƒ³ã‚’furniture_idã®ã‚»ãƒƒãƒˆã«å¤‰æ›
        scene_to_furniture_sets = {}
        
        for scene_id, scene_data in scenes_dict.items():
            furniture_ids = self._extract_furniture_ids(scene_data)
            
            # æœ€å°ã‚¢ã‚¤ãƒ†ãƒ æ•°ãƒã‚§ãƒƒã‚¯
            if len(furniture_ids) >= min_items:
                scene_to_furniture_sets[scene_id] = set(furniture_ids)
        
        if self.debug_mode:
            print(f"æœ€å°ã‚¢ã‚¤ãƒ†ãƒ æ•°({min_items})ä»¥ä¸Šã®ã‚·ãƒ¼ãƒ³: {len(scene_to_furniture_sets)}")
        
        # Step 2: åŒ…å«é–¢ä¿‚ã‚’æ¤œå‡º
        inclusion_pairs = self._detect_inclusion_pairs(scene_to_furniture_sets, debug_limit)
        
        if not inclusion_pairs:
            if self.debug_mode:
                print("âœ… åŒ…å«é–¢ä¿‚ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
            return scenes_dict
        
        # Step 3: é™¤å»å¯¾è±¡ã‚·ãƒ¼ãƒ³ã‚’æ±ºå®š
        scenes_to_remove = self._determine_removal_targets(inclusion_pairs, scene_to_furniture_sets)
        
        # Step 4: é™¤å»å®Ÿè¡Œ
        filtered_scenes = self._apply_removal(scenes_dict, scenes_to_remove)
        
        # Step 5: é™¤å»å¾Œã®æ¤œè¨¼
        if self.debug_mode:
            self._validate_removal(filtered_scenes, min_items)
        
        return filtered_scenes
    
    def _extract_furniture_ids(self, scene_data):
        """ã‚·ãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰furniture_idã‚’æŠ½å‡º"""
        
        furniture_ids = []
        
        if isinstance(scene_data, dict):
            # è¾æ›¸å½¢å¼ã®å ´åˆ
            if 'item_ids' in scene_data:
                furniture_ids = list(scene_data['item_ids'])
            elif 'features' in scene_data and 'item_ids' in scene_data:
                furniture_ids = list(scene_data['item_ids'])
            else:
                # ãã®ä»–ã®è¾æ›¸ã‚­ãƒ¼ã‚’æ¢ç´¢
                for key, value in scene_data.items():
                    if isinstance(value, (list, np.ndarray)) and len(value) > 0:
                        if isinstance(value[0], str):
                            furniture_ids = list(value)
                            break
        
        elif isinstance(scene_data, list):
            # ãƒªã‚¹ãƒˆå½¢å¼ã®å ´åˆ
            for item in scene_data:
                if isinstance(item, tuple) and len(item) >= 1:
                    furniture_ids.append(str(item[0]))
                elif isinstance(item, str):
                    furniture_ids.append(item)
                elif isinstance(item, dict) and 'furniture_id' in item:
                    furniture_ids.append(str(item['furniture_id']))
        
        # é‡è¤‡é™¤å»ã¨æ–‡å­—åˆ—åŒ–
        return list(set([str(fid) for fid in furniture_ids if fid]))
    
    def _detect_inclusion_pairs(self, scene_to_furniture_sets, debug_limit):
        """åŒ…å«é–¢ä¿‚ã®ãƒšã‚¢ã‚’æ¤œå‡º"""
        
        inclusion_pairs = []
        scene_ids = list(scene_to_furniture_sets.keys())
        
        if self.debug_mode:
            print(f"\nğŸ” åŒ…å«é–¢ä¿‚æ¤œå‡ºä¸­... ({len(scene_ids)}ã‚·ãƒ¼ãƒ³)")
        
        debug_count = 0
        
        # å…¨ãƒšã‚¢ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆåŠ¹ç‡åŒ–å¯èƒ½ã ãŒã€ã¾ãšæ­£ç¢ºæ€§ã‚’é‡è¦–ï¼‰
        for i, scene_id1 in enumerate(tqdm(scene_ids, desc="åŒ…å«é–¢ä¿‚ãƒã‚§ãƒƒã‚¯", disable=not self.debug_mode)):
            set1 = scene_to_furniture_sets[scene_id1]
            
            for scene_id2 in scene_ids[i+1:]:
                set2 = scene_to_furniture_sets[scene_id2]
                
                # åŒ…å«é–¢ä¿‚ãƒã‚§ãƒƒã‚¯
                if set1.issubset(set2) and set1 != set2:
                    inclusion_pairs.append((scene_id1, scene_id2, 'subset'))
                    
                    if self.debug_mode and debug_count < debug_limit:
                        print(f"\nåŒ…å«é–¢ä¿‚ç™ºè¦‹: {scene_id1} âŠ† {scene_id2}")
                        print(f"  {scene_id1}({len(set1)}å€‹): {sorted(list(set1))[:5]}...")
                        print(f"  {scene_id2}({len(set2)}å€‹): {sorted(list(set2))[:5]}...")
                        print(f"  å·®åˆ†({len(set2-set1)}å€‹): {sorted(list(set2-set1))[:3]}...")
                        debug_count += 1
                        
                elif set2.issubset(set1) and set1 != set2:
                    inclusion_pairs.append((scene_id2, scene_id1, 'subset'))
                    
                    if self.debug_mode and debug_count < debug_limit:
                        print(f"\nåŒ…å«é–¢ä¿‚ç™ºè¦‹: {scene_id2} âŠ† {scene_id1}")
                        print(f"  {scene_id2}({len(set2)}å€‹): {sorted(list(set2))[:5]}...")
                        print(f"  {scene_id1}({len(set1)}å€‹): {sorted(list(set1))[:5]}...")
                        print(f"  å·®åˆ†({len(set1-set2)}å€‹): {sorted(list(set1-set2))[:3]}...")
                        debug_count += 1
        
        if self.debug_mode:
            print(f"\nğŸ“Š åŒ…å«é–¢ä¿‚æ¤œå‡ºçµæœ: {len(inclusion_pairs)}ãƒšã‚¢")
        
        return inclusion_pairs
    
    def _determine_removal_targets(self, inclusion_pairs, scene_to_furniture_sets):
        """é™¤å»å¯¾è±¡ã‚·ãƒ¼ãƒ³ã‚’æ±ºå®š"""
        
        # æˆ¦ç•¥: ã‚ˆã‚Šå°ã•ã„ã‚·ãƒ¼ãƒ³ï¼ˆéƒ¨åˆ†é›†åˆï¼‰ã‚’é™¤å»
        scenes_to_remove = set()
        
        for smaller_scene, larger_scene, relation_type in inclusion_pairs:
            scenes_to_remove.add(smaller_scene)
        
        if self.debug_mode:
            print(f"\nğŸ“‹ é™¤å»æˆ¦ç•¥: å°ã•ã„ã‚·ãƒ¼ãƒ³ï¼ˆéƒ¨åˆ†é›†åˆï¼‰ã‚’é™¤å»")
            print(f"é™¤å»å¯¾è±¡ã‚·ãƒ¼ãƒ³æ•°: {len(scenes_to_remove)}")
            
            # é™¤å»å¯¾è±¡ã®ä¾‹ã‚’è¡¨ç¤º
            if scenes_to_remove:
                print(f"é™¤å»å¯¾è±¡ä¾‹:")
                for i, scene_id in enumerate(list(scenes_to_remove)[:5]):
                    furniture_set = scene_to_furniture_sets[scene_id]
                    print(f"  {scene_id}: {len(furniture_set)}å€‹ã®ã‚¢ã‚¤ãƒ†ãƒ ")
        
        return scenes_to_remove
    
    def _apply_removal(self, scenes_dict, scenes_to_remove):
        """é™¤å»ã‚’å®Ÿè¡Œ"""
        
        original_count = len(scenes_dict)
        
        filtered_scenes = {
            scene_id: scene_data 
            for scene_id, scene_data in scenes_dict.items() 
            if scene_id not in scenes_to_remove
        }
        
        if self.debug_mode:
            removed_count = original_count - len(filtered_scenes)
            print(f"\nâœ‚ï¸  é™¤å»å®Ÿè¡Œå®Œäº†")
            print(f"  å…ƒã®ã‚·ãƒ¼ãƒ³æ•°: {original_count}")
            print(f"  é™¤å»ã•ã‚ŒãŸã‚·ãƒ¼ãƒ³: {removed_count}")
            print(f"  æ®‹å­˜ã‚·ãƒ¼ãƒ³æ•°: {len(filtered_scenes)}")
            print(f"  é™¤å»ç‡: {removed_count/original_count*100:.1f}%")
        
        return filtered_scenes
    
    def _validate_removal(self, filtered_scenes, min_items):
        """é™¤å»å¾Œã®æ¤œè¨¼"""
        
        print(f"\nğŸ” é™¤å»å¾Œã®æ¤œè¨¼...")
        
        # å†åº¦åŒ…å«é–¢ä¿‚ã‚’ãƒã‚§ãƒƒã‚¯
        verification_remover = InclusionRelationshipRemover(debug_mode=False)
        scene_to_furniture_sets = {}
        
        for scene_id, scene_data in filtered_scenes.items():
            furniture_ids = verification_remover._extract_furniture_ids(scene_data)
            if len(furniture_ids) >= min_items:
                scene_to_furniture_sets[scene_id] = set(furniture_ids)
        
        verification_pairs = verification_remover._detect_inclusion_pairs(scene_to_furniture_sets, debug_limit=3)
        
        if len(verification_pairs) == 0:
            print("âœ… åŒ…å«é–¢ä¿‚ãŒå®Œå…¨ã«é™¤å»ã•ã‚Œã¾ã—ãŸï¼")
        else:
            print(f"âš ï¸  ã¾ã {len(verification_pairs)}å€‹ã®åŒ…å«é–¢ä¿‚ãŒæ®‹ã£ã¦ã„ã¾ã™")
            # æ®‹å­˜ã™ã‚‹åŒ…å«é–¢ä¿‚ã®ä¾‹ã‚’è¡¨ç¤º
            for i, (smaller, larger, _) in enumerate(verification_pairs[:3]):
                set1 = scene_to_furniture_sets[smaller]
                set2 = scene_to_furniture_sets[larger]
                print(f"  æ®‹å­˜ä¾‹{i+1}: {smaller}({len(set1)}) âŠ† {larger}({len(set2)})")


def test_inclusion_removal_on_real_data(annotations_json, limit_scenes=None):
    """
    å®Ÿéš›ã®DeepFurnitureãƒ‡ãƒ¼ã‚¿ã§åŒ…å«é–¢ä¿‚é™¤å»ã‚’ãƒ†ã‚¹ãƒˆ
    
    Args:
        annotations_json: ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        limit_scenes: ãƒ†ã‚¹ãƒˆã™ã‚‹ã‚·ãƒ¼ãƒ³æ•°ã®ä¸Šé™ï¼ˆNoneã®å ´åˆã¯å…¨ãƒ‡ãƒ¼ã‚¿ï¼‰
    """
    
    print("ğŸ§ª å®Ÿãƒ‡ãƒ¼ã‚¿ã§ã®åŒ…å«é–¢ä¿‚é™¤å»ãƒ†ã‚¹ãƒˆé–‹å§‹")
    print(f"ãƒ•ã‚¡ã‚¤ãƒ«: {annotations_json}")
    if limit_scenes:
        print(f"ãƒ†ã‚¹ãƒˆå¯¾è±¡: æœ€åˆã®{limit_scenes}ã‚·ãƒ¼ãƒ³")
    else:
        print(f"ãƒ†ã‚¹ãƒˆå¯¾è±¡: å…¨ã‚·ãƒ¼ãƒ³")
    
    # 1. ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
    with open(annotations_json, 'r') as f:
        annotations = json.load(f)
    
    print(f"ç·ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³æ•°: {len(annotations)}")
    
    # 2. å…¨ã‚·ãƒ¼ãƒ³ã‚’æ§‹ç¯‰ï¼ˆåˆ¶é™ãªã—ã¾ãŸã¯åˆ¶é™ã‚ã‚Šï¼‰
    test_scenes = {}
    processed_count = 0
    
    for scene_record in tqdm(annotations, desc="ã‚·ãƒ¼ãƒ³æ§‹ç¯‰ä¸­"):
        if limit_scenes and processed_count >= limit_scenes:
            break
            
        scene_id = scene_record.get("scene", {}).get("sceneTaskID")
        if not scene_id:
            continue
        
        # ã‚¢ã‚¤ãƒ†ãƒ åé›†
        scene_items = []
        for instance in scene_record.get("instances", []):
            furniture_id = str(instance.get("identityID"))
            category_id = instance.get("categoryID")
            
            if furniture_id and category_id is not None:
                scene_items.append((furniture_id, category_id))
        
        if len(scene_items) >= 4:
            test_scenes[str(scene_id)] = scene_items
            processed_count += 1
    
    print(f"ãƒ†ã‚¹ãƒˆç”¨ã‚·ãƒ¼ãƒ³æ§‹ç¯‰å®Œäº†: {len(test_scenes)}ã‚·ãƒ¼ãƒ³")
    
    remover = InclusionRelationshipRemover(debug_mode=True)
    filtered_scenes = remover.remove_inclusion_relationships(test_scenes, min_items=4, debug_limit=15)
    
    # 4. çµæœã‚µãƒãƒªãƒ¼
    print(f"\nğŸ“Š ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼:")
    print(f"  ãƒ†ã‚¹ãƒˆå¯¾è±¡ã‚·ãƒ¼ãƒ³: {len(test_scenes)}")
    print(f"  é™¤å»å¾Œã‚·ãƒ¼ãƒ³: {len(filtered_scenes)}")
    print(f"  é™¤å»ã•ã‚ŒãŸã‚·ãƒ¼ãƒ³: {len(test_scenes) - len(filtered_scenes)}")
    print(f"  é™¤å»ç‡: {(len(test_scenes) - len(filtered_scenes))/len(test_scenes)*100:.1f}%")
    
    return test_scenes, filtered_scenes


# =============================================================================
# ãƒ¡ã‚¤ãƒ³é–¢æ•°
# =============================================================================

# ãƒ¡ã‚¤ãƒ³é–¢æ•°ã‚‚æ›´æ–°
def main():
    parser = argparse.ArgumentParser(description="Dataset generator with inclusion relationship removal")
    parser.add_argument('--dataset', choices=['iqon3000', 'deepfurniture'], required=True, help='Dataset type to process')
    parser.add_argument('--batch-size', type=int, default=32, help='Batch size for feature extraction')
    parser.add_argument('--input-dir', type=str, help='Input directory for IQON3000 dataset')
    parser.add_argument('--output-dir', type=str, required=True, help='Output directory for processed dataset')
    parser.add_argument('--user-based', action='store_true', help='Use user-based splitting (recommended)')
    
    # DeepFurnitureé–¢é€£ã®å¼•æ•°
    parser.add_argument('--image-dir', type=str, help='DeepFurniture image directory')
    parser.add_argument('--annotations-json', type=str, help='DeepFurniture annotations.json file')
    parser.add_argument('--furnitures-jsonl', type=str, help='DeepFurniture furnitures.jsonl file')
    parser.add_argument('--no-inclusion-removal', action='store_true', help='Disable inclusion relationship removal for DeepFurniture')
    
    args = parser.parse_args()
    
    if args.dataset == 'iqon3000':
        if not args.input_dir:
            parser.error("--input-dir is required for IQON3000 dataset")
        
        if args.user_based:
            print("ğŸ”¥ Processing IQON3000 dataset: 7 categories (USER-BASED SPLIT)")
            print("   Key improvements:")
            print("   - USER-BASED splitting: No user overlap between splits")
            print("   - Split FIRST, then normalize")
            print("   - Use training data statistics only")
            print("   - Prevent information leakage")
            process_iqon3000(args.input_dir, args.output_dir, args.batch_size)
        else:
            print("ğŸ”¥ Processing IQON3000 dataset: 7 categories (SCENE-BASED SPLIT)")
            print("   âš ï¸ Warning: This may cause user overlap between splits")
            process_iqon3000(args.input_dir, args.output_dir, args.batch_size)

    elif args.dataset == 'deepfurniture':
        if not all([args.image_dir, args.annotations_json, args.furnitures_jsonl]):
            parser.error("--image-dir, --annotations-json, and --furnitures-jsonl are required for DeepFurniture dataset")
        
        apply_inclusion_removal = not args.no_inclusion_removal
        
        if apply_inclusion_removal:
            print("ğŸ”¥ Processing DeepFurniture dataset: WITH inclusion relationship removal")
        else:
            print("ğŸ”¥ Processing DeepFurniture dataset: WITHOUT inclusion relationship removal")
        
        process_deepfurniture_with_inclusion_removal(
            image_dir=args.image_dir,
            annotations_json=args.annotations_json,
            furnitures_jsonl=args.furnitures_jsonl,
            output_dir=args.output_dir,
            batch_size=args.batch_size,
            apply_inclusion_removal=apply_inclusion_removal
        )


if __name__ == "__main__":
    main()