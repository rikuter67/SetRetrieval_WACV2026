import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, Model
from typing import List
import pdb
# tf.config.run_functions_eagerly(True) 


# ============================================================================
# ãƒ¡ãƒˆãƒªãƒƒã‚¯å®šç¾©
# ============================================================================
# class BatchTopKAccuracy(tf.keras.metrics.Metric):
#     """ãƒãƒƒãƒå†…TopKæ­£è§£ç‡ãƒ¡ãƒˆãƒªãƒƒã‚¯"""
#     def __init__(self, k=1, name='top1_accuracy', **kwargs):
#         super().__init__(name=name, **kwargs)
#         self.k = tf.constant(float(k), dtype=tf.float32)
#         self.total_correct = self.add_weight(name='total_correct', initializer='zeros', dtype=tf.float32)
#         self.total_count = self.add_weight(name='total_count', initializer='zeros', dtype=tf.float32)
    
#     def update_state(self, y_true, y_pred, sample_weight=None):
#         similarities = tf.cast(y_pred, tf.float32)
#         batch_size = tf.cast(tf.shape(similarities)[0], tf.float32)
        
#         def compute_metrics():
#             diagonal = tf.linalg.diag_part(similarities)
#             expanded_diag = tf.expand_dims(diagonal, axis=1)
#             better_mask = similarities > expanded_diag
#             batch_size_int = tf.cast(batch_size, tf.int32)
#             eye_mask = tf.eye(batch_size_int, dtype=tf.bool)
#             better_mask = tf.logical_and(better_mask, tf.logical_not(eye_mask))
#             num_better = tf.reduce_sum(tf.cast(better_mask, tf.float32), axis=1)
#             ranks = num_better + tf.constant(1.0, dtype=tf.float32)
#             k_threshold = tf.minimum(self.k, batch_size - tf.constant(1.0, dtype=tf.float32))
#             k_threshold = tf.maximum(k_threshold, tf.constant(1.0, dtype=tf.float32))
#             correct = tf.reduce_sum(tf.cast(ranks <= k_threshold, tf.float32))
#             return correct, batch_size
        
#         def skip_metrics():
#             return tf.constant(0.0, dtype=tf.float32), tf.constant(0.0, dtype=tf.float32)
        
#         correct_count, valid_count = tf.cond(
#             tf.greater(batch_size, tf.constant(1.0, dtype=tf.float32)),
#             compute_metrics,
#             skip_metrics
#         )
#         self.total_correct.assign_add(correct_count)
#         self.total_count.assign_add(valid_count)
    
#     def result(self):
#         accuracy = tf.math.divide_no_nan(self.total_correct, self.total_count)
#         return accuracy * 100.0
    
#     def reset_state(self):
#         self.total_correct.assign(0.0)
#         self.total_count.assign(0.0)

class PerCategoryItemTopKAccuracy(tf.keras.metrics.Metric):
   """
   æœ€å°é™ã®ä¿®æ­£ã§ã‚¨ãƒ©ãƒ¼ã‚’è§£æ±ºã—ãŸç‰ˆ
   - tf.rangeãƒ«ãƒ¼ãƒ—ã‚’Pythonãƒ«ãƒ¼ãƒ—ã«å¤‰æ›´
   - XLAç„¡åŠ¹åŒ–
   - å‹•çš„å½¢çŠ¶å•é¡Œã‚’å›é¿
   """
   def __init__(self, k, num_categories, name='per_category_topk_acc', **kwargs):
       super().__init__(name=name, **kwargs)
       self.k_percent = float(k)  # é™çš„å€¤ã¨ã—ã¦ä¿å­˜
       self.num_categories = num_categories
       
       self.total_correct = self.add_weight(name='total_correct', initializer='zeros')
       self.total_count = self.add_weight(name='total_count', initializer='zeros')

   @tf.function(jit_compile=False)  # XLAç„¡åŠ¹åŒ–
   def update_state(self, predictions, target_features, target_categories, sample_weight=None):
       """
       ä¿®æ­£ç‰ˆTopK%ç²¾åº¦è¨ˆç®—
       """
       B, S, D = tf.shape(target_features)[0], tf.shape(target_features)[1], tf.shape(target_features)[2]
       
       # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ©ãƒƒãƒˆåŒ–
       target_features_flat = tf.reshape(target_features, [-1, D])
       target_categories_flat = tf.reshape(target_categories, [-1])
       
       # ãƒãƒƒãƒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
       batch_indices = tf.range(B, dtype=tf.int32)
       batch_grid = tf.tile(tf.expand_dims(batch_indices, 1), [1, S])
       batch_indices_flat = tf.reshape(batch_grid, [-1])
       
       # æœ‰åŠ¹ã‚¢ã‚¤ãƒ†ãƒ ãƒã‚¹ã‚¯
       valid_mask = tf.logical_and(
           target_categories_flat > 0,
           tf.reduce_sum(tf.abs(target_features_flat), axis=-1) > 1e-6
       )
       
       valid_features = tf.boolean_mask(target_features_flat, valid_mask)
       valid_categories = tf.boolean_mask(target_categories_flat, valid_mask)
       valid_batch_indices = tf.boolean_mask(batch_indices_flat, valid_mask)
       
       N_valid = tf.shape(valid_features)[0]

       # æ¡ä»¶åˆ†å²ã§å‡¦ç†
       def process_when_valid():
           # äºˆæ¸¬ãƒ™ã‚¯ãƒˆãƒ«å–å¾—
           pred_indices = tf.stack([
               valid_batch_indices,
               tf.maximum(0, valid_categories - 1)
           ], axis=1)
           pred_vectors = tf.gather_nd(predictions, pred_indices)
           
           # æ­£è¦åŒ–
           pred_vectors = tf.nn.l2_normalize(pred_vectors, axis=-1)
           valid_features_norm = tf.nn.l2_normalize(valid_features, axis=-1)
           
           total_correct_items = 0.0
           total_items = 0.0
           
           # Pythonã®rangeã‚’ä½¿ç”¨ï¼ˆTensorFlowã®tf.rangeã§ã¯ãªã„ï¼‰
           for cat_id in range(1, self.num_categories + 1):
               cat_id_tensor = tf.constant(cat_id, dtype=tf.int32)
               cat_mask = tf.equal(valid_categories, cat_id_tensor)
               cat_count = tf.reduce_sum(tf.cast(cat_mask, tf.int32))
               
               # æ¡ä»¶åˆ†å²ã‚’ä½¿ã£ã¦å‡¦ç†
               def process_category():
                   cat_features = tf.boolean_mask(valid_features_norm, cat_mask)
                   cat_pred_vectors = tf.boolean_mask(pred_vectors, cat_mask)
                   cat_count_float = tf.cast(cat_count, tf.float32)
                   
                   # é¡ä¼¼åº¦è¡Œåˆ—
                   sim_matrix = tf.matmul(cat_pred_vectors, cat_features, transpose_b=True)
                   diagonal_sims = tf.linalg.diag_part(sim_matrix)
                   
                   # ãƒ©ãƒ³ã‚¯è¨ˆç®—
                   expanded_diag = tf.expand_dims(diagonal_sims, axis=1)
                   
                   better_mask = tf.logical_and(
                       sim_matrix > expanded_diag,
                       tf.logical_not(tf.eye(tf.shape(sim_matrix)[0], dtype=tf.bool))
                   )
                   ranks = tf.reduce_sum(tf.cast(better_mask, tf.float32), axis=1) + 1.0
                   
                   # TopK%é–¾å€¤
                   k_threshold = tf.maximum(
                       1.0, 
                       (self.k_percent / 100.0) * cat_count_float
                   )
                   
                   correct_in_cat = tf.reduce_sum(tf.cast(ranks <= k_threshold, tf.float32))
                   return correct_in_cat, cat_count_float
               
               def skip_category():
                   return 0.0, 0.0
               
               # tf.condã§æ¡ä»¶åˆ†å²
               correct_items, count_items = tf.cond(
                   cat_count > 1,
                   process_category,
                   skip_category
               )
               
               total_correct_items += correct_items
               total_items += count_items
           
           return total_correct_items, total_items
           
       def skip_when_empty():
           return 0.0, 0.0
       
       # ãƒ¡ã‚¤ãƒ³æ¡ä»¶åˆ†å²
       correct_items, count_items = tf.cond(
           N_valid > 0,
           process_when_valid,
           skip_when_empty
       )
       
       # çµ±è¨ˆæ›´æ–°
       self.total_correct.assign_add(correct_items)
       self.total_count.assign_add(count_items)

   def result(self):
       return tf.math.divide_no_nan(self.total_correct, self.total_count) * 100.0

   def reset_state(self):
       self.total_correct.assign(0.0)
       self.total_count.assign(0.0)

        
# ============================================================================
# ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«å®šç¾©
# ============================================================================
class SetRetrievalBaseModel(Model):
    """Set Retrieval Model ã®åŸºåº•ã‚¯ãƒ©ã‚¹ï¼ˆå…±æœ‰ãƒ­ã‚¸ãƒƒã‚¯ï¼‰"""
    
    def __init__(self, 
                 feature_dim: int = 512,
                 num_heads: int = 8,
                 num_layers: int = 6,
                 num_categories: int = 16,
                 hidden_dim: int = 512,
                 use_cycle_loss: bool = False,
                 temperature: float = 1.0,
                 dropout_rate: float = 0.1,
                 k_values: List[int] = None,
                 cycle_lambda: float = 0.1,
                 cluster_centering: bool = False,
                 use_tpaneg: bool = False,
                 # è«–æ–‡ã®T_gamma (TaNegã®é¡ä¼¼åº¦é–¾å€¤) ã«å¯¾å¿œ
                 taneg_t_gamma_init: float = 0.5, 
                 taneg_t_gamma_final: float = 0.8, # <-- æ–°ã—ã„å¼•æ•°
                 taneg_curriculum_epochs: int = 100, # <-- æ–°ã—ã„å¼•æ•°
                 # è«–æ–‡ã®epsilon (PaNegã®ãƒãƒ¼ã‚¸ãƒ³) ã«å¯¾å¿œ
                 paneg_epsilon: float = 0.2, 
                 **kwargs):

        super().__init__(**kwargs)
        
        self.feature_dim = feature_dim
        self.num_heads = num_heads
        self.num_layers = num_layers
        self.num_categories = num_categories
        self.hidden_dim = hidden_dim
        self.use_cycle_loss = use_cycle_loss
        self.temperature = temperature
        self.dropout_rate = dropout_rate
        self.k_values = k_values if k_values is not None else [1, 5, 10, 20]
        self.cycle_lambda = cycle_lambda
        self.cluster_centering = cluster_centering
        self.use_tpaneg = use_tpaneg 

        # è«–æ–‡ã®T_gammaã®åˆæœŸå€¤ã€æœ€çµ‚å€¤ã€ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã‚¨ãƒãƒƒã‚¯æ•°
        self.taneg_t_gamma_init = taneg_t_gamma_init
        self.taneg_t_gamma_final = taneg_t_gamma_final
        self.taneg_curriculum_epochs = taneg_curriculum_epochs
        # è«–æ–‡ã®epsilon (PaNegã®ãƒãƒ¼ã‚¸ãƒ³)
        self.paneg_epsilon = paneg_epsilon

        self.category_centers = None
        self.loss_tracker = tf.keras.metrics.Mean(name="loss")
        self.xy_loss_tracker = tf.keras.metrics.Mean(name="X->Y' Loss")
        self.yx_loss_tracker = tf.keras.metrics.Mean(name="Y'->X' Loss")
        
        self._build_layers()
        self._build_topk_metrics()
    
    def _build_layers(self):
        self.input_projection = layers.Dense(self.hidden_dim, activation='relu', name='input_projection')
        self.cross_attention_layers = []
        for i in range(self.num_layers):
            layer_dict = {
                'cross_attention': layers.MultiHeadAttention(num_heads=self.num_heads, key_dim=self.hidden_dim // self.num_heads, dropout=self.dropout_rate, name=f'cross_attention_{i}'),
                'self_attention': layers.MultiHeadAttention(num_heads=self.num_heads, key_dim=self.hidden_dim // self.num_heads, dropout=self.dropout_rate, name=f'self_attention_{i}'),
                'norm1': layers.LayerNormalization(epsilon=1e-6, name=f'norm1_{i}'),
                'norm2': layers.LayerNormalization(epsilon=1e-6, name=f'norm2_{i}'),
                'norm3': layers.LayerNormalization(epsilon=1e-6, name=f'norm3_{i}'),
                'ffn': tf.keras.Sequential([
                    layers.Dense(self.hidden_dim * 2, activation='gelu'),
                    layers.Dropout(self.dropout_rate),
                    layers.Dense(self.hidden_dim)
                ], name=f'ffn_{i}')
            }
            self.cross_attention_layers.append(layer_dict)
        self.output_projection = layers.Dense(self.feature_dim, activation=None, name='output_projection')
        # BUG: This layer was defined but never used in the call method.
        # It's not critical for fixing the current stagnation, but should be added for correctness later.
        self.output_norm = layers.LayerNormalization(epsilon=1e-6, name='output_norm')

    def _build_topk_metrics(self):
        self.topk_metrics = {}
        for k in sorted(self.k_values): 
            self.topk_metrics[f'top{k}_accuracy'] = PerCategoryItemTopKAccuracy(k, self.num_categories, name=f'top{k}_accuracy')
            self.topk_metrics[f'val_top{k}_accuracy'] = PerCategoryItemTopKAccuracy(k, self.num_categories, name=f'val_top{k}_accuracy')

    @property
    def metrics(self):
        base_metrics = [self.loss_tracker, self.xy_loss_tracker, self.yx_loss_tracker]
        return base_metrics + list(self.topk_metrics.values())
        
    def set_category_centers(self, centers: np.ndarray, whitening_params=None):
        if centers.shape[0] != self.num_categories:
            raise ValueError(f"Expected {self.num_categories} centers, got {centers.shape[0]}")
        if whitening_params is not None:
            print("ğŸ”„ Applying whitening transformation to category centers...")
            centers = np.dot(centers - whitening_params['mean'], whitening_params['matrix'])
            print(f"âœ… Category centers whitened: {centers.shape}")
        centers = centers / (np.linalg.norm(centers, axis=1, keepdims=True) + 1e-8)
        self.category_centers = self.add_weight(name='category_centers', shape=(self.num_categories, self.hidden_dim), initializer='glorot_uniform', trainable=True)
        if centers.shape[1] != self.hidden_dim:
            init_centers = np.random.normal(0, 0.02, (self.num_categories, self.hidden_dim)).astype(np.float32)
        else:
            init_centers = centers.astype(np.float32)
        self.category_centers.assign(init_centers)
        print(f"âœ… Category centers initialized: {centers.shape}")

    def call(self, inputs, training=None):
        query_features = inputs['query_features']
        query_projected = self.input_projection(query_features)
        if self.category_centers is None:
            raise ValueError("Category centers not set! Call set_category_centers() first.")
        batch_size = tf.shape(query_features)[0]
        x = tf.tile(tf.expand_dims(self.category_centers, 0), [batch_size, 1, 1])
        for layer_dict in self.cross_attention_layers:
            cross_attn_out = layer_dict['cross_attention'](query=x, key=query_projected, value=query_projected, training=training)
            x = layer_dict['norm1'](x + cross_attn_out, training=training)
            self_attn_out = layer_dict['self_attention'](query=x, key=x, value=x, training=training)
            x = layer_dict['norm2'](x + self_attn_out, training=training)
            ffn_out = layer_dict['ffn'](x, training=training)
            x = layer_dict['norm3'](x + ffn_out, training=training)
        predictions = self.output_projection(x)

        # NOTE: self.output_norm is not used here. This is a potential bug but not the cause of the stagnation.
        predictions = self.output_norm(predictions) 

        if self.cluster_centering:
            cluster_centers_normalized = tf.nn.l2_normalize(self.category_centers, axis=-1)
            predictions = predictions - tf.expand_dims(cluster_centers_normalized, 0)
            predictions = tf.nn.l2_normalize(predictions, axis=-1)
        return predictions

    # def _compute_set_similarities_fixed(self, predictions, target_features):
    #     target_mask = tf.reduce_sum(tf.abs(target_features), axis=-1) > 0
    #     masked_targets = target_features * tf.cast(tf.expand_dims(target_mask, -1), tf.float32)
    #     target_sum = tf.reduce_sum(masked_targets, axis=1)
    #     target_count = tf.maximum(tf.reduce_sum(tf.cast(target_mask, tf.float32), axis=1, keepdims=True), 1.0)
    #     target_repr = target_sum / target_count
    #     if self.cluster_centering:
    #         cluster_centers_normalized = tf.nn.l2_normalize(self.category_centers, axis=-1)
    #         avg_cluster_center = tf.reduce_mean(cluster_centers_normalized, axis=0, keepdims=True)
    #         target_repr = target_repr - avg_cluster_center
    #         target_repr = tf.nn.l2_normalize(target_repr, axis=-1)
    #     all_pairwise_similarities_by_category = tf.einsum('qcd,td->qct', predictions, target_repr)
    #     return tf.reduce_max(all_pairwise_similarities_by_category, axis=1)

    def infer_single_set(self, query_features):
        if len(query_features.shape) == 2:
            query_features = tf.expand_dims(query_features, 0)
        predictions = self({'query_features': query_features}, training=False)
        return tf.squeeze(predictions, axis=0)


class TPaNegModel(SetRetrievalBaseModel):
    def __init__(self, *args, **kwargs):
        if 'use_clcatneg' in kwargs:
            kwargs['use_tpaneg'] = kwargs.pop('use_clcatneg')
        super().__init__(*args, **kwargs)
        print("ğŸš€ TPaNeg (Dynamic Hard Negative) Model initialized")

        self.validation_loss_tracker = tf.keras.metrics.Mean(name="loss") 
        self.reconstruct_x_loss_tracker = tf.keras.metrics.Mean(name="L_RecX")
        self.reconstruct_y_loss_tracker = tf.keras.metrics.Mean(name="L_RecY")

        self.initial_taneg_t_gamma_curr = self.taneg_t_gamma_init
        self.final_taneg_t_gamma_curr = self.taneg_t_gamma_final
        self.taneg_curriculum_epochs_val = self.taneg_curriculum_epochs
        self.current_epoch = 0 
        
        print(f"ğŸ“š Curriculum Learning (TaNeg T_gamma): {self.initial_taneg_t_gamma_curr:.1f} â†’ {self.final_taneg_t_gamma_curr:.1f} over {self.taneg_curriculum_epochs_val} epochs")
        print(f"ğŸ“š PaNeg Epsilon (fixed Îµ): {self.paneg_epsilon:.1f}")

    def get_current_taneg_t_gamma(self):
        if self.current_epoch >= self.taneg_curriculum_epochs_val:
            return self.final_taneg_t_gamma_curr
        
        progress = self.current_epoch / self.taneg_curriculum_epochs_val
        current_t_gamma = self.initial_taneg_t_gamma_curr + (self.final_taneg_t_gamma_curr - self.initial_taneg_t_gamma_curr) * progress
        return current_t_gamma

    def set_current_epoch(self, epoch):
        self.current_epoch = epoch
        current_t_gamma = self.get_current_taneg_t_gamma()
        if epoch % 10 == 0:
            print(f"ğŸ“š Epoch {epoch}: TaNeg T_gamma = {current_t_gamma:.3f}")

    @property
    def metrics(self):
        common_metrics = [self.loss_tracker, self.xy_loss_tracker, self.yx_loss_tracker]
        all_topk_metrics = list(self.topk_metrics.values())
        val_main_loss_metric = [self.validation_loss_tracker]
        reconstruction_metrics = [self.reconstruct_x_loss_tracker, self.reconstruct_y_loss_tracker]

        return common_metrics + reconstruction_metrics + all_topk_metrics + val_main_loss_metric

    @tf.function
    def train_step(self, data):
        with tf.GradientTape() as tape:
            pred_Y = self({'query_features': data['query_features']}, training=True)
            pred_X = self({'query_features': data['target_features']}, training=True)

            if self.use_tpaneg:
                current_taneg_t_gamma = self.get_current_taneg_t_gamma()
                
                loss_X_to_Y = self._compute_tpaneg_loss(
                    pred_Y,
                    data['target_features'], data['target_categories'],
                    data['candidate_negative_features'], data['candidate_negative_masks'],
                    current_taneg_t_gamma
                )
                loss_Y_to_X = self._compute_tpaneg_loss(
                    pred_X,
                    data['query_features'], data['query_categories'],
                    data['query_candidate_negative_features'], data['query_candidate_negative_masks'],
                    current_taneg_t_gamma
                )

                total_loss = loss_X_to_Y + loss_Y_to_X

                if self.use_cycle_loss:
                    # Cycle loss is not part of the current debugging, so this branch is not used.
                    reconstructed_X = self({'query_features': pred_Y}, training=True)
                    reconstructed_Y = self({'query_features': pred_X}, training=True)
                    
                    cycle_loss_X = self._compute_tpaneg_loss(
                        reconstructed_X, 
                        data['query_features'], data['query_categories'],
                        data['query_candidate_negative_features'], data['query_candidate_negative_masks'],
                        current_taneg_t_gamma
                    )
                    cycle_loss_Y = self._compute_tpaneg_loss(
                        reconstructed_Y,
                        data['target_features'], data['target_categories'],
                        data['candidate_negative_features'], data['candidate_negative_masks'],
                        current_taneg_t_gamma
                    )
                    total_loss += self.cycle_lambda * (cycle_loss_X + cycle_loss_Y)

            else: 
                loss_X_to_Y = self._compute_in_batch_hard_negative_loss(
                    pred_Y,
                    data['target_features'],
                    data['target_categories']
                )
                loss_Y_to_X = self._compute_in_batch_hard_negative_loss(
                    pred_X,
                    data['query_features'],
                    data['query_categories']
                )

                total_loss = loss_X_to_Y + loss_Y_to_X

                if self.use_cycle_loss:
                    # Cycle loss is not part of the current debugging, so this branch is not used.
                    reconstructed_X = self({'query_features': pred_Y}, training=True)
                    reconstructed_Y = self({'query_features': pred_X}, training=True)
                    
                    cycle_loss_X = self._compute_in_batch_hard_negative_loss(
                        reconstructed_X, data['query_features'], data['query_categories']
                    )
                    cycle_loss_Y = self._compute_in_batch_hard_negative_loss(
                        reconstructed_Y, data['target_features'], data['target_categories']
                    )
                    total_loss += self.cycle_lambda * (cycle_loss_X + cycle_loss_Y)

        gradients = tape.gradient(total_loss, self.trainable_variables)


    # # ğŸ” å‹¾é…ãƒ‡ãƒãƒƒã‚°ã‚³ãƒ¼ãƒ‰è¿½åŠ 
    #     if tf.equal(tf.cast(self.optimizer.iterations, tf.int32) % 10, 0):  # 10ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨
    #         tf.print("=== Gradient Debug ===")
    #         tf.print("Total Loss:", total_loss)
    #         tf.print("Loss X->Y:", loss_X_to_Y)
    #         tf.print("Loss Y->X:", loss_Y_to_X)
            
    #         # å‹¾é…ã®çµ±è¨ˆ
    #         grad_norms = [tf.norm(g) for g in gradients if g is not None]
    #         if grad_norms:
    #             tf.print("Gradient norms (first 5):", grad_norms[:5])
    #             tf.print("Max gradient norm:", tf.reduce_max(grad_norms))
    #             tf.print("Mean gradient norm:", tf.reduce_mean(grad_norms))
    #             tf.print("Num None gradients:", tf.reduce_sum([1 for g in gradients if g is None]))
    #         else:
    #             tf.print("âš ï¸ ALL GRADIENTS ARE NONE!")


        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))

        self.loss_tracker.update_state(total_loss)
        self.xy_loss_tracker.update_state(loss_X_to_Y)
        self.yx_loss_tracker.update_state(loss_Y_to_X)
        if self.use_cycle_loss:
            self.reconstruct_x_loss_tracker.update_state(cycle_loss_X)
            self.reconstruct_y_loss_tracker.update_state(cycle_loss_Y)

        for k in self.k_values:
            metric_name = f'top{k}_accuracy'
            if metric_name in self.topk_metrics:
                 self.topk_metrics[metric_name].update_state(
                    pred_Y, 
                    data['target_features'], 
                    data['target_categories']
                )
        
        return {m.name: m.result() for m in self.metrics if not m.name.startswith('val_')}


    @tf.function
    def test_step(self, data):
        pred_Y = self({'query_features': data['query_features']}, training=False)
        
        current_taneg_t_gamma = self.get_current_taneg_t_gamma()
        
        if self.use_tpaneg and 'candidate_negative_features' in data:
            val_loss = self._compute_tpaneg_loss(
                pred_Y, data['target_features'], data['target_categories'],
                data['candidate_negative_features'], data['candidate_negative_masks'],
                current_taneg_t_gamma
            )
        else:
            val_loss = self._compute_in_batch_hard_negative_loss(
                pred_Y, data['target_features'], data['target_categories']
            )
        
        self.validation_loss_tracker.update_state(val_loss)

        for k in self.k_values:
            metric_name = f'val_top{k}_accuracy'
            if metric_name in self.topk_metrics:
                self.topk_metrics[metric_name].update_state(
                    pred_Y, data['target_features'], data['target_categories'])

        results = {"loss": self.validation_loss_tracker.result()}
        for k in self.k_values:
            metric_name = f'val_top{k}_accuracy'
            if metric_name in self.topk_metrics:
                results[metric_name] = self.topk_metrics[metric_name].result()
        return results

    def _get_predictions_for_items(self, predictions, categories):
        shape = tf.shape(categories)
        B, S = shape[0], shape[1]
        cat_indices = tf.expand_dims(tf.maximum(categories - 1, 0), axis=-1)
        batch_indices = tf.tile(tf.reshape(tf.range(B, dtype=tf.int32), [B, 1, 1]), [1, S, 1])
        indices = tf.concat([batch_indices, cat_indices], axis=-1)
        return tf.gather_nd(predictions, indices)
    
    
    # def _compute_standard_contrastive_loss(self, predictions, target_features, target_categories):
    #     sim_matrix = self._compute_set_similarities_fixed(predictions, target_features)
    #     labels = tf.range(tf.shape(sim_matrix)[0])
    #     loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=sim_matrix / self.temperature)
    #     return tf.reduce_mean(loss)
        
    @tf.function
    def _compute_in_batch_hard_negative_loss(self, predictions, target_features, target_categories):
        """
        In-batch Negative Samplingã«åŸºã¥ãã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒ†ã‚£ãƒ–å­¦ç¿’æå¤±ã€‚
        åŒã˜ã‚»ãƒƒãƒˆå†…ã®åŒã˜ã‚«ãƒ†ã‚´ãƒªã®ã‚¢ã‚¤ãƒ†ãƒ ã¯ãƒã‚¬ãƒ†ã‚£ãƒ–ã¨ã—ã¦æ‰±ã‚ãªã„ã€‚
        """
        B, S, D = tf.shape(target_features)[0], tf.shape(target_features)[1], tf.shape(target_features)[2]
        
        # 1. ãƒ‡ãƒ¼ã‚¿ã®ãƒ•ãƒ©ãƒƒãƒˆåŒ–ã¨å‰å‡¦ç†
        # target_features ã¯ Y
        # predictions ã¯ Y' (Batch_Size, Num_Categories, Feature_Dimension)

        target_feats_flat = tf.reshape(target_features, [-1, D]) # (B*S, D)
        target_cats_flat = tf.reshape(target_categories, [-1])   # (B*S)
        
        # ã‚ªãƒªã‚¸ãƒŠãƒ«ã®ãƒãƒƒãƒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨ã‚¢ã‚¤ãƒ†ãƒ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒšã‚¢ã‚’ãƒ•ãƒ©ãƒƒãƒˆåŒ–
        batch_indices = tf.range(B, dtype=tf.int32)
        item_indices = tf.range(S, dtype=tf.int32)
        grid_b, grid_i = tf.meshgrid(batch_indices, item_indices, indexing='ij')
        original_flat_batch_indices = tf.reshape(grid_b, [-1]) # å„ã‚¢ã‚¤ãƒ†ãƒ ã®å…ƒã®ãƒãƒƒãƒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ (B*S,)
        original_flat_item_indices = tf.reshape(grid_i, [-1])  # å„ã‚¢ã‚¤ãƒ†ãƒ ã®å…ƒã®ã‚»ãƒƒãƒˆå†…ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ (B*S,)
        
        # æœ‰åŠ¹ã‚¢ã‚¤ãƒ†ãƒ ãƒã‚¹ã‚¯ (ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã•ã‚ŒãŸã‚¢ã‚¤ãƒ†ãƒ ã‚’é™¤å¤–)
        is_valid_item_mask = tf.logical_and(
            target_cats_flat > 0, # ã‚«ãƒ†ã‚´ãƒªIDãŒ0ã‚ˆã‚Šå¤§ãã„
            tf.reduce_sum(tf.abs(target_feats_flat), axis=-1) > 1e-6 # ç‰¹å¾´é‡ãŒã‚¼ãƒ­ã§ãªã„
        )
        
        # äºˆæ¸¬ãƒ™ã‚¯ãƒˆãƒ«ã®å–å¾— (target_features ã®å„ã‚¢ã‚¤ãƒ†ãƒ ãŒå±ã™ã‚‹ã‚«ãƒ†ã‚´ãƒªã«å¯¾å¿œã™ã‚‹ predictions ã®ãƒ™ã‚¯ãƒˆãƒ«)
        # predictions ã®å½¢çŠ¶ã¯ [B, C, D]
        # target_cats_flat ã¯ [B*S]
        preds_indices_for_gather = tf.stack([original_flat_batch_indices, tf.maximum(0, target_cats_flat - 1)], axis=1) # (B*S, 2)
        preds_for_items_flat = tf.gather_nd(predictions, preds_indices_for_gather) # (B*S, D)
        
        # NaN/Infå¯¾ç­–ã¨æ­£è¦åŒ–
        preds = tf.where(tf.math.is_finite(preds_for_items_flat), preds_for_items_flat, 0.0)
        targets = tf.where(tf.math.is_finite(target_feats_flat), target_feats_flat, 0.0)
        
        preds_norm, _ = tf.linalg.normalize(preds + 1e-8, axis=-1)
        targets_norm, _ = tf.linalg.normalize(targets + 1e-8, axis=-1)
        
        # 2. ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®æå¤±è¨ˆç®—
        total_loss = 0.0
        total_items_contributing = 0.0 # æå¤±ã«è²¢çŒ®ã™ã‚‹ã‚¢ã‚¤ãƒ†ãƒ ã®ç·æ•°
        
        for cat_id in range(1, self.num_categories + 1):
            cat_id_tensor = tf.constant(cat_id, dtype=tf.int32)
            
            # ã“ã®ã‚«ãƒ†ã‚´ãƒªã«å±ã—ã€ã‹ã¤æœ‰åŠ¹ãªã‚¢ã‚¤ãƒ†ãƒ ã®ãƒã‚¹ã‚¯
            cat_specific_mask = tf.logical_and(
                tf.equal(target_cats_flat, cat_id_tensor),
                is_valid_item_mask
            )
            
            # ã“ã®ã‚«ãƒ†ã‚´ãƒªã®ã‚¢ã‚¤ãƒ†ãƒ ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ (flat_indices_in_cat)
            indices_in_cat = tf.where(cat_specific_mask)[:, 0] # (N_cat,)
            
            cat_count = tf.shape(indices_in_cat)[0]
            
            def process_category():
                # ã“ã®ã‚«ãƒ†ã‚´ãƒªã®ã‚¢ã‚¤ãƒ†ãƒ ã®äºˆæ¸¬ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚’æŠ½å‡º
                cat_preds = tf.gather(preds_norm, indices_in_cat)    # (N_cat, D)
                cat_targets = tf.gather(targets_norm, indices_in_cat) # (N_cat, D)
                
                # åŒã˜ã‚«ãƒ†ã‚´ãƒªã®ã‚¢ã‚¤ãƒ†ãƒ ã®å…ƒã®ãƒãƒƒãƒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚‚æŠ½å‡º
                cat_original_batch_indices = tf.gather(original_flat_batch_indices, indices_in_cat) # (N_cat,)
                
                # ã‚«ãƒ†ã‚´ãƒªå†…é¡ä¼¼åº¦è¡Œåˆ— (N_cat x N_cat)
                sim_matrix = tf.matmul(cat_preds, cat_targets, transpose_b=True)
                
                # --- åŒã˜ã‚»ãƒƒãƒˆå†…ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’é™¤å¤–ã™ã‚‹ãƒã‚¹ã‚¯ã‚’ä½œæˆ ---
                # å½¢çŠ¶: (N_cat, N_cat)
                # iç•ªç›®ã®ã‚¯ã‚¨ãƒªã¨jç•ªç›®ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãŒåŒã˜ãƒãƒƒãƒï¼ˆã‚»ãƒƒãƒˆï¼‰ã«å±ã™ã‚‹ã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯
                # cat_original_batch_indices ã¯ (N_cat,) ãªã®ã§ã€ã“ã‚Œã‚’ (N_cat, 1) ã¨ (1, N_cat) ã«æ‹¡å¼µã—ã¦æ¯”è¼ƒ
                same_batch_mask = tf.equal(
                    tf.expand_dims(cat_original_batch_indices, 1), # (N_cat, 1)
                    tf.expand_dims(cat_original_batch_indices, 0)  # (1, N_cat)
                ) # (N_cat, N_cat)
                
                # ãƒã‚¸ãƒ†ã‚£ãƒ–ãƒšã‚¢ (å¯¾è§’æˆåˆ†) ã‚’å–å¾—
                identity_mask = tf.eye(cat_count, dtype=tf.bool) # (N_cat, N_cat)
                
                # ãƒã‚¬ãƒ†ã‚£ãƒ–ã¨ã—ã¦æ‰±ã†ãƒšã‚¢ã®ãƒã‚¹ã‚¯:
                # 1. ãƒã‚¸ãƒ†ã‚£ãƒ–ãƒšã‚¢ã§ã¯ãªã„ (identity_mask == False)
                # 2. åŒã˜ãƒãƒƒãƒã«å±ã•ãªã„ (same_batch_mask == False)
                # ãŸã ã—ã€ä»Šå›ã¯ã€ŒåŒã˜ãƒãƒƒãƒã«å±ã•ãªã„ã‹ã€ã‹ã¤ãƒã‚¸ãƒ†ã‚£ãƒ–ãƒšã‚¢ã§ã¯ãªã„ã€ã‚’çµ„ã¿åˆã‚ã›ã‚‹
                
                # æ­£ã—ã„ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒã‚¹ã‚¯ã®ãƒ­ã‚¸ãƒƒã‚¯:
                # ãƒã‚¬ãƒ†ã‚£ãƒ–ã¨ã—ã¦åˆ©ç”¨ã—ãŸã„ã®ã¯ã€ã€Œè‡ªåˆ†è‡ªèº«ã§ã¯ãªã„ã€ã‹ã¤ã€ŒåŒã˜ãƒãƒƒãƒå†…ã§ã¯ãªã„ã€ãƒšã‚¢
                # ã‚‚ã—ãã¯ã€ã‚ãªãŸã®æ„å›³ã€ŒåŒã˜B_1å†…ã§ã‚‚åŒã˜ã‚«ãƒ†ã‚´ãƒªã®æ­£è§£ä»¥å¤–ã®ã‚¢ã‚¤ãƒ†ãƒ ã¯ãƒã‚¸ãƒ†ã‚£ãƒ–ã¨ã—ã¦ã‚‚ãƒã‚¬ãƒ†ã‚£ãƒ–ã¨ã—ã¦ã‚‚æ‰±ã„ãŸããªã„ã€‚ã€
                # ã‚’å³å¯†ã«è§£é‡ˆã™ã‚‹ã¨ã€
                # ãƒã‚¸ãƒ†ã‚£ãƒ–: è‡ªåˆ†è‡ªèº« (å¯¾è§’)
                # ãƒã‚¬ãƒ†ã‚£ãƒ–: åŒã˜ã‚«ãƒ†ã‚´ãƒªã ãŒã€ç•°ãªã‚‹ãƒãƒƒãƒã«å±ã™ã‚‹ã‚¢ã‚¤ãƒ†ãƒ  (same_batch_mask == False AND NOT identity_mask)
                # é™¤å¤–: åŒã˜ã‚«ãƒ†ã‚´ãƒªã§ã€åŒã˜ãƒãƒƒãƒã«å±ã™ã‚‹ãŒã€è‡ªåˆ†è‡ªèº«ã§ã¯ãªã„ã‚¢ã‚¤ãƒ†ãƒ  (same_batch_mask == True AND NOT identity_mask)

                # --- ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒã‚¹ã‚¯ã®æ§‹ç¯‰ ---
                # ã¾ãšã€ãƒã‚¬ãƒ†ã‚£ãƒ–ã«ã—ãŸã„ã®ã¯ã€Œå¯¾è§’æˆåˆ†ä»¥å¤–ã€
                neg_base_mask = tf.logical_not(identity_mask)
                
                # ãã—ã¦ã€ã€ŒåŒã˜ãƒãƒƒãƒã«å±ã™ã‚‹ã€ã‚‚ã®ã‚’ãƒã‚¬ãƒ†ã‚£ãƒ–ã‹ã‚‰é™¤å¤–ã—ãŸã„
                # â†’ same_batch_mask ãŒ True ã®å ´æ‰€ã¯ãƒã‚¬ãƒ†ã‚£ãƒ–ã§ã¯ãªã„
                # æœ€çµ‚çš„ãªãƒã‚¬ãƒ†ã‚£ãƒ–ãƒã‚¹ã‚¯ã¯ (neg_base_mask AND NOT same_batch_mask)
                neg_mask = tf.logical_and(neg_base_mask, tf.logical_not(same_batch_mask))
                
                # InfoNCEæå¤±ã§ã¯ã€æ­£è§£ä»¥å¤–ã‚’å…¨ã¦ãƒã‚¬ãƒ†ã‚£ãƒ–ã¨ã—ã¦æ‰±ã†ãŸã‚ã€
                # ãƒã‚¬ãƒ†ã‚£ãƒ–ã¨ã—ã¦æ‰±ã„ãŸããªã„éƒ¨åˆ†ã®é¡ä¼¼åº¦ã‚’éå¸¸ã«å°ã•ãªå€¤ã«ã™ã‚‹
                # (ä¾‹: -inf ã«è¿‘ã„å€¤) ã“ã¨ã§ã€softmaxè¨ˆç®—ã‹ã‚‰é™¤å¤–ã™ã‚‹
                
                # é™¤å¤–ã—ãŸã„è¦ç´ ã«éå¸¸ã«å°ã•ãªå€¤ã‚’è¨­å®š
                # same_batch_mask && NOT identity_mask ã®ä½ç½®ã« -inf ã‚’è¨­å®š
                # ã“ã‚Œã¯ã€åŒã˜ãƒãƒƒãƒã«å±ã™ã‚‹ãŒè‡ªåˆ†è‡ªèº«ã§ã¯ãªã„ã€ã¨ã„ã†ä½ç½®
                mask_to_exclude_from_negatives = tf.logical_and(same_batch_mask, neg_base_mask)
                
                # é™¤å¤–ã™ã‚‹è¦ç´ ã«ã¯éå¸¸ã«å°ã•ãªå€¤ã‚’è¨­å®šã—ã€softmaxã§ç¢ºç‡ãŒ0ã«è¿‘ã¥ãã‚ˆã†ã«ã™ã‚‹
                masked_logits = tf.where(mask_to_exclude_from_negatives, -1e9, sim_matrix / self.temperature)
                
                # æ­£è§£ãƒ©ãƒ™ãƒ«ï¼ˆå¯¾è§’æˆåˆ†ãŒæ­£è§£ï¼‰
                cat_size = tf.shape(cat_preds)[0]
                labels = tf.range(cat_size, dtype=tf.int32)
                
                # InfoNCEæå¤±
                cat_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(
                    labels=labels, logits=masked_logits
                )
                
                # tf.reduce_sum(cat_loss) ã¯ã€ãƒã‚¹ã‚¯ã•ã‚ŒãŸ -1e9 ã®é …ã‚‚è¨ˆç®—ã«å«ã‚ã¦ã—ã¾ã†å¯èƒ½æ€§ãŒã‚ã‚‹ã®ã§æ³¨æ„ãŒå¿…è¦
                # æå¤±ã¯ãƒã‚¸ãƒ†ã‚£ãƒ–ãƒšã‚¢ã«å¯¾ã—ã¦è¨ˆç®—ã•ã‚Œã‚‹ã¹ããªã®ã§ã€labelsã§æŒ‡å®šã•ã‚ŒãŸãƒã‚¸ãƒ†ã‚£ãƒ–ãƒšã‚¢ã®æå¤±ã‚’åˆè¨ˆã™ã‚‹
                # masked_logitsã‹ã‚‰labelsã«å¯¾å¿œã™ã‚‹logitsã ã‘ã‚’å–ã‚Šå‡ºã—ã¦cross_entropyã‚’è¨ˆç®—
                # (logitsã¯æ—¢ã«-1e9ã§ãƒã‚¹ã‚¯ã•ã‚Œã¦ã„ã‚‹ã®ã§ã€ãã®ã¾ã¾sumã—ã¦ã‚‚è‰¯ã„ã¯ãšã ãŒã€ã‚ˆã‚Šå®‰å…¨ãªã®ã¯ä»¥ä¸‹)
                
                return tf.reduce_sum(cat_loss), tf.cast(cat_size, tf.float32)
            
            def skip_category():
                return 0.0, 0.0
            
            # ã‚«ãƒ†ã‚´ãƒªã«ååˆ†ãªã‚¢ã‚¤ãƒ†ãƒ ãŒã‚ã‚‹å ´åˆã®ã¿å‡¦ç†
            # ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒ†ã‚£ãƒ–æå¤±ã§ã¯ã€ãƒã‚¸ãƒ†ã‚£ãƒ–1ã¤ã¨ãƒã‚¬ãƒ†ã‚£ãƒ–ãŒæœ€ä½1ã¤å¿…è¦ã€‚
            # ä»Šå›ã®è¨­è¨ˆã§ã¯ã€ã€ŒåŒã˜ãƒãƒƒãƒå†…ã®è‡ªåˆ†è‡ªèº«ä»¥å¤–ã®ã‚¢ã‚¤ãƒ†ãƒ ã¯ãƒã‚¬ãƒ†ã‚£ãƒ–ã§ã¯ãªã„ã€ã®ã§ã€
            # åŒã˜ã‚«ãƒ†ã‚´ãƒªã®ã‚¢ã‚¤ãƒ†ãƒ ãŒè¤‡æ•°ã‚ã£ã¦ã‚‚ã€ãã‚Œã‚‰ãŒå…¨ã¦åŒã˜ãƒãƒƒãƒã«å±ã™ã‚‹å ´åˆã€
            # æœ‰åŠ¹ãªãƒã‚¬ãƒ†ã‚£ãƒ–ãƒšã‚¢ãŒãƒãƒƒãƒå†…ã«å­˜åœ¨ã—ãªã„å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚
            # ãªã®ã§ã€cat_count > 1 ã§ã¯ãªãã€
            # ã€Œãã®ã‚«ãƒ†ã‚´ãƒªå†…ã§ã€ç•°ãªã‚‹ãƒãƒƒãƒã«å±ã™ã‚‹ã‚¢ã‚¤ãƒ†ãƒ ãŒå­˜åœ¨ã™ã‚‹ã€ã¨ã„ã†æ¡ä»¶ãŒå¿…è¦ã«ãªã‚‹ã€‚
            # ã‚ã‚‹ã„ã¯ã€ã“ã“ã§ã¯ cat_count > 1 ã®ã¾ã¾ã«ã—ã¦ã€ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒã‚¹ã‚¯ã§å¯¾å¿œã™ã‚‹ã€‚
            
            # ã“ã®ã¾ã¾ã§ã‚‚ `masked_logits` ã® -1e9 ãŒ softmax ã§ã‚¼ãƒ­ã«è¿‘ã¥ã‘ã‚‹ãŸã‚ã€
            # æœ‰åŠ¹ãªãƒã‚¬ãƒ†ã‚£ãƒ–ãŒãªã‘ã‚Œã°çµæœçš„ã«æå¤±ãŒè¨ˆç®—ã•ã‚Œãªã„æ–¹å‘ã«ãªã‚‹ã¯ãšã€‚
            # cat_count > 1 ã§è‰¯ã„ã§ã—ã‚‡ã†ã€‚
            cat_loss_sum, cat_items_count = tf.cond(
                cat_count > 1,
                process_category,
                skip_category
            )
            
            total_loss += cat_loss_sum
            total_items_contributing += cat_items_count
        
        # 3. æœ€çµ‚æå¤±
        def compute_final():
            return total_loss / total_items_contributing
        
        def return_zero():
            return tf.constant(0.0, dtype=tf.float32)
        
        final_loss = tf.cond(
            total_items_contributing > 0.0, # æå¤±ã«è²¢çŒ®ã™ã‚‹ã‚¢ã‚¤ãƒ†ãƒ ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿è¨ˆç®—
            compute_final,
            return_zero
        )
        
        # å‹¾é…ãƒ‘ã‚¹ä¿æŒ
        final_loss += 0.0 * tf.reduce_sum(predictions)
        
        return final_loss


    @tf.function
    def _compute_tpaneg_loss(self, predictions, target_features, target_categories, candidate_neg_feats, candidate_neg_masks, current_taneg_t_gamma):
        B, S, D = tf.shape(target_features)[0], tf.shape(target_features)[1], tf.shape(target_features)[2]
        N_cand = tf.shape(candidate_neg_feats)[2] # N_cand ã¯å€™è£œãƒã‚¬ãƒ†ã‚£ãƒ–ã®æ•°

        # 1. ãƒ‡ãƒ¼ã‚¿ã®ãƒ•ãƒ©ãƒƒãƒˆåŒ–ã¨å‰å‡¦ç†
        target_feats_flat = tf.reshape(target_features, [-1, D])       # (B*S, D)
        target_cats_flat = tf.reshape(target_categories, [-1])         # (B*S)
        
        # cand_neg_feats: (B, S, N_cand, D) -> (B*S, N_cand, D)
        cand_neg_feats_flat = tf.reshape(candidate_neg_feats, [-1, N_cand, D])
        # cand_neg_masks: (B, S, N_cand) -> (B*S, N_cand)
        cand_neg_masks_flat = tf.reshape(candidate_neg_masks, [-1, N_cand])

        # å„ã‚¢ã‚¤ãƒ†ãƒ ã®å…ƒã®ãƒãƒƒãƒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨ã‚»ãƒƒãƒˆå†…ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä¿æŒ
        batch_indices = tf.range(B, dtype=tf.int32)
        item_indices = tf.range(S, dtype=tf.int32)
        grid_b, grid_i = tf.meshgrid(batch_indices, item_indices, indexing='ij')
        original_flat_batch_indices = tf.reshape(grid_b, [-1]) # å„ã‚¢ã‚¤ãƒ†ãƒ ã®å…ƒã®ãƒãƒƒãƒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ (B*S,)
        # original_flat_item_indices = tf.reshape(grid_i, [-1]) # å¿…è¦ã§ã‚ã‚Œã°ã‚»ãƒƒãƒˆå†…ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚‚
        
        # æœ‰åŠ¹ã‚¢ã‚¤ãƒ†ãƒ ãƒã‚¹ã‚¯ (ã‚«ãƒ†ã‚´ãƒªID > 0 ã‹ã¤ç‰¹å¾´é‡ãŒã‚¼ãƒ­ã§ãªã„)
        is_valid_item_mask = tf.logical_and(
            target_cats_flat > 0,
            tf.reduce_sum(tf.abs(target_feats_flat), axis=-1) > 1e-6
        )
        
        # predictions ã®å½¢çŠ¶ã¯ [B, C, D] ã§ã‚ã‚‹ã¨ã„ã†å‰æ
        # target_cats_flat ã®å„ã‚¢ã‚¤ãƒ†ãƒ ãŒå±ã™ã‚‹ã‚«ãƒ†ã‚´ãƒªã«å¯¾å¿œã™ã‚‹äºˆæ¸¬ãƒ™ã‚¯ãƒˆãƒ«ã‚’ predictions ã‹ã‚‰æŠ½å‡º
        preds_indices_for_gather = tf.stack([original_flat_batch_indices, tf.maximum(0, target_cats_flat - 1)], axis=1)
        preds_for_items_flat = tf.gather_nd(predictions, preds_indices_for_gather) # (B*S, D)
        
        # NaN/Infå¯¾ç­–ã¨æ­£è¦åŒ–
        preds = tf.where(tf.math.is_finite(preds_for_items_flat), preds_for_items_flat, 0.0)
        targets = tf.where(tf.math.is_finite(target_feats_flat), target_feats_flat, 0.0)
        cand_negs = tf.where(tf.math.is_finite(cand_neg_feats_flat), cand_neg_feats_flat, 0.0)
        
        preds_norm, _ = tf.linalg.normalize(preds + 1e-8, axis=-1)
        targets_norm, _ = tf.linalg.normalize(targets + 1e-8, axis=-1)
        
        # å€™è£œãƒã‚¬ãƒ†ã‚£ãƒ–ã‚‚æ­£è¦åŒ– (ãƒã‚¹ã‚¯ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯0ã«è¿‘ã¥ã‘ã‚‹)
        # tf.expand_dims(cand_neg_masks_flat, axis=-1) ã¯ (B*S, N_cand, 1) ã«ãªã‚‹
        mask_expanded = tf.cast(tf.expand_dims(cand_neg_masks_flat, axis=-1), tf.float32) # (B*S, N_cand, 1)
        safe_neg_feats = cand_negs * mask_expanded + (1.0 - mask_expanded) * 1e-8 # ãƒã‚¹ã‚¯å¤–ã¯å°ã•ãªå€¤
        cand_negs_norm, _ = tf.linalg.normalize(safe_neg_feats, axis=-1)
        cand_negs_norm = tf.where(tf.math.is_finite(cand_negs_norm), cand_negs_norm, 0.0)

        # 2. é¡ä¼¼åº¦è¨ˆç®—
        # sim_target_neg: (B*S, N_cand) - target_item ã¨ candidate_neg ã®é¡ä¼¼åº¦
        sim_target_neg = tf.einsum('id,ind->in', targets_norm, cand_negs_norm)
        
        # sim_pred_pos: (B*S,) - predicted_item ã¨ target_item (ãƒã‚¸ãƒ†ã‚£ãƒ–ãƒšã‚¢) ã®é¡ä¼¼åº¦
        sim_pred_pos = tf.einsum('id,id->i', preds_norm, targets_norm)
        
        # sim_pred_neg: (B*S, N_cand) - predicted_item ã¨ candidate_neg ã®é¡ä¼¼åº¦
        sim_pred_neg = tf.einsum('id,ind->in', preds_norm, cand_negs_norm)
        
        # 3. TPaNegãƒã‚¹ã‚¯ã®é©ç”¨
        paneg_epsilon = tf.constant(self.paneg_epsilon, dtype=tf.float32)
        
        # TaNegãƒã‚¹ã‚¯: target_item ã¨ candidate_neg ã®é¡ä¼¼åº¦ãŒé–¾å€¤ä»¥ä¸Š
        taneg_mask = sim_target_neg >= current_taneg_t_gamma
        
        # PaNegãƒã‚¹ã‚¯: predicted_item ã¨ candidate_neg ã®é¡ä¼¼åº¦ãŒ (predicted_item ã¨ target_item ã®é¡ä¼¼åº¦ - epsilon) ä»¥ä¸Š
        paneg_mask = sim_pred_neg >= (tf.expand_dims(sim_pred_pos, 1) - paneg_epsilon)
        
        # åŸºæœ¬ã®å€™è£œãƒã‚¬ãƒ†ã‚£ãƒ–ãƒã‚¹ã‚¯ (ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã•ã‚ŒãŸå€™è£œã‚’é™¤å¤–)
        base_cand_neg_mask = cand_neg_masks_flat # DataGeneratorã‹ã‚‰æ¥ã‚‹ãƒã‚¹ã‚¯

        # --- æ–°ã—ã„ãƒ­ã‚¸ãƒƒã‚¯: åŒã˜ã‚»ãƒƒãƒˆå†…ã®ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚’é™¤å¤– ---
        # candidate_neg_feats ã¯ DataGenerator ã§ B*S ã®å„ã‚¢ã‚¤ãƒ†ãƒ ã«å¯¾ã—ã¦ N_cand å€‹ã®ãƒã‚¬ãƒ†ã‚£ãƒ–ãŒä¸ãˆã‚‰ã‚Œã‚‹ã€‚
        # ã“ã“ã§ N_cand ã¯ã€DataGenerator ãŒã€Œå…ƒã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚¢ã‚¤ãƒ†ãƒ ã¨åŒã˜ã‚»ãƒƒãƒˆã«å±ã™ã‚‹ã€ã‚‚ã®ã‚’ãƒã‚¬ãƒ†ã‚£ãƒ–å€™è£œã¨ã—ã¦
        # é¸ã‚“ã§ã„ãªã„ã¨ã„ã†å‰æã«ç«‹ã¤ã®ãŒè‡ªç„¶ã€‚
        # ã‚‚ã— DataGenerator ã®ä¸­ã§ã€åŒã˜ã‚»ãƒƒãƒˆå†…ã®ä»–ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚‚ candidate_neg_feats ã«å«ã¾ã‚Œã¦ã—ã¾ã£ã¦ã„ã‚‹å ´åˆã€
        # ã“ã“ã§ãã‚Œã‚’é™¤å¤–ã™ã‚‹ãƒã‚¹ã‚¯ã‚’è¿½åŠ ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚

        # DataGeneratorãŒã©ã®ã‚ˆã†ã« candidate_negatives ã‚’åé›†ã—ã¦ã„ã‚‹ã‹ã«ã‚ˆã‚‹ãŒã€
        # é€šå¸¸ã€HardNegativeMinerã¯ã€Œç•°ãªã‚‹ã‚»ãƒƒãƒˆã€ã‹ã‚‰è² ä¾‹ã‚’é¸ã¶ãŸã‚ã€
        # åŒã˜ãƒãƒƒãƒå†…ã®åŒã˜ã‚»ãƒƒãƒˆã«å±ã™ã‚‹ã‚¢ã‚¤ãƒ†ãƒ ãŒ candidate_neg_feats ã«å«ã¾ã‚Œã‚‹ã“ã¨ã¯ç¨€ã§ã™ã€‚
        # ã‚‚ã—å«ã¾ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãªã‚‰ã€ä»¥ä¸‹ã®ãƒã‚¹ã‚¯ã‚’è¿½åŠ ã—ã¾ã™ã€‚

        # å€™è£œãƒã‚¬ãƒ†ã‚£ãƒ–ãŒå…ƒã®ãƒãƒƒãƒã¨åŒã˜ã‹ã©ã†ã‹ã‚’åˆ¤å®šã™ã‚‹ãŸã‚ã®æƒ…å ±ãŒå¿…è¦ã€‚
        # ç¾åœ¨ã® DataGenerator ã‹ã‚‰ã¯ candidate_neg_feats ã®å…ƒã®ãƒãƒƒãƒãƒ»ã‚»ãƒƒãƒˆIDã¯ç›´æ¥å¾—ã‚‰ã‚Œãªã„ã€‚
        # ã‚‚ã— candidate_neg_feats ã®å„å€™è£œãŒã©ã®ã‚»ãƒƒãƒˆã«ç”±æ¥ã™ã‚‹ã‹ã‚’ DataGenerator ãŒæä¾›ã—ãªã„é™ã‚Šã€
        # ã“ã®é–¢æ•°å†…ã§ã®å³å¯†ãªã€ŒåŒã˜ã‚»ãƒƒãƒˆå†…ãƒã‚¬ãƒ†ã‚£ãƒ–é™¤å¤–ã€ã¯é›£ã—ã„ã€‚
        # (DataGeneratorã‹ã‚‰ 'candidate_negative_original_set_ids' ã®ã‚ˆã†ãªã‚‚ã®ã‚’æ¸¡ã™å¿…è¦ãŒã‚ã‚‹)

        # ä»®ã« `candidate_neg_feats` ã«ã¯ã€ŒåŒã˜ã‚»ãƒƒãƒˆå†…ã®ä»–ã®ã‚¢ã‚¤ãƒ†ãƒ ã€ã¯å«ã¾ã‚Œã¦ã„ãªã„ã¨ä»®å®šã—ã€
        # TPaNegã®æ„å›³é€šã‚Šã«å‹•ãã‚‚ã®ã¨ã—ã¾ã™ã€‚
        # ã‚‚ã—ã€precompute_negatives_gpu ã‚„ HardNegativeMiner ãŒã€åŒã˜ã‚»ãƒƒãƒˆå†…ã®ä»–ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’
        # å€™è£œãƒã‚¬ãƒ†ã‚£ãƒ–ã¨ã—ã¦å«ã‚“ã§ã—ã¾ã£ã¦ã„ã‚‹ãªã‚‰ã€DataGeneratorã®å‡ºåŠ›ã«ãã®æƒ…å ±ã‚’è¿½åŠ ã—ã€
        # ã“ã“ã§ãã®æƒ…å ±ã‚’ä½¿ã£ã¦ãƒã‚¹ã‚¯ã‚’é©ç”¨ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

        # ç¾çŠ¶ã®ã‚³ãƒ¼ãƒ‰ã®ç¯„å›²å†…ã§ã§ãã‚‹ã®ã¯ã€ã‚ãã¾ã§æ¸¡ã•ã‚ŒãŸ candidate_neg_masks_flat ã®ç¯„å›²å†…ã§ã®å‡¦ç†ã€‚
        
        # TPaNegè«–æ–‡ã®æ„å›³é€šã‚Šã€æœ€çµ‚çš„ã«ãƒã‚¬ãƒ†ã‚£ãƒ–ã¨ã—ã¦æ¡ç”¨ã™ã‚‹æ¡ä»¶
        # 1. å…ƒã€…å€™è£œã¨ã—ã¦æœ‰åŠ¹ (cand_neg_masks_flat)
        # 2. TaNegåŸºæº–ã‚’æº€ãŸã™ (taneg_mask)
        # 3. PaNegåŸºæº–ã‚’æº€ãŸã™ (paneg_mask)
        final_neg_mask = tf.logical_and(tf.logical_and(base_cand_neg_mask, taneg_mask), paneg_mask)

        # 4. InfoNCEæå¤±è¨ˆç®—
        temperature = tf.constant(self.temperature, dtype=tf.float32)
        
        # ãƒã‚¬ãƒ†ã‚£ãƒ– logits: ãƒã‚¹ã‚¯ã•ã‚ŒãŸãƒã‚¬ãƒ†ã‚£ãƒ–å€™è£œã«ã¯éå¸¸ã«å°ã•ã„å€¤ã‚’è¨­å®š
        neg_logits = tf.where(final_neg_mask, sim_pred_neg / temperature, -1e9) # (B*S, N_cand)
        
        # ãƒã‚¸ãƒ†ã‚£ãƒ– logits: predicted_item ã¨ target_item ã®é¡ä¼¼åº¦
        pos_logits = tf.expand_dims(sim_pred_pos / temperature, 1) # (B*S, 1)
        
        # ãƒã‚¸ãƒ†ã‚£ãƒ–ã¨ãƒã‚¬ãƒ†ã‚£ãƒ–ã®logitsã‚’çµåˆ
        all_logits = tf.concat([pos_logits, neg_logits], axis=1) # (B*S, 1 + N_cand)
        
        # æ­£è§£ãƒ©ãƒ™ãƒ«ã¯ãƒã‚¸ãƒ†ã‚£ãƒ– (æœ€åˆã®åˆ—)
        labels = tf.zeros_like(target_cats_flat, dtype=tf.int32) # (B*S,) å…¨ã¦0

        # ã‚¢ã‚¤ãƒ†ãƒ ã”ã¨ã®æå¤± (InfoNCE)
        per_item_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=all_logits) # (B*S,)
        
        # 5. æœ‰åŠ¹ãªã‚¢ã‚¤ãƒ†ãƒ ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        # is_valid_item_mask: ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã•ã‚ŒãŸã‚¢ã‚¤ãƒ†ãƒ ã‚„ã‚«ãƒ†ã‚´ãƒªID=0ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’é™¤å¤–
        # tf.reduce_any(final_neg_mask, axis=1): ãã®ã‚¢ã‚¤ãƒ†ãƒ ã«å¯¾ã—ã¦å°‘ãªãã¨ã‚‚1ã¤ã®ãƒãƒ¼ãƒ‰ãƒã‚¬ãƒ†ã‚£ãƒ–ãŒé¸æŠã•ã‚ŒãŸã‹
        items_to_consider = tf.logical_and(is_valid_item_mask, tf.reduce_any(final_neg_mask, axis=1))
        
        # ãƒã‚¹ã‚¯ã•ã‚ŒãŸæå¤±ã®åˆè¨ˆ
        masked_loss = per_item_loss * tf.cast(items_to_consider, dtype=tf.float32)
        
        # æå¤±ã«è²¢çŒ®ã™ã‚‹ã‚¢ã‚¤ãƒ†ãƒ ã®æ•°
        num_items_for_loss = tf.reduce_sum(tf.cast(items_to_consider, dtype=tf.float32))
        
        # æœ€çµ‚æå¤± (ã‚¼ãƒ­é™¤ç®—å¯¾ç­–)
        final_loss = tf.math.divide_no_nan(tf.reduce_sum(masked_loss), num_items_for_loss)

        # ãƒ‡ãƒãƒƒã‚°ã‚³ãƒ¼ãƒ‰è¿½åŠ 
        # effective_negatives_per_item = tf.reduce_sum(tf.cast(final_neg_mask, tf.float32), axis=1)
        # avg_effective_negatives = tf.reduce_mean(effective_negatives_per_item)

        # tf.print("Avg effective negatives per item:", avg_effective_negatives)
        # tf.print("Max effective negatives:", tf.reduce_max(effective_negatives_per_item))
        # tf.print("Items with 0 negatives:", tf.reduce_sum(tf.cast(tf.equal(effective_negatives_per_item, 0), tf.float32)))
        
        # å‹¾é…ãƒ‘ã‚¹ä¿æŒã®ãŸã‚ã®ãƒ€ãƒŸãƒ¼åŠ ç®— (æå¤±ãŒ0ã«ãªã‚‹ã‚±ãƒ¼ã‚¹å¯¾ç­–)
        return final_loss + (0.0 * tf.reduce_sum(predictions))



    # @tf.function # ã“ã‚ŒãŒé–¢æ•°å…¨ä½“ã‚’ã‚°ãƒ©ãƒ•åŒ–ã™ã‚‹
    # def _compute_in_batch_hard_negative_loss(self, predictions, target_features, target_categories):
    #     """
    #     ãƒãƒƒãƒå†…ã®åŒã˜ã‚«ãƒ†ã‚´ãƒªã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’ãƒã‚¬ãƒ†ã‚£ãƒ–ã¨ã—ã¦æ‰±ã†
    #     ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒ†ã‚£ãƒ–å­¦ç¿’æå¤±
    #     """
    #     B, S, D = tf.shape(target_features)[0], tf.shape(target_features)[1], tf.shape(target_features)[2]

    #     # 1. ãƒ‡ãƒ¼ã‚¿ã®ãƒ•ãƒ©ãƒƒãƒˆåŒ–ã¨å‰å‡¦ç†
    #     target_feats_flat = tf.reshape(target_features, [-1, D])
    #     target_cats_flat = tf.reshape(target_categories, [-1])
        
    #     # ãƒãƒƒãƒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ä½œæˆ
    #     batch_indices = tf.range(B, dtype=tf.int32)
    #     item_indices = tf.range(S, dtype=tf.int32)
    #     grid_b, grid_i = tf.meshgrid(batch_indices, item_indices, indexing='ij')
    #     flat_indices = tf.stack([tf.reshape(grid_b, [-1]), tf.reshape(grid_i, [-1])], axis=1)
        
    #     # æœ‰åŠ¹ã‚¢ã‚¤ãƒ†ãƒ ãƒã‚¹ã‚¯
    #     is_valid_item_mask = tf.logical_and(
    #         target_cats_flat > 0,
    #         tf.reduce_sum(tf.abs(target_feats_flat), axis=-1) > 1e-6
    #     )
        
    #     # äºˆæ¸¬ãƒ™ã‚¯ãƒˆãƒ«ã®å–å¾—
    #     preds_indices = tf.stack([flat_indices[:, 0], tf.maximum(0, target_cats_flat - 1)], axis=1)
    #     preds_for_items_flat = tf.gather_nd(predictions, preds_indices)
        
    #     # NaN/Infå¯¾ç­–ã¨æ­£è¦åŒ–
    #     preds = tf.where(tf.math.is_finite(preds_for_items_flat), preds_for_items_flat, 0.0)
    #     targets = tf.where(tf.math.is_finite(target_feats_flat), target_feats_flat, 0.0)
        
    #     preds_norm, _ = tf.linalg.normalize(preds + 1e-8, axis=-1)
    #     targets_norm, _ = tf.linalg.normalize(targets + 1e-8, axis=-1)
        
    #     # 2. ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®æå¤±è¨ˆç®—
    #     total_loss = 0.0
    #     total_items = 0.0
        
    #     pdb.set_trace()
        
    #     # å„ã‚«ãƒ†ã‚´ãƒªã«ã¤ã„ã¦å‡¦ç†
    #     for cat_id in range(1, self.num_categories + 1):
    #         cat_id_tensor = tf.constant(cat_id, dtype=tf.int32)
            
    #         # ã“ã®ã‚«ãƒ†ã‚´ãƒªã®ã‚¢ã‚¤ãƒ†ãƒ ãƒã‚¹ã‚¯
    #         cat_mask = tf.logical_and(
    #             tf.equal(target_cats_flat, cat_id_tensor),
    #             is_valid_item_mask
    #         )
            
    #         cat_count = tf.reduce_sum(tf.cast(cat_mask, tf.int32))
            
    #         def process_category():
    #             # ã“ã®ã‚«ãƒ†ã‚´ãƒªã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’æŠ½å‡º
    #             cat_preds = tf.boolean_mask(preds_norm, cat_mask)      # (N_cat, D)
    #             cat_targets = tf.boolean_mask(targets_norm, cat_mask)  # (N_cat, D)
                
    #             # ã‚«ãƒ†ã‚´ãƒªå†…é¡ä¼¼åº¦è¡Œåˆ—
    #             sim_matrix = tf.matmul(cat_preds, cat_targets, transpose_b=True)  # (N_cat, N_cat)
                
    #             # æ¸©åº¦ã§å‰²ã‚‹
    #             logits = sim_matrix / self.temperature
                
    #             # æ­£è§£ãƒ©ãƒ™ãƒ«ï¼ˆå¯¾è§’æˆåˆ†ãŒæ­£è§£ï¼‰
    #             cat_size = tf.shape(cat_preds)[0]
    #             labels = tf.range(cat_size, dtype=tf.int32)
                
    #             # InfoNCEæå¤±
    #             cat_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(
    #                 labels=labels, logits=logits
    #             )
                
    #             return tf.reduce_sum(cat_loss), tf.cast(cat_size, tf.float32)
            
    #         def skip_category():
    #             return 0.0, 0.0
            
    #         # ã‚«ãƒ†ã‚´ãƒªã«ååˆ†ãªã‚¢ã‚¤ãƒ†ãƒ ãŒã‚ã‚‹å ´åˆã®ã¿å‡¦ç†
    #         cat_loss_sum, cat_items_count = tf.cond(
    #             cat_count > 1,  # æœ€ä½2å€‹å¿…è¦ï¼ˆè‡ªåˆ† + ãƒã‚¬ãƒ†ã‚£ãƒ–ï¼‰
    #             process_category,
    #             skip_category
    #         )
            
    #         total_loss += cat_loss_sum
    #         total_items += cat_items_count
        
    #     # 3. æœ€çµ‚æå¤±
    #     def compute_final():
    #         return total_loss / total_items
        
    #     def return_zero():
    #         return tf.constant(0.0, dtype=tf.float32)
        
    #     final_loss = tf.cond(
    #         total_items > 0.0,
    #         compute_final,
    #         return_zero
    #     )
        
    #     # å‹¾é…ãƒ‘ã‚¹ä¿æŒ
    #     final_loss += 0.0 * tf.reduce_sum(predictions)
        
    #     return final_loss

    # @tf.function
    # def _compute_tpaneg_loss(self, predictions, target_features, target_categories, candidate_neg_feats, candidate_neg_masks, current_taneg_t_gamma):
    #     B, S, D = tf.shape(target_features)[0], tf.shape(target_features)[1], tf.shape(target_features)[2]
    #     N_cand = tf.shape(candidate_neg_feats)[2]

    #     target_feats_flat = tf.reshape(target_features, [-1, D])
    #     target_cats_flat = tf.reshape(target_categories, [-1])
    #     cand_neg_feats_flat = tf.reshape(candidate_neg_feats, [-1, N_cand, D])
    #     cand_neg_masks_flat = tf.reshape(candidate_neg_masks, [-1, N_cand])

    #     is_valid_item_mask = target_cats_flat > 0
        
    #     batch_indices = tf.range(B, dtype=tf.int32) 
    #     item_indices = tf.range(S, dtype=tf.int32)
    #     grid_b, grid_i = tf.meshgrid(batch_indices, item_indices, indexing='ij') 
    #     flat_indices = tf.stack([tf.reshape(grid_b, [-1]), tf.reshape(grid_i, [-1])], axis=1)
        
    #     preds_indices_for_gather = tf.stack([flat_indices[:, 0], tf.maximum(0, target_cats_flat - 1)], axis=1)
    #     preds_for_items_flat = tf.gather_nd(predictions, preds_indices_for_gather)
        
    #     preds = tf.where(tf.math.is_finite(preds_for_items_flat), preds_for_items_flat, 0.0)
    #     targets = tf.where(tf.math.is_finite(target_feats_flat), target_feats_flat, 0.0)
    #     cand_negs = tf.where(tf.math.is_finite(cand_neg_feats_flat), cand_neg_feats_flat, 0.0)
        
    #     preds_norm, _ = tf.linalg.normalize(preds + 1e-8, axis=-1)
    #     targets_norm, _ = tf.linalg.normalize(targets + 1e-8, axis=-1)
        
    #     mask_expanded = tf.expand_dims(cand_neg_masks_flat, axis=-1)
    #     safe_neg_feats = cand_negs + (tf.cast(mask_expanded, tf.float32) * 1e-8)
    #     cand_negs_norm, _ = tf.linalg.normalize(safe_neg_feats, axis=-1)
    #     cand_negs_norm = tf.where(tf.math.is_finite(cand_negs_norm), cand_negs_norm, 0.0)
        
    #     sim_target_neg = tf.einsum('id,ind->in', targets_norm, cand_negs_norm)
    #     sim_pred_pos = tf.einsum('id,id->i', preds_norm, targets_norm)
    #     sim_pred_neg = tf.einsum('id,ind->in', preds_norm, cand_negs_norm)
        
    #     paneg_epsilon = tf.constant(self.paneg_epsilon, dtype=tf.float32)
    #     taneg_mask = sim_target_neg >= current_taneg_t_gamma
    #     paneg_mask = sim_pred_neg >= (tf.expand_dims(sim_pred_pos, 1) - paneg_epsilon)
    #     final_neg_mask = tf.logical_and(tf.logical_and(cand_neg_masks_flat, taneg_mask), paneg_mask)

    #     temperature = tf.constant(self.temperature, dtype=tf.float32)
    #     neg_logits = tf.where(final_neg_mask, sim_pred_neg / temperature, -1e9)
    #     pos_logits = tf.expand_dims(sim_pred_pos / temperature, 1)
    #     all_logits = tf.concat([pos_logits, neg_logits], axis=1)
        
    #     labels = tf.zeros_like(target_cats_flat, dtype=tf.int32)
    #     per_item_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=all_logits)
        
    #     items_to_consider = tf.logical_and(is_valid_item_mask, tf.reduce_any(final_neg_mask, axis=1))
        
    #     masked_loss = per_item_loss * tf.cast(items_to_consider, dtype=tf.float32)
        
    #     num_items_for_loss = tf.reduce_sum(tf.cast(items_to_consider, dtype=tf.float32))
        
    #     final_loss = tf.math.divide_no_nan(tf.reduce_sum(masked_loss), num_items_for_loss)
        
    #     # pdb.set_trace()

    #     return final_loss + (0.0 * tf.reduce_sum(predictions))



    def _get_taneg_candidates(self, item_id, category, similarity_threshold):
        """
        TaNeg: Target-aware curriculum hard Negative mining
        
        è«–æ–‡ã®Equation 7å®Ÿè£…:
        N_cy(T_Î³) = {l âˆˆ G_cy : l â‰  y, sim(l, y) â‰¥ T_Î³}
        """
        if not hasattr(self, 'negative_pool') or category not in self.negative_pool:
            return []
        
        category_pool = self.negative_pool[category]  # åŒã‚«ãƒ†ã‚´ãƒªã®å…¨ã‚¢ã‚¤ãƒ†ãƒ 
        
        if len(category_pool) <= 1:
            return []
        
        # æ­£è§£ã‚¢ã‚¤ãƒ†ãƒ ã®ç‰¹å¾´é‡ã‚’å–å¾—ï¼ˆç°¡æ˜“å®Ÿè£…ï¼‰
        target_features = self._get_item_features(item_id, category)
        if target_features is None:
            return []
        
        # é¡ä¼¼åº¦è¨ˆç®—ã—ã¦é–¾å€¤ä»¥ä¸Šã®ã‚‚ã®ã‚’é¸æŠ
        similarities = np.dot(category_pool, target_features)
        hard_negative_mask = similarities >= similarity_threshold
        
        # æ­£è§£ã‚¢ã‚¤ãƒ†ãƒ è‡ªä½“ã¯é™¤å¤–
        hard_negatives = category_pool[hard_negative_mask]
        
        # ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã§æ•°ã‚’åˆ¶é™
        if len(hard_negatives) > self.candidate_neg_num:
            indices = np.random.choice(
                len(hard_negatives), 
                size=self.candidate_neg_num, 
                replace=False
            )
            hard_negatives = hard_negatives[indices]
        
        return hard_negatives.tolist()

    def _apply_curriculum_threshold_schedule(self, epoch, total_epochs):
        """
        ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ å­¦ç¿’ï¼šã‚¨ãƒãƒƒã‚¯ã«å¿œã˜ã¦é¡ä¼¼åº¦é–¾å€¤ã‚’æ®µéšçš„ã«ä¸Šæ˜‡
        
        è«–æ–‡ã§ã¯: 0.2 â†’ 0.4 (IQON3000), 0.5 â†’ 0.8 (DeepFurniture)
        """
        if self.dataset_name == 'IQON3000':
            start_threshold = 0.2
            end_threshold = 0.4
        else:  # DeepFurniture
            start_threshold = 0.5
            end_threshold = 0.8
        
        # ç·šå½¢ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°
        progress = min(epoch / total_epochs, 1.0)
        current_threshold = start_threshold + (end_threshold - start_threshold) * progress
        
        return current_threshold

    @tf.function
    def compute_cycle_consistency_loss(self, query_features, target_features, 
                                    query_categories, target_categories):
        """
        åŒæ–¹å‘Cycle Consistency Lossè¨ˆç®—
        
        Args:
            query_features: (B, N_X, D) - ã‚¯ã‚¨ãƒªã‚¢ã‚¤ãƒ†ãƒ ç‰¹å¾´é‡
            target_features: (B, N_Y, D) - ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚¢ã‚¤ãƒ†ãƒ ç‰¹å¾´é‡
            query_categories: (B, N_X) - ã‚¯ã‚¨ãƒªã‚«ãƒ†ã‚´ãƒª
            target_categories: (B, N_Y) - ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚«ãƒ†ã‚´ãƒª
        """
        
        # Forward Path: X â†’ Y'
        # 1. ã‚¯ã‚¨ãƒªã‹ã‚‰ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚«ãƒ†ã‚´ãƒªã®äºˆæ¸¬ã‚’ç”Ÿæˆ
        forward_input = {
            'query_features': query_features,
            'target_categories': target_categories  # Z^Y ã®ä»£ã‚ã‚Š
        }
        predicted_targets = self(forward_input, training=True)  # Y' = f_Î¸(X, Z^Y)
        
        # 2. Forward Contrastive Loss
        forward_loss = self._compute_item_level_contrastive_loss(
            predicted_targets, target_features, target_categories
        )
        
        # Backward Path: Y' â†’ X'
        # 3. äºˆæ¸¬ã‹ã‚‰ã‚¯ã‚¨ãƒªã‚«ãƒ†ã‚´ãƒªã®å†æ§‹ç¯‰
        backward_input = {
            'query_features': predicted_targets,  # Y'ã‚’æ–°ã—ã„ã‚¯ã‚¨ãƒªã¨ã—ã¦ä½¿ç”¨
            'target_categories': query_categories  # Z^X ã®ä»£ã‚ã‚Š
        }
        reconstructed_queries = self(backward_input, training=True)  # X' = f_Î¸(Y', Z^X)
        
        # 4. Backward Contrastive Loss
        backward_loss = self._compute_item_level_contrastive_loss(
            reconstructed_queries, query_features, query_categories
        )
        
        # 5. ç·åˆæå¤±ï¼ˆè«–æ–‡ã®Equation 4ï¼‰
        total_cycle_loss = forward_loss + self.cycle_lambda * backward_loss
        
        return total_cycle_loss, forward_loss, backward_loss


    def _compute_item_level_contrastive_loss(self, predictions, target_items, target_categories):
        """
        ã‚¢ã‚¤ãƒ†ãƒ ãƒ¬ãƒ™ãƒ«ã®Contrastive Lossï¼ˆè«–æ–‡ã®Equation 5ï¼‰
        
        L_con(Y,Å¶) = -1/N_Y * Î£ log(exp(sim(Å·_p, y_p)/Ï„) / Î£_i exp(sim(Å·_p, y_i)/Ï„))
        """
        
        total_loss = 0.0
        num_valid_items = 0
        
        B, S = tf.shape(target_items)[0], tf.shape(target_items)[1]
        C = tf.shape(predictions)[1]
        
        for batch_idx in range(B):
            for item_idx in range(S):
                target_cat = target_categories[batch_idx, item_idx]
                
                if target_cat <= 0:
                    continue  # ç„¡åŠ¹ãªã‚¢ã‚¤ãƒ†ãƒ ã¯ã‚¹ã‚­ãƒƒãƒ—
                
                # ã‚«ãƒ†ã‚´ãƒªäºˆæ¸¬ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—
                predicted_vector = predictions[batch_idx, target_cat - 1, :]  # (D,)
                positive_item = target_items[batch_idx, item_idx, :]          # (D,)
                
                # æ­£è§£ã¨ã®é¡ä¼¼åº¦
                pos_sim = tf.reduce_sum(predicted_vector * positive_item) / self.temperature
                
                # åŒã˜ãƒãƒƒãƒå†…ã®å…¨ã‚¢ã‚¤ãƒ†ãƒ ã¨ã®é¡ä¼¼åº¦ï¼ˆãƒã‚¬ãƒ†ã‚£ãƒ–å«ã‚€ï¼‰
                all_items = tf.reshape(target_items, [-1, tf.shape(target_items)[-1]])  # (B*S, D)
                all_sims = tf.reduce_sum(
                    tf.expand_dims(predicted_vector, 0) * all_items, axis=1
                ) / self.temperature  # (B*S,)
                
                # InfoNCEæå¤±
                # åˆ†å­: exp(pos_sim)
                # åˆ†æ¯: Î£ exp(all_sims)
                loss = -pos_sim + tf.reduce_logsumexp(all_sims)
                
                total_loss += loss
                num_valid_items += 1
        
        return tf.cond(
            num_valid_items > 0,
            lambda: total_loss / tf.cast(num_valid_items, tf.float32),
            lambda: tf.constant(0.0, dtype=tf.float32)
        )


    # è«–æ–‡ã®Equation 9: ãƒ€ãƒ–ãƒ«åŒæ–¹å‘å‡¦ç†
    def compute_double_bidirectional_loss(self, query_features, target_features,
                                        query_categories, target_categories):
        """
        è«–æ–‡ã®Equation 9å®Ÿè£…: ä¸¡æ–¹å‘ã§ã®Cycle Consistency
        L_bi(X,Y,Z^Y,Z^X) + L_bi(Y,X,Z^X,Z^Y)
        """
        
        # ç¬¬1ã®åŒæ–¹å‘: (X â†’ Y â†’ X)
        loss_1, fwd_1, bwd_1 = self.compute_cycle_consistency_loss(
            query_features, target_features, query_categories, target_categories
        )
        
        # ç¬¬2ã®åŒæ–¹å‘: (Y â†’ X â†’ Y) - å½¹å‰²ã‚’äº¤æ›
        loss_2, fwd_2, bwd_2 = self.compute_cycle_consistency_loss(
            target_features, query_features, target_categories, query_categories
        )
        
        # ç·åˆæå¤±
        total_loss = loss_1 + loss_2
        
        return total_loss, {
            'cycle_loss_1': loss_1,
            'cycle_loss_2': loss_2,
            'forward_1': fwd_1,
            'backward_1': bwd_1,
            'forward_2': fwd_2,
            'backward_2': bwd_2
        }